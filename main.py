"""
B√öHO - Scraper de Gaceta Oficial de Bolivia
Punto de entrada principal del sistema

Fase 3: Extracci√≥n de Metadatos Legales
"""

import sys
from pathlib import Path

# Agregar el directorio ra√≠z al path para imports
sys.path.insert(0, str(Path(__file__).parent))

from scraper.gaceta_scraper import crear_datos_ejemplo, GacetaScraper


def main():
    """
    Funci√≥n principal del scraper.

    Ejecuta el scraper con un l√≠mite peque√±o de documentos
    y muestra un resumen de metadatos extra√≠dos.
    """
    print("="*80)
    print("ü¶â B√öHO - Sistema de Scraping de Gaceta Oficial de Bolivia")
    print("   FASE 3: Extracci√≥n y Normalizaci√≥n de Metadatos")
    print("="*80)
    print()

    # Configuraci√≥n
    NUM_DOCUMENTOS_PRUEBA = 8  # L√≠mite peque√±o para pruebas
    OUTPUT_DIR = "data"
    EXPORT_DIR = "exports"

    print(f"‚öôÔ∏è  Configuraci√≥n:")
    print(f"   - Documentos a procesar: {NUM_DOCUMENTOS_PRUEBA}")
    print(f"   - Directorio de PDFs: {OUTPUT_DIR}/")
    print(f"   - Directorio de exports: {EXPORT_DIR}/")
    print()

    # Ejecutar scraper con datos de ejemplo
    # En producci√≥n, esto se reemplazar√≠a con el scraping real
    print("üöÄ Iniciando scraper...\n")

    scraper = crear_datos_ejemplo(
        output_dir=OUTPUT_DIR,
        export_dir=EXPORT_DIR,
        num_docs=NUM_DOCUMENTOS_PRUEBA
    )

    # Mostrar resumen de metadatos
    print("\n" + "="*80)
    print("üìä RESUMEN DE METADATOS EXTRA√çDOS")
    print("="*80)

    resumen = scraper.obtener_resumen(limit=5)

    if resumen:
        for i, doc in enumerate(resumen, 1):
            print(f"\n{i}. DOCUMENTO:")
            print(f"   üìù T√≠tulo: {doc['titulo'][:70]}...")
            print(f"   üè∑Ô∏è  Tipo de norma: {doc['tipo_norma']}")
            print(f"   üî¢ N√∫mero: {doc['numero_norma'] or 'No detectado'}")
            print(f"   üìÖ Fecha: {doc['fecha_publicacion_aproximada'] or 'No detectada'}")
            print(f"   üìÑ Archivo: {doc['archivo_descargado']}")
            print(f"   ‚úÖ Estado: {doc['estado']}")
    else:
        print("\n‚ö†Ô∏è  No se procesaron documentos")

    # Informaci√≥n del CSV
    print("\n" + "="*80)
    print("üìÅ ARCHIVOS GENERADOS")
    print("="*80)

    csv_path = Path(EXPORT_DIR) / "gaceta_log.csv"
    if csv_path.exists():
        print(f"\n‚úÖ CSV de log generado:")
        print(f"   üìç Ruta: {csv_path.absolute()}")
        print(f"   üìä Registros: {len(scraper.documentos)}")
        print(f"\n   Columnas del CSV:")
        print(f"   - titulo")
        print(f"   - url_pdf")
        print(f"   - archivo_descargado")
        print(f"   - fecha_extraccion")
        print(f"   - estado")
        print(f"   - tipo_norma           [NUEVA - Fase 3]")
        print(f"   - numero_norma         [NUEVA - Fase 3]")
        print(f"   - fecha_publicacion_aproximada [NUEVA - Fase 3]")
    else:
        print("\n‚ö†Ô∏è  No se gener√≥ el archivo CSV")

    # Estad√≠sticas de metadatos
    print("\n" + "="*80)
    print("üìà ESTAD√çSTICAS DE EXTRACCI√ìN")
    print("="*80)

    if scraper.documentos:
        # Contar tipos de norma
        tipos_norma = {}
        metadatos_completos = 0
        metadatos_parciales = 0

        for doc in scraper.documentos:
            # Contar tipo
            tipo = doc.get('tipo_norma', 'Desconocido')
            tipos_norma[tipo] = tipos_norma.get(tipo, 0) + 1

            # Contar completitud de metadatos
            tiene_numero = bool(doc.get('numero_norma'))
            tiene_fecha = bool(doc.get('fecha_publicacion_aproximada'))

            if tiene_numero and tiene_fecha:
                metadatos_completos += 1
            elif tiene_numero or tiene_fecha:
                metadatos_parciales += 1

        print(f"\nüìä Tipos de norma detectados:")
        for tipo, cantidad in sorted(tipos_norma.items(), key=lambda x: x[1], reverse=True):
            porcentaje = (cantidad / len(scraper.documentos)) * 100
            print(f"   - {tipo}: {cantidad} ({porcentaje:.1f}%)")

        print(f"\nüéØ Completitud de metadatos:")
        print(f"   - Metadatos completos (tipo + n√∫mero + fecha): {metadatos_completos}")
        print(f"   - Metadatos parciales (tipo + n√∫mero o fecha): {metadatos_parciales}")
        print(f"   - Solo tipo detectado: {len(scraper.documentos) - metadatos_completos - metadatos_parciales}")

    print("\n" + "="*80)
    print("‚úÖ Proceso completado exitosamente")
    print("="*80)
    print()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Proceso interrumpido por el usuario")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå Error fatal: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
