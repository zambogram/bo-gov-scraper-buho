"""
Scraper base para todos los sitios
Soporte para scraping hist√≥rico completo y delta updates
"""
from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path
import requests
import time
import logging
from datetime import datetime

from scraper.models import Documento
from config import get_site_config

logger = logging.getLogger(__name__)


class BaseScraper(ABC):
    """Clase base para todos los scrapers con soporte para scraping hist√≥rico"""

    def __init__(self, site_id: str):
        """
        Inicializar scraper

        Args:
            site_id: ID del sitio (tcp, tsj, etc.)
        """
        self.site_id = site_id
        self.config = get_site_config(site_id)

        if not self.config:
            raise ValueError(f"Configuraci√≥n no encontrada para sitio: {site_id}")

        # Configuraci√≥n de requests
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 BUHO Legal Scraper/1.0'
        })

        # Delay entre requests
        self.delay = self.config.scraper.get('delay_entre_requests', 2)

        # Items por p√°gina
        self.items_por_pagina = self.config.scraper.get('items_por_pagina', 20)

    @abstractmethod
    def listar_documentos(
        self,
        limite: Optional[int] = None,
        modo: str = "delta",
        pagina: int = 1
    ) -> List[Dict[str, Any]]:
        """
        Listar documentos disponibles en el sitio

        Args:
            limite: N√∫mero m√°ximo de documentos a retornar
            modo: 'full' para hist√≥rico completo, 'delta' para solo nuevos
            pagina: N√∫mero de p√°gina a obtener (para paginaci√≥n)

        Returns:
            Lista de diccionarios con metadata de documentos
        """
        pass

    def listar_documentos_historico_completo(
        self,
        limite_total: Optional[int] = None,
        progress_callback: Optional[callable] = None
    ) -> List[Dict[str, Any]]:
        """
        Listar TODOS los documentos del sitio (scraping hist√≥rico completo)

        Recorre todas las p√°ginas disponibles hasta agotar resultados.

        Args:
            limite_total: L√≠mite total de documentos (None = sin l√≠mite)
            progress_callback: Funci√≥n callback para reportar progreso

        Returns:
            Lista completa de documentos
        """
        todos_documentos = []
        pagina = 1
        documentos_obtenidos = 0

        logger.info(f"üîÑ Iniciando scraping hist√≥rico completo de {self.site_id}")

        while True:
            # Verificar si ya alcanzamos el l√≠mite
            if limite_total and documentos_obtenidos >= limite_total:
                logger.info(f"‚úì Alcanzado l√≠mite de {limite_total} documentos")
                break

            # Calcular cu√°ntos documentos solicitar en esta p√°gina
            limite_pagina = None
            if limite_total:
                restantes = limite_total - documentos_obtenidos
                limite_pagina = min(self.items_por_pagina, restantes)

            # Listar documentos de esta p√°gina
            try:
                logger.info(f"üìÑ Obteniendo p√°gina {pagina}...")
                documentos_pagina = self.listar_documentos(
                    limite=limite_pagina,
                    modo="full",
                    pagina=pagina
                )

                # Si no hay m√°s documentos, terminar
                if not documentos_pagina:
                    logger.info(f"‚úì No hay m√°s documentos en p√°gina {pagina}")
                    break

                # Agregar documentos de esta p√°gina
                todos_documentos.extend(documentos_pagina)
                documentos_obtenidos += len(documentos_pagina)

                # Callback de progreso
                if progress_callback:
                    progress_callback(
                        f"P√°gina {pagina}: {len(documentos_pagina)} documentos "
                        f"(total: {documentos_obtenidos})"
                    )

                logger.info(
                    f"‚úì P√°gina {pagina}: {len(documentos_pagina)} documentos "
                    f"(acumulado: {documentos_obtenidos})"
                )

                # Si obtuvimos menos documentos que el m√°ximo por p√°gina,
                # probablemente sea la √∫ltima p√°gina
                if len(documentos_pagina) < self.items_por_pagina:
                    logger.info(f"‚úì √öltima p√°gina alcanzada (p√°gina {pagina})")
                    break

                # Siguiente p√°gina
                pagina += 1

                # Delay entre p√°ginas
                time.sleep(self.delay)

            except Exception as e:
                logger.error(f"‚úó Error en p√°gina {pagina}: {e}")
                # Continuar con la siguiente p√°gina en caso de error
                pagina += 1
                if pagina > 100:  # M√°ximo de seguridad
                    logger.warning("Alcanzado l√≠mite de seguridad de 100 p√°ginas")
                    break

        logger.info(
            f"‚úÖ Scraping hist√≥rico completado: {documentos_obtenidos} documentos totales"
        )

        return todos_documentos

    @abstractmethod
    def descargar_pdf(self, url: str, ruta_destino: Path) -> bool:
        """
        Descargar PDF de un documento

        Args:
            url: URL del PDF
            ruta_destino: Ruta donde guardar el PDF

        Returns:
            True si se descarg√≥ correctamente
        """
        pass

    def _download_file(self, url: str, destino: Path, timeout: int = 30) -> bool:
        """
        M√©todo auxiliar para descargar archivos

        Args:
            url: URL del archivo
            destino: Ruta de destino
            timeout: Timeout en segundos

        Returns:
            True si se descarg√≥ correctamente
        """
        try:
            # Asegurar que el directorio existe
            destino.parent.mkdir(parents=True, exist_ok=True)

            # Descargar
            response = self.session.get(url, timeout=timeout, stream=True)
            response.raise_for_status()

            # Guardar
            with open(destino, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            logger.info(f"‚úì Descargado: {destino.name}")

            # Esperar antes del siguiente request
            time.sleep(self.delay)

            return True

        except Exception as e:
            logger.error(f"‚úó Error descargando {url}: {e}")
            return False

    def crear_documento_desde_metadata(self, metadata: Dict[str, Any]) -> Documento:
        """
        Crear objeto Documento desde metadata

        Args:
            metadata: Diccionario con metadata del documento

        Returns:
            Documento inicializado
        """
        return Documento(
            id_documento=metadata.get('id_documento', ''),
            site=self.site_id,
            tipo_documento=metadata.get('tipo_documento', ''),
            numero_norma=metadata.get('numero_norma'),
            fecha=metadata.get('fecha'),
            fecha_publicacion=metadata.get('fecha_publicacion'),
            titulo=metadata.get('titulo'),
            sumilla=metadata.get('sumilla'),
            url_origen=metadata.get('url'),
            metadata=metadata
        )

    def __del__(self):
        """Cerrar sesi√≥n al destruir el objeto"""
        if hasattr(self, 'session'):
            self.session.close()
