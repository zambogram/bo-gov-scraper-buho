# ğŸ¦‰ BÃšHO - Scraper del Estado Boliviano

## ğŸ“– Â¿QUÃ‰ ES ESTE PROYECTO?

Este proyecto es un **scraper** (programa que descarga y procesa informaciÃ³n de sitios web) diseÃ±ado para extraer documentos y datos de pÃ¡ginas gubernamentales de Bolivia. El objetivo es recopilar, organizar y procesar informaciÃ³n pÃºblica de forma automÃ¡tica.

---

## ğŸ“‚ ESTRUCTURA DEL PROYECTO

```
bo-gov-scraper-buho/
â”‚
â”œâ”€â”€ venv/                    â† Entorno virtual de Python (NO se sube a GitHub)
â”‚   â””â”€â”€ (librerÃ­as instaladas aquÃ­)
â”‚
â”œâ”€â”€ app/                     â† Interfaz visual del proyecto
â”‚   â””â”€â”€ streamlit_app.py     â† AplicaciÃ³n web para visualizar datos
â”‚
â”œâ”€â”€ scraper/                 â† CÃ³digo principal del scraper
â”‚   â”œâ”€â”€ __init__.py          â† Archivo que marca esta carpeta como mÃ³dulo Python
â”‚   â””â”€â”€ metadata.py          â† Funciones para extraer metadatos de documentos
â”‚
â”œâ”€â”€ data/                    â† Datos descargados
â”‚   â””â”€â”€ README.md            â† AquÃ­ se guardarÃ¡n PDFs, imÃ¡genes, etc.
â”‚
â”œâ”€â”€ exports/                 â† Archivos exportados procesados
â”‚   â””â”€â”€ README.md            â† AquÃ­ se guardarÃ¡n Excel, CSV, etc.
â”‚
â”œâ”€â”€ main.py                  â† Punto de entrada principal del programa
â”œâ”€â”€ requirements.txt         â† Lista de librerÃ­as necesarias
â”œâ”€â”€ .gitignore               â† Archivos que Git ignorarÃ¡
â”œâ”€â”€ LICENSE                  â† Licencia del proyecto
â””â”€â”€ README.md                â† DescripciÃ³n general del proyecto

```

---

## ğŸ¯ Â¿PARA QUÃ‰ SIRVE CADA CARPETA?

### ğŸ“ **venv/** - Entorno Virtual
**Â¿QuÃ© es?** Una "burbuja" aislada donde viven todas las librerÃ­as de Python que necesita este proyecto.

**Â¿Por quÃ© existe?** Para que las librerÃ­as de este proyecto no interfieran con otros proyectos de Python en tu computadora. Es como tener un set de herramientas especÃ­ficas solo para este trabajo.

**Importante:** Esta carpeta NO se sube a GitHub (estÃ¡ en `.gitignore`).

---

### ğŸ“ **app/** - AplicaciÃ³n Visual
**Â¿QuÃ© es?** AquÃ­ va el cÃ³digo de la interfaz web hecha con Streamlit.

**Â¿Para quÃ© sirve?** Streamlit permite crear una pÃ¡gina web interactiva donde puedes:
- Ver los datos que se han descargado
- Iniciar el proceso de scraping con botones
- Visualizar estadÃ­sticas y grÃ¡ficos
- Exportar datos a Excel

**Ejemplo:** Piensa en esto como el "tablero de control" del proyecto.

---

### ğŸ“ **scraper/** - Motor del Scraper
**Â¿QuÃ© es?** El "cerebro" del proyecto. AquÃ­ va todo el cÃ³digo que hace el trabajo de:
- Navegar por las pÃ¡ginas web
- Descargar documentos (PDFs, imÃ¡genes)
- Extraer texto usando OCR (reconocimiento Ã³ptico de caracteres)
- Organizar la informaciÃ³n

**Archivos importantes:**
- `__init__.py`: Archivo especial que convierte la carpeta en un "mÃ³dulo" de Python (permite importar cÃ³digo desde aquÃ­)
- `metadata.py`: Funciones para extraer informaciÃ³n como fecha, autor, tÃ­tulo, etc. de los documentos

---

### ğŸ“ **data/** - Datos Sin Procesar
**Â¿QuÃ© es?** AlmacÃ©n de todo lo que se descarga directamente de internet.

**Â¿QuÃ© contiene?**
- PDFs gubernamentales
- ImÃ¡genes de documentos
- HTML de pÃ¡ginas web
- Archivos temporales

**Importante:** Esta carpeta crece con el tiempo y puede ocupar mucho espacio.

---

### ğŸ“ **exports/** - Datos Procesados
**Â¿QuÃ© es?** AquÃ­ se guardan los resultados finales, ya procesados y organizados.

**Â¿QuÃ© contiene?**
- Archivos Excel (.xlsx) con datos tabulados
- CSV con informaciÃ³n extraÃ­da
- Reportes en PDF
- Bases de datos SQLite

**Diferencia con `data/`:**
- `data/` = lo que descargas (crudo, sin procesar)
- `exports/` = lo que produces (limpio, organizado, listo para usar)

---

## ğŸ› ï¸ HERRAMIENTAS INSTALADAS

### **Web Scraping** (Descarga y navegaciÃ³n)
- **requests** â†’ Descarga pÃ¡ginas web (como un navegador simple)
- **beautifulsoup4** â†’ Lee y analiza HTML (estructura de pÃ¡ginas web)
- **lxml** â†’ Procesa XML y HTML rÃ¡pidamente
- **selenium** â†’ Automatiza un navegador real (para pÃ¡ginas complejas con JavaScript)

### **OCR y Procesamiento de ImÃ¡genes** (Lectura de texto en imÃ¡genes)
- **pytesseract** â†’ Lee texto de imÃ¡genes (OCR = Optical Character Recognition)
- **Pillow** â†’ Edita y procesa imÃ¡genes
- **pdf2image** â†’ Convierte PDFs a imÃ¡genes (para luego aplicar OCR)

### **AnÃ¡lisis de Datos**
- **pandas** â†’ Organiza datos en tablas (como Excel pero en Python)
- **openpyxl** â†’ Lee y escribe archivos Excel

### **Interfaz de Usuario**
- **streamlit** â†’ Crea aplicaciones web interactivas sin saber HTML/CSS/JavaScript

### **Utilidades**
- **python-dotenv** â†’ Lee variables de entorno (configuraciones secretas)
- **tqdm** â†’ Muestra barras de progreso bonitas en la terminal

---

## ğŸš€ DESARROLLO POR FASES

### **FASE 1: ConfiguraciÃ³n Inicial** âœ… COMPLETADA
**Objetivo:** Preparar el entorno de trabajo.

**Tareas realizadas:**
- âœ… Crear entorno virtual (`venv/`)
- âœ… Instalar todas las librerÃ­as necesarias
- âœ… Verificar que todo funciona correctamente
- âœ… Documentar el proyecto (este archivo)

---

### **FASE 2: Primer Scraper Funcional** ğŸ¯ SIGUIENTE PASO
**Objetivo:** Crear un scraper simple que funcione.

**Tareas pendientes:**
1. Identificar la primera pÃ¡gina objetivo (ejemplo: gaceta oficial, ministerio especÃ­fico)
2. Crear funciÃ³n bÃ¡sica de descarga con `requests`
3. Extraer enlaces a documentos con `beautifulsoup4`
4. Descargar PDFs encontrados
5. Guardar archivos en `data/` con nombres organizados
6. Crear log de actividad (registro de lo que se descargÃ³)

**Entregable:** Un script que descargue 10 documentos de prueba.

---

### **FASE 3: OCR y ExtracciÃ³n de Texto**
**Objetivo:** Convertir PDFs e imÃ¡genes a texto legible.

**Tareas:**
1. Convertir PDFs a imÃ¡genes con `pdf2image`
2. Aplicar OCR con `pytesseract`
3. Limpiar texto extraÃ­do (quitar caracteres raros)
4. Guardar texto en archivos `.txt`
5. Crear funciÃ³n para detectar idioma (espaÃ±ol)

**Entregable:** Textos extraÃ­dos de los 10 documentos de Fase 2.

---

### **FASE 4: ExtracciÃ³n de Metadatos**
**Objetivo:** Obtener informaciÃ³n sobre cada documento.

**Tareas:**
1. Extraer fecha de publicaciÃ³n
2. Identificar entidad emisora (ministerio, secretarÃ­a, etc.)
3. Detectar tipo de documento (resoluciÃ³n, ley, decreto, etc.)
4. Extraer nÃºmero de documento
5. Crear tabla con todos los metadatos

**Entregable:** Excel con columnas: Fecha | Entidad | Tipo | NÃºmero | Archivo

---

### **FASE 5: Interfaz Streamlit**
**Objetivo:** Crear una aplicaciÃ³n web para controlar el scraper.

**Tareas:**
1. DiseÃ±ar pÃ¡gina principal con tÃ­tulo y descripciÃ³n
2. BotÃ³n "Iniciar Scraping"
3. Mostrar progreso en tiempo real
4. Tabla con documentos descargados
5. BotÃ³n para exportar a Excel
6. GrÃ¡ficos de estadÃ­sticas (documentos por fecha, por entidad)

**Entregable:** AplicaciÃ³n web funcional.

---

### **FASE 6: AutomatizaciÃ³n y Escalabilidad**
**Objetivo:** Hacer el scraper robusto y automÃ¡tico.

**Tareas:**
1. Manejo de errores (si una pÃ¡gina no carga, continuar)
2. Sistema de reintentos automÃ¡ticos
3. Guardar progreso (si se interrumpe, continuar donde quedÃ³)
4. Scraping paralelo (varios documentos a la vez)
5. Programar ejecuciÃ³n automÃ¡tica diaria

**Entregable:** Scraper que corre solo, sin supervisiÃ³n.

---

### **FASE 7: BÃºsqueda y AnÃ¡lisis**
**Objetivo:** Permitir buscar en todos los documentos.

**Tareas:**
1. Crear base de datos SQLite
2. Indexar todos los textos extraÃ­dos
3. FunciÃ³n de bÃºsqueda por palabra clave
4. BÃºsqueda avanzada (por fecha, entidad, tipo)
5. Exportar resultados de bÃºsqueda

**Entregable:** Buscador funcional en la app de Streamlit.

---

## âš™ï¸ CÃ“MO USAR ESTE PROYECTO

### **Activar el entorno virtual**
Cada vez que trabajes en el proyecto, primero debes "activar" el entorno virtual:

```bash
source venv/bin/activate
```

**Â¿CÃ³mo sÃ© que estÃ¡ activado?**
VerÃ¡s `(venv)` al inicio de tu lÃ­nea de comandos:
```
(venv) user@computer:~/bo-gov-scraper-buho$
```

### **Ejecutar el programa principal**
```bash
python main.py
```

### **Ejecutar la aplicaciÃ³n Streamlit**
```bash
streamlit run app/streamlit_app.py
```
Esto abrirÃ¡ una pÃ¡gina web en `http://localhost:8501`

### **Instalar nuevas librerÃ­as**
Si necesitas agregar una librerÃ­a:
```bash
pip install nombre-libreria
pip freeze > requirements.txt  # Actualiza el archivo
```

### **Desactivar el entorno virtual**
Cuando termines de trabajar:
```bash
deactivate
```

---

## ğŸ“ NOTAS IMPORTANTES

1. **NUNCA subas la carpeta `venv/` a GitHub** â†’ Ya estÃ¡ en `.gitignore`
2. **NUNCA subas datos sensibles** â†’ ContraseÃ±as, tokens, claves van en `.env`
3. **Documentos descargados pueden ser grandes** â†’ La carpeta `data/` puede crecer mucho
4. **Respeta los robots.txt** â†’ No todos los sitios permiten scraping
5. **AÃ±ade delays entre requests** â†’ No satures los servidores (usa `time.sleep(1)`)

---

## ğŸ› SOLUCIÃ“N DE PROBLEMAS COMUNES

### **Error: "No module named X"**
**SoluciÃ³n:**
```bash
source venv/bin/activate
pip install -r requirements.txt
```

### **Error: "pytesseract no funciona"**
**SoluciÃ³n:** Necesitas instalar Tesseract OCR en tu sistema:
```bash
# Ubuntu/Debian
sudo apt-get install tesseract-ocr tesseract-ocr-spa

# macOS
brew install tesseract tesseract-lang
```

### **Error: "pdf2image no funciona"**
**SoluciÃ³n:** Necesitas instalar Poppler:
```bash
# Ubuntu/Debian
sudo apt-get install poppler-utils

# macOS
brew install poppler
```

---

## ğŸ“š RECURSOS DE APRENDIZAJE

- **Python BÃ¡sico:** https://docs.python.org/es/3/tutorial/
- **BeautifulSoup:** https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **Pandas:** https://pandas.pydata.org/docs/user_guide/index.html
- **Streamlit:** https://docs.streamlit.io/
- **Web Scraping Ã‰tico:** https://www.scraperapi.com/blog/web-scraping-ethics/

---

## âœ… CHECKLIST PARA COMENZAR FASE 2

Antes de iniciar el desarrollo del primer scraper, verifica:

- [x] Entorno virtual creado y activado
- [x] Todas las librerÃ­as instaladas correctamente
- [x] Archivo `requirements.txt` actualizado
- [x] Estructura de carpetas lista
- [x] `.gitignore` configurado
- [ ] PÃ¡gina objetivo identificada
- [ ] Navegador web abierto para inspeccionar HTML
- [ ] Archivo `main.py` listo para escribir cÃ³digo

---

**Â¡Listo para comenzar! ğŸš€**

_Ãšltima actualizaciÃ³n: 2025-11-18_
