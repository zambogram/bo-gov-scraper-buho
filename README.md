# ğŸ¦‰ BO-GOV-SCRAPER-BUHO

**Scraper completo de pÃ¡ginas del Estado boliviano + OCR + metadatos para BÃšHO**

Sistema integral de scraping, procesamiento y almacenamiento local de normativa legal boliviana, con interfaz web interactiva y CLI potente.

---

## ğŸ“‹ Tabla de Contenidos

- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Uso RÃ¡pido](#-uso-rÃ¡pido)
- [Interfaz Web (Streamlit)](#-interfaz-web-streamlit)
- [CLI](#-cli)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [ConfiguraciÃ³n](#-configuraciÃ³n)
- [Pipeline de Procesamiento](#-pipeline-de-procesamiento)
- [Sitios Soportados](#-sitios-soportados)
- [Desarrollo](#-desarrollo)

---

## âœ¨ CaracterÃ­sticas

### Pipeline Completo
- **Scraping automatizado** de sitios gubernamentales bolivianos
- **ExtracciÃ³n de texto** desde PDFs con soporte para OCR
- **Parsing legal** inteligente: divisiÃ³n automÃ¡tica en artÃ­culos, secciones, capÃ­tulos
- **Delta updates**: procesamiento incremental (solo documentos nuevos)
- **Modo histÃ³rico**: scraping completo de archivos histÃ³ricos

### Almacenamiento Local Controlable
- PDFs originales (opcional)
- Texto normalizado (.txt)
- Estructura JSON con metadatos y artÃ­culos
- Sistema de Ã­ndices para actualizaciones incrementales

### Interfaz Web (Streamlit)
- Control total del pipeline desde la UI
- VisualizaciÃ³n de documentos y artÃ­culos
- EstadÃ­sticas en tiempo real
- Logs de proceso

### CLI Potente
- Scraping por sitio o todos los sitios
- Modo delta o histÃ³rico completo
- Control granular de quÃ© guardar
- EstadÃ­sticas globales

---

## ğŸš€ InstalaciÃ³n

### Requisitos
- Python 3.12+
- Tesseract OCR (opcional, para PDFs escaneados)

### Pasos

1. **Clonar repositorio**
   ```bash
   git clone https://github.com/zambogram/bo-gov-scraper-buho.git
   cd bo-gov-scraper-buho
   ```

2. **Crear entorno virtual**
   ```bash
   python -m venv venv
   source venv/bin/activate  # En Windows: venv\Scripts\activate
   ```

3. **Instalar dependencias**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar (opcional)**
   ```bash
   cp .env.example .env
   # Editar .env segÃºn necesidades
   ```

---

## ğŸ’¡ Uso RÃ¡pido

### Interfaz Web (Streamlit)

```bash
streamlit run app/streamlit_app.py
```

Abre tu navegador en `http://localhost:8501`

### CLI - Ejemplos BÃ¡sicos

```bash
# Listar sitios disponibles
python main.py listar

# Scraping rÃ¡pido de TCP (solo nuevos, 3 documentos)
python main.py scrape tcp --mode delta --limit 3

# Scraping completo de TSJ guardando PDFs
python main.py scrape tsj --mode full --limit 10 --save-pdf

# Ver estadÃ­sticas
python main.py stats
```

---

## ğŸŒ Interfaz Web (Streamlit)

La interfaz web proporciona control total sobre el pipeline de scraping.

### Sidebar: ConfiguraciÃ³n

**1. Sitio**
- Seleccionar sitio a procesar
- Ver informaciÃ³n: tipo, categorÃ­a, prioridad, Ãºltima actualizaciÃ³n

**2. Modo de Scraping**
- **Delta**: Solo documentos nuevos (recomendado)
- **HistÃ³rico completo**: Procesar todo el archivo
- **LÃ­mite por corrida**: CuÃ¡ntos documentos procesar (default: 50)

**3. QuÃ© Guardar**
- [ ] Guardar PDF original
- [x] Guardar texto normalizado (.txt)
- [x] Guardar estructura JSON (.json)

**4. Acciones**
- **Raspar sitio seleccionado**: Ejecutar pipeline para un sitio
- **Raspar TODOS los sitios**: Ejecutar pipeline para todos

### PestaÃ±as Principales

**ğŸ“„ Documentos**
- Tabla de documentos procesados
- Vista previa de texto
- Metadata completa

**ğŸ“‘ ArtÃ­culos**
- Todos los artÃ­culos parseados
- Filtros por tipo y documento
- Vista detallada

**ğŸ“Š EstadÃ­sticas**
- MÃ©tricas globales
- EstadÃ­sticas por sitio
- GrÃ¡ficos interactivos

**ğŸ“ Logs**
- Logs de sesiÃ³n actual en tiempo real
- Logs histÃ³ricos por sitio

---

## ğŸ–¥ï¸ CLI

### Comandos Disponibles

#### `listar` (aliases: `list`, `ls`)
Listar todos los sitios disponibles

```bash
python main.py listar
```

#### `scrape`
Ejecutar scraping de uno o todos los sitios

```bash
# Sintaxis
python main.py scrape [SITIO] [OPCIONES]

# Sitios disponibles
tcp, tsj, asfi, sin, contraloria, all
```

**Opciones:**

| OpciÃ³n | DescripciÃ³n | Default |
|--------|-------------|---------|
| `--mode {full,delta}` | Modo de scraping | `delta` |
| `--limit N` | LÃ­mite de documentos | Sin lÃ­mite |
| `--save-pdf` | Guardar PDFs originales | No guardar |
| `--no-txt` | NO guardar texto | Guardar |
| `--no-json` | NO guardar JSON | Guardar |

**Ejemplos:**

```bash
# Delta update de TCP (solo nuevos, 50 docs)
python main.py scrape tcp --mode delta --limit 50

# HistÃ³rico completo de TSJ con PDFs (20 docs)
python main.py scrape tsj --mode full --limit 20 --save-pdf

# Todos los sitios, delta, 10 docs cada uno
python main.py scrape all --mode delta --limit 10

# Solo JSON, sin texto ni PDFs
python main.py scrape asfi --no-txt
```

#### `stats`
Ver estadÃ­sticas globales

```bash
python main.py stats
```

---

## ğŸ“ Estructura del Proyecto

```
bo-gov-scraper-buho/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py          # Interfaz web Streamlit
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py                # ConfiguraciÃ³n global
â”‚   â””â”€â”€ sites_catalog.yaml         # CatÃ¡logo de sitios
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py                  # Modelos de datos
â”‚   â”œâ”€â”€ pipeline.py                # Pipeline central
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ pdf_extractor.py       # ExtracciÃ³n de texto/OCR
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ legal_parser.py        # Parser legal (artÃ­culos)
â”‚   â””â”€â”€ sites/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_scraper.py        # Scraper base
â”‚       â”œâ”€â”€ tcp_scraper.py
â”‚       â”œâ”€â”€ tsj_scraper.py
â”‚       â”œâ”€â”€ asfi_scraper.py
â”‚       â”œâ”€â”€ sin_scraper.py
â”‚       â””â”€â”€ contraloria_scraper.py
â”œâ”€â”€ data/                          # Datos locales (gitignored)
â”‚   â”œâ”€â”€ raw/{site}/pdfs/           # PDFs sin procesar
â”‚   â”œâ”€â”€ normalized/{site}/text/    # Texto normalizado
â”‚   â”œâ”€â”€ normalized/{site}/json/    # JSON estructurado
â”‚   â””â”€â”€ index/{site}/index.json    # Ãndices delta
â”œâ”€â”€ logs/                          # Logs por sitio
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ PIPELINE_LOCAL.md          # Doc del pipeline
â”œâ”€â”€ main.py                        # CLI principal
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (.env)

```bash
# Directorios base
DATA_BASE_DIR=data
LOGS_DIR=logs
EXPORTS_DIR=exports

# Scraping
MAX_CONCURRENT_DOWNLOADS=3
REQUEST_TIMEOUT=30
RETRY_ATTEMPTS=3

# OCR (opcional)
TESSERACT_PATH=/usr/bin/tesseract
TESSERACT_LANG=spa

# Supabase (opcional)
SUPABASE_URL=
SUPABASE_KEY=
```

### CatÃ¡logo de Sitios

Los sitios se configuran en `config/sites_catalog.yaml`:

```yaml
sites:
  tcp:
    id: tcp
    nombre: "Tribunal Constitucional Plurinacional"
    tipo: "Tribunal"
    url_base: "https://www.tcpbolivia.bo"
    prioridad: 1
    activo: true
    # ... mÃ¡s configuraciÃ³n
```

---

## ğŸ”„ Pipeline de Procesamiento

Ver [docs/PIPELINE_LOCAL.md](docs/PIPELINE_LOCAL.md) para documentaciÃ³n detallada.

### Flujo General

Para cada documento:

1. **Descarga de PDF**
   - Guardar en `data/raw/{site}/pdfs/` (opcional)
   - O usar archivo temporal

2. **ExtracciÃ³n de Texto**
   - PyPDF2 para PDFs digitales
   - Tesseract OCR para PDFs escaneados
   - Guardar en `data/normalized/{site}/text/{id}.txt`

3. **Parsing Legal**
   - Dividir en artÃ­culos, secciones, capÃ­tulos
   - Extraer metadata (nÃºmero, tÃ­tulo, contenido)

4. **Guardado JSON**
   - Estructura completa del documento
   - Array de artÃ­culos con metadata
   - Guardar en `data/normalized/{site}/json/{id}.json`

5. **ActualizaciÃ³n de Ãndice**
   - Actualizar `data/index/{site}/index.json`
   - Hash MD5 para delta updates
   - Rutas a archivos generados

---

## ğŸ›ï¸ Sitios Soportados

| Sitio | ID | Tipo | Prioridad | Estado |
|-------|----|----- |-----------|--------|
| TCP | `tcp` | Tribunal | 1 | âœ… Activo |
| TSJ | `tsj` | Tribunal | 1 | âœ… Activo |
| ASFI | `asfi` | Ente Regulador | 2 | âœ… Activo |
| SIN | `sin` | Ente Tributario | 2 | âœ… Activo |
| ContralorÃ­a | `contraloria` | Control | 2 | âœ… Activo |
| Gaceta Oficial | `gaceta_oficial` | Gaceta | 1 | ğŸ”œ PrÃ³ximamente |

**Nota:** Los scrapers actuales retornan datos de ejemplo. Se debe implementar la lÃ³gica de scraping real para cada sitio segÃºn su estructura web.

---

## ğŸ› ï¸ Desarrollo

### Agregar Nuevo Sitio

1. **Agregar entrada en `config/sites_catalog.yaml`**
2. **Crear scraper en `scraper/sites/{sitio}_scraper.py`**
3. **Heredar de `BaseScraper`**
4. **Implementar mÃ©todos:**
   - `listar_documentos(limite)`
   - `descargar_pdf(url, ruta_destino)`
5. **Registrar en `scraper/sites/__init__.py`**

### Testing

```bash
# Probar CLI
python main.py listar
python main.py scrape tcp --limit 1

# Probar UI
streamlit run app/streamlit_app.py
```

### Logging

Los logs se guardan en:
- `logs/{site}/scrape_{timestamp}.log`
- Stdout/stderr durante ejecuciÃ³n

---

## ğŸ“œ Licencia

[Especificar licencia]

---

## ğŸ‘¥ Contribuidores

Proyecto BÃšHO - Sistema de informaciÃ³n legal boliviano

---

## ğŸ“ Contacto

[InformaciÃ³n de contacto]

---

## ğŸ—ºï¸ Roadmap

- [x] Pipeline completo de scraping local
- [x] Interfaz Streamlit con control total
- [x] CLI robusto
- [x] Sistema de delta updates
- [ ] Scrapers reales para cada sitio
- [ ] Scraper de Gaceta Oficial
- [ ] SincronizaciÃ³n con Supabase
- [ ] API REST
- [ ] Tests automatizados
- [ ] Docker containerization

---

**Ãšltima actualizaciÃ³n:** 2025-11-18
