# ğŸ¦‰ BÃšHO - Motor Multi-sitio de Scraping JurÃ­dico Boliviano

**Sistema completo de captura, procesamiento y exportaciÃ³n de normativa y jurisprudencia de fuentes estatales bolivianas.**

---

## ğŸ“‹ Ãndice

- [DescripciÃ³n](#-descripciÃ³n)
- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [InstalaciÃ³n](#-instalaciÃ³n)
- [Uso RÃ¡pido](#-uso-rÃ¡pido)
- [Comandos CLI](#-comandos-cli)
- [Interfaz Web](#-interfaz-web)
- [CatÃ¡logo de Sitios](#-catÃ¡logo-de-sitios)
- [Arquitectura](#-arquitectura)
- [DocumentaciÃ³n](#-documentaciÃ³n)
- [Roadmap](#-roadmap)

---

## ğŸ¯ DescripciÃ³n

BÃšHO es un **motor multi-sitio de scraping jurÃ­dico** diseÃ±ado especÃ­ficamente para el ecosistema legal boliviano. El sistema:

1. **Scrapea automÃ¡ticamente** sitios estatales (Gaceta Oficial, TCP, TSJ, ASFI, SIN, etc.)
2. **Procesa documentos** (PDFs, HTMLs) extrayendo texto con OCR si es necesario
3. **Segmenta contenido** en unidades Ãºtiles (artÃ­culos, secciones, fundamentos, etc.)
4. **Exporta datos** en formato JSONL listo para Supabase/pgvector
5. **Actualiza incrementalmente** solo documentos nuevos o modificados (delta-update)

### Objetivo

Crear la **base de datos legal mÃ¡s completa de Bolivia** para alimentar aplicaciones LegalTech con bÃºsqueda semÃ¡ntica, RAG, y anÃ¡lisis jurÃ­dico.

---

## âœ¨ CaracterÃ­sticas

### Arquitectura Multi-sitio

- âœ… **CatÃ¡logo centralizado** de todos los sitios estatales bolivianos
- âœ… **Scrapers modulares** por sitio con configuraciÃ³n unificada
- âœ… **Sistema de prioridades** (Ola 1 MVP, Ola 2, Ola 3+)
- âœ… **Delta-update inteligente** (solo procesa lo nuevo)

### Procesamiento Avanzado

- ğŸ“„ **ExtracciÃ³n de PDFs** (texto digital + OCR para escaneados)
- ğŸ” **DetecciÃ³n automÃ¡tica** de tipo de documento (ley, decreto, sentencia, etc.)
- âœ‚ï¸ **SegmentaciÃ³n legal** en artÃ­culos, secciones, fundamentos, etc.
- ğŸ·ï¸ **Metadatos ricos** (tipo_norma, nÃºmero, fecha, fuente, etc.)

### Interfaces

- ğŸ’» **CLI completo** con Rich (tablas, colores, progreso)
- ğŸŒ **UI web con Streamlit** (dashboard, filtros, stats, scraping)
- ğŸ“Š **EstadÃ­sticas en tiempo real** del catÃ¡logo

### ExportaciÃ³n

- ğŸ“¤ **Formato Supabase** (JSONL + schema SQL)
- ğŸ”Œ **Listo para pgvector** (bÃºsqueda semÃ¡ntica)
- ğŸ“ˆ **MÃ©tricas por sitio** (documentos, artÃ­culos, Ãºltima actualizaciÃ³n)

---

## ğŸš€ InstalaciÃ³n

### Requisitos

- Python 3.9+
- pip

### Paso 1: Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/bo-gov-scraper-buho.git
cd bo-gov-scraper-buho
```

### Paso 2: Instalar dependencias

```bash
pip install -r requirements.txt
```

### Paso 3: Verificar instalaciÃ³n

```bash
python main.py --version
python main.py validate
```

---

## âš¡ Uso RÃ¡pido

### 1. Listar todos los sitios catalogados

```bash
python main.py list
```

### 2. Ver solo sitios de Ola 1 (MVP)

```bash
python main.py list --prioridad 1
```

### 3. Ver informaciÃ³n detallada de un sitio

```bash
python main.py info gaceta_oficial
python main.py info tcp
python main.py info asfi
```

### 4. Ver estadÃ­sticas del catÃ¡logo

```bash
python main.py stats
```

### 5. Ejecutar scraping (cuando estÃ© implementado)

```bash
python main.py scrape gaceta_oficial --limit 10
python main.py demo-ola1 --limit 5
```

### 6. Iniciar interfaz web

```bash
streamlit run app/streamlit_app.py
```

---

## ğŸ“‹ Comandos CLI

### `list` - Listar sitios

```bash
# Todos los sitios
python main.py list

# Filtrar por prioridad
python main.py list --prioridad 1
python main.py list --prioridad 2

# Filtrar por estado
python main.py list --estado implementado
python main.py list --estado pendiente

# Filtrar por tipo
python main.py list --tipo normativa
python main.py list --tipo jurisprudencia
python main.py list --tipo regulador

# Filtrar por nivel
python main.py list --nivel nacional
python main.py list --nivel departamental

# Salida JSON
python main.py list --prioridad 1 --json
```

### `info` - InformaciÃ³n detallada

```bash
# Ver detalles de un sitio
python main.py info gaceta_oficial

# Salida JSON
python main.py info tcp --json
```

### `stats` - EstadÃ­sticas

```bash
# EstadÃ­sticas del catÃ¡logo completo
python main.py stats

# Salida JSON
python main.py stats --json
```

### `validate` - Validar catÃ¡logo

```bash
# Verificar integridad del catÃ¡logo
python main.py validate
```

### `scrape` - Ejecutar scraping

```bash
# Scraping limitado
python main.py scrape gaceta_oficial --limit 10

# Scraping completo
python main.py scrape tcp --full

# Forzar re-scraping
python main.py scrape asfi --force
```

### `demo-ola1` - Demo sitios prioritarios

```bash
# Demo de scraping Ola 1
python main.py demo-ola1

# Con lÃ­mite personalizado
python main.py demo-ola1 --limit 5
```

---

## ğŸŒ Interfaz Web

### Iniciar Streamlit

```bash
streamlit run app/streamlit_app.py
```

### CaracterÃ­sticas de la UI

- **Dashboard**: MÃ©tricas generales, distribuciones, sitios Ola 1
- **Sitios**: NavegaciÃ³n con filtros (prioridad, estado, nivel, tipo)
- **EstadÃ­sticas**: Resumen completo, tabla exportable a CSV
- **ConfiguraciÃ³n**: ValidaciÃ³n de catÃ¡logo, rutas del proyecto

### Pantallas

1. **ğŸ  Dashboard** - Vista general con mÃ©tricas clave
2. **ğŸ“‹ Sitios** - CatÃ¡logo completo con filtros y tarjetas expandibles
3. **ğŸ“Š EstadÃ­sticas** - AnÃ¡lisis detallado con exportaciÃ³n CSV
4. **âš™ï¸ ConfiguraciÃ³n** - ValidaciÃ³n y opciones del sistema

---

## ğŸ“š CatÃ¡logo de Sitios

El archivo `config/sites_catalog.yaml` es la **fuente de verdad** del sistema.

### Sitios Ola 1 (MVP - Prioridad 1)

| Site ID | Nombre | Tipo | Estado |
|---------|--------|------|--------|
| `gaceta_oficial` | Gaceta Oficial del Estado Plurinacional | Normativa | â³ Pendiente |
| `tsj_genesis` | Tribunal Supremo de Justicia - GENESIS | Jurisprudencia | â³ Pendiente |
| `tcp` | Tribunal Constitucional Plurinacional | Jurisprudencia | â³ Pendiente |
| `asfi` | Autoridad de SupervisiÃ³n del Sistema Financiero | Regulador | â³ Pendiente |
| `sin` | Servicio de Impuestos Nacionales | Regulador | â³ Pendiente |

### Sitios Ola 2 (Importante - Prioridad 2)

- **contraloria** - ContralorÃ­a General del Estado
- **silep** - Sistema de InformaciÃ³n Legal
- **ait** - Autoridad de ImpugnaciÃ³n Tributaria
- **aps** - Autoridad de Pensiones y Seguros
- **att** - Autoridad de Telecomunicaciones y Transportes

### Sitios Ola 3 (Complementario - Prioridad 3)

- **lexivox** - Compendio Normativo
- **anb** - Aduana Nacional
- **Gacetas departamentales** (9 departamentos)
- **Municipios principales** (La Paz, Santa Cruz, etc.)

**Total catalogados**: 15+ sitios
**ExpansiÃ³n potencial**: 30+ sitios

### Ver catÃ¡logo completo

- Archivo: [`config/sites_catalog.yaml`](config/sites_catalog.yaml)
- DocumentaciÃ³n: [`docs/SITES_CATALOG.md`](docs/SITES_CATALOG.md)

---

## ğŸ—ï¸ Arquitectura

### Estructura del Proyecto

```
bo-gov-scraper-buho/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sites_catalog.yaml        # CatÃ¡logo central de sitios
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ catalog.py                # Gestor del catÃ¡logo
â”‚   â”œâ”€â”€ sites/                    # Scrapers individuales (futuros)
â”‚   â”œâ”€â”€ extractors/               # ExtracciÃ³n texto/OCR (futuro)
â”‚   â”œâ”€â”€ parsers/                  # Parsers legales (futuro)
â”‚   â””â”€â”€ exporters/                # Exportadores (futuro)
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py          # UI web
â”œâ”€â”€ data/                         # Datos descargados
â”œâ”€â”€ exports/                      # Exportaciones JSONL
â”œâ”€â”€ docs/                         # DocumentaciÃ³n
â”œâ”€â”€ main.py                       # CLI principal
â””â”€â”€ requirements.txt              # Dependencias
```

### Flujo de Procesamiento

```
Sitio Web â†’ Scraping â†’ PDF Download â†’ Texto/OCR â†’ Parser Legal â†’ ArtÃ­culos + Metadatos â†’ JSONL â†’ Supabase
```

### Componentes

1. **Catalog Manager** (`scraper/catalog.py`)
   - GestiÃ³n del catÃ¡logo YAML
   - BÃºsquedas y filtros
   - ActualizaciÃ³n de metadatos

2. **CLI** (`main.py`)
   - Comandos interactivos
   - Formateo con Rich
   - GestiÃ³n de scraping

3. **UI Streamlit** (`app/streamlit_app.py`)
   - Dashboard visual
   - NavegaciÃ³n y filtros
   - EstadÃ­sticas y exportaciÃ³n

4. **Scrapers** (prÃ³ximamente)
   - Un mÃ³dulo por sitio
   - ConfiguraciÃ³n desde catÃ¡logo
   - Delta-update automÃ¡tico

---

## ğŸ“– DocumentaciÃ³n

### Documentos Disponibles

- [**README.md**](README.md) - Este archivo (guÃ­a general)
- [**docs/SITES_CATALOG.md**](docs/SITES_CATALOG.md) - GuÃ­a del catÃ¡logo de sitios
- [**docs/USO_PRACTICO.md**](docs/USO_PRACTICO.md) - Tutorial paso a paso
- **docs/ARCHITECTURE.md** (prÃ³ximamente) - Arquitectura tÃ©cnica
- **docs/SCRAPERS.md** (prÃ³ximamente) - GuÃ­a de desarrollo de scrapers

### Ejemplos de Uso

Ver [`docs/USO_PRACTICO.md`](docs/USO_PRACTICO.md) para:
- InstalaciÃ³n completa paso a paso
- Flujos de trabajo recomendados
- Casos de uso reales
- Troubleshooting

---

## ğŸ—ºï¸ Roadmap

### âœ… Fase 1: FundaciÃ³n (COMPLETADO)

- [x] CatÃ¡logo central de sitios con URLs reales
- [x] MÃ³dulo de gestiÃ³n del catÃ¡logo (catalog.py)
- [x] CLI completo con comandos bÃ¡sicos
- [x] UI Streamlit con dashboard y filtros
- [x] DocumentaciÃ³n inicial

### ğŸ”„ Fase 2: Scrapers Ola 1 (EN CURSO)

- [ ] Implementar scraper Gaceta Oficial
- [ ] Implementar scraper TSJ GENESIS
- [ ] Implementar scraper TCP
- [ ] Implementar scraper ASFI
- [ ] Implementar scraper SIN
- [ ] Implementar scraper ContralorÃ­a

### ğŸ“… Fase 3: Procesamiento

- [ ] MÃ³dulo de extracciÃ³n de texto (PyMuPDF + Tesseract)
- [ ] DetecciÃ³n de PDFs escaneados vs digitales
- [ ] Parsers legales (artÃ­culos, sentencias, etc.)
- [ ] Sistema de metadatos rico
- [ ] Delta-update con hashes

### ğŸ“… Fase 4: ExportaciÃ³n

- [ ] Exportador a formato Supabase (JSONL)
- [ ] Schema SQL con pgvector
- [ ] Sync automÃ¡tico con Supabase
- [ ] ValidaciÃ³n de datos exportados

### ğŸ“… Fase 5: Ola 2 y 3

- [ ] Scrapers Ola 2 (SILEP, AIT, APS, ATT)
- [ ] Scrapers Ola 3 (Lexivox, ANB, departamentales)
- [ ] Scrapers municipales

### ğŸ“… Fase 6: ProducciÃ³n

- [ ] Scheduler automÃ¡tico (cron/Airflow)
- [ ] Monitoreo y alertas
- [ ] Logs estructurados
- [ ] Tests completos (pytest)
- [ ] CI/CD
- [ ] DockerizaciÃ³n

---

## ğŸ› ï¸ TecnologÃ­as

### Core

- **Python 3.9+** - Lenguaje principal
- **PyYAML** - GestiÃ³n del catÃ¡logo
- **Click + Rich** - CLI interactivo
- **Streamlit** - UI web

### Scraping (futuro)

- **Requests** - HTTP requests
- **BeautifulSoup** - Parsing HTML
- **Selenium** - Sitios dinÃ¡micos
- **lxml** - Procesamiento XML/HTML rÃ¡pido

### Procesamiento PDF (futuro)

- **PyMuPDF** - ExtracciÃ³n de texto
- **pdfplumber** - AnÃ¡lisis de estructura
- **Tesseract** - OCR para PDFs escaneados
- **pytesseract** - Wrapper Python

### Datos

- **Pandas** - ManipulaciÃ³n de datos
- **SQLAlchemy** - ORM (futuro)
- **Supabase** - Base de datos (externo)

---

## ğŸ¤ Contribuir

Este es un proyecto interno de BÃšHO LegalTech. Para consultas:
- Contacto: [tu-email]
- DocumentaciÃ³n: ver carpeta `docs/`

---

## ğŸ“„ Licencia

Propietario - BÃšHO LegalTech Bolivia Â© 2025

---

## ğŸ¦‰ Sobre BÃšHO

**BÃšHO** es una LegalTech boliviana enfocada en democratizar el acceso a la informaciÃ³n jurÃ­dica mediante tecnologÃ­a de punta (IA, RAG, bÃºsqueda semÃ¡ntica).

### VisiÃ³n

Crear la **base de datos legal mÃ¡s completa de Bolivia** y hacerla accesible para abogados, empresas, ciudadanos y desarrolladores.

### MisiÃ³n

Transformar la prÃ¡ctica legal boliviana mediante herramientas tecnolÃ³gicas que reduzcan costos, aumenten eficiencia y mejoren el acceso a la justicia.

---

**Hecho con â¤ï¸ en Bolivia ğŸ‡§ğŸ‡´**
