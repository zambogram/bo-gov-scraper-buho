# ğŸ¦‰ BÃšHO - Scraper Completo de Leyes Bolivianas

Sistema profesional de scraping, procesamiento, normalizaciÃ³n y exportaciÃ³n de documentos legales del Estado Plurinacional de Bolivia.

## ğŸ“‹ CaracterÃ­sticas Principales

### âœ¨ Funcionalidades Completas

- **Scraping Multi-Sitio**: Scrapea simultÃ¡neamente +33 sitios del gobierno boliviano
- **Scraping con Selenium**: Scraper especializado para TCP Jurisprudencia con navegaciÃ³n dinÃ¡mica
- **Procesamiento Inteligente**: PDF, DOC, DOCX e imÃ¡genes con OCR automÃ¡tico
- **ExtracciÃ³n de Metadatos**: Extrae automÃ¡ticamente nÃºmero de ley, Ã¡rea del derecho, fechas, etc.
- **DivisiÃ³n de PDFs**: Divide PDFs grandes en secciones manejables
- **Base de Datos SQLite**: Registro histÃ³rico completo con bÃºsqueda y estadÃ­sticas
- **ExportaciÃ³n MÃºltiple**: CSV, JSON, Excel con todos los metadatos
- **NormalizaciÃ³n**: Convierte todos los documentos a PDF con texto buscable
- **Interfaz Web**: Dashboard Streamlit para visualizaciÃ³n y gestiÃ³n

### ğŸ¯ Sitios Gubernamentales Soportados

El sistema estÃ¡ configurado para scrapear 33+ sitios incluyendo:

- Gaceta Oficial de Bolivia
- Asamblea Legislativa Plurinacional
- **Tribunal Constitucional Plurinacional** (con Selenium para jurisprudencia completa)
- Ministerios (Justicia, EconomÃ­a, Trabajo, Salud, etc.)
- Ã“rgano Judicial
- ContralorÃ­a General del Estado
- Autoridades Regulatorias (ASFI, SIN, Aduana, etc.)
- Y muchos mÃ¡s...

## ğŸš€ InstalaciÃ³n

### Requisitos Previos

- Python 3.8+
- Tesseract OCR (para reconocimiento de texto en imÃ¡genes)
- LibreOffice (opcional, para convertir DOC a PDF)
- Chrome/Chromium (para scraping del TCP con Selenium)

### InstalaciÃ³n de Tesseract

**Ubuntu/Debian:**
```bash
sudo apt-get update
sudo apt-get install tesseract-ocr tesseract-ocr-spa
```

**macOS:**
```bash
brew install tesseract tesseract-lang
```

**Windows:**
Descargar desde: https://github.com/UB-Mannheim/tesseract/wiki

### InstalaciÃ³n del Proyecto

1. Clonar el repositorio:
```bash
git clone https://github.com/zambogram/bo-gov-scraper-buho.git
cd bo-gov-scraper-buho
```

2. Crear entorno virtual:
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. Instalar dependencias:
```bash
pip install -r requirements.txt
```

## ğŸ“– Uso

### Modo CLI (LÃ­nea de Comandos)

El sistema ofrece mÃºltiples comandos para diferentes operaciones:

#### 1. Scraping de Sitios Web

Scrapea todos los sitios configurados y descarga documentos:

```bash
python main.py --scrapear --workers 5
```

#### 1.1. Scraping del TCP (Tribunal Constitucional) con Selenium

Scrapea jurisprudencia del TCP usando Selenium para navegaciÃ³n dinÃ¡mica:

```bash
# Solo scraping del TCP
python main.py --tcp

# Flujo completo del TCP (scrapear + procesar + exportar)
python main.py --tcp-completo --ocr
```

El scraper del TCP extrae:
- NÃºmero de resoluciÃ³n
- Tipo de jurisprudencia
- Tipo resolutivo
- Fecha
- Sumilla completa
- Magistrados
- Ãrea/materia
- PDFs de sentencias

Ver documentaciÃ³n completa en: `scraper/sites/README_TCP.md`

#### 2. Procesamiento de Documentos

Procesa los documentos descargados, extrae texto y metadatos:

```bash
python main.py --procesar --ocr --dividir-pdfs
```

Opciones:
- `--ocr`: Aplica OCR a documentos escaneados
- `--dividir-pdfs`: Divide PDFs grandes en secciones

#### 3. ExportaciÃ³n de Datos

Exporta los datos a diferentes formatos:

```bash
python main.py --exportar --formato csv json excel
```

#### 4. Ver EstadÃ­sticas

Muestra estadÃ­sticas completas del sistema:

```bash
python main.py --stats
```

#### 5. Flujo Completo

Ejecuta todo el proceso (scrapear + procesar + exportar):

```bash
python main.py --completo --workers 5 --ocr --dividir-pdfs
```

### Modo Interfaz Web (Streamlit)

Lanza la interfaz web interactiva:

```bash
streamlit run app/streamlit_app.py
```

## ğŸ“Š Estructura de Datos

### Metadatos ExtraÃ­dos

Para cada documento legal, el sistema extrae y almacena:

**IdentificaciÃ³n:**
- NÃºmero de ley (ej: "Ley 1178")
- Tipo de norma (Ley, Decreto Supremo, ResoluciÃ³n, etc.)
- TÃ­tulo completo
- CÃ³digo Ãºnico (hash SHA256)

**ClasificaciÃ³n JurÃ­dica:**
- Ãrea del derecho (Constitucional, Penal, Laboral, etc.)
- JerarquÃ­a normativa
- Materias y palabras clave

**InformaciÃ³n Temporal:**
- Fecha de promulgaciÃ³n
- Fecha de publicaciÃ³n
- Fecha de vigencia
- Vigencia actual (sÃ­/no)

**Origen y Fuente:**
- Ã“rgano emisor
- Firmante
- URL de origen
- Sitio web fuente
- Fecha de scraping

**Documento:**
- Formato original (PDF, DOC, etc.)
- TamaÃ±o en bytes
- NÃºmero de pÃ¡ginas
- Hashes MD5 y SHA256
- Rutas de archivos

**Procesamiento:**
- OCR aplicado (sÃ­/no)
- Confianza del OCR (0-1)
- Idioma detectado
- Texto completo extraÃ­do
- Estado de procesamiento

**Contenido:**
- ArtÃ­culos principales
- Total de artÃ­culos
- Total de palabras y caracteres
- Relaciones con otras leyes

## ğŸ—‚ï¸ Estructura del Proyecto

```
bo-gov-scraper-buho/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ sites_config.yaml          # ConfiguraciÃ³n de 33+ sitios
â”‚   â””â”€â”€ metadata_schema.yaml       # Esquema de metadatos
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ multi_site_scraper.py      # Scraper principal
â”‚   â”œâ”€â”€ sites/                     # Scrapers especializados
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tcp_jurisprudencia_scraper.py  # Scraper TCP con Selenium
â”‚   â”‚   â””â”€â”€ README_TCP.md          # DocumentaciÃ³n del TCP scraper
â”‚   â”œâ”€â”€ document_processor.py      # Procesador con OCR
â”‚   â”œâ”€â”€ metadata.py                # Extractor de metadatos
â”‚   â”œâ”€â”€ pdf_splitter.py            # Divisor de PDFs
â”‚   â””â”€â”€ database.py                # Gestor de BD SQLite
â”œâ”€â”€ exporters/
â”‚   â”œâ”€â”€ csv_exporter.py
â”‚   â”œâ”€â”€ json_exporter.py
â”‚   â””â”€â”€ excel_exporter.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Documentos originales
â”‚   â”‚   â””â”€â”€ tcp_jurisprudencia/    # Sentencias del TCP
â”‚   â”œâ”€â”€ processed/                 # Documentos procesados
â”‚   â””â”€â”€ laws.db                    # Base de datos SQLite
â”œâ”€â”€ exports/                        # Exportaciones
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py           # Interfaz web
â”œâ”€â”€ main.py                         # Script principal
â”œâ”€â”€ requirements.txt                # Dependencias
â””â”€â”€ README.md                       # Este archivo
```

## ğŸ”§ ConfiguraciÃ³n

### Configurar Sitios Web

Edita `config/sites_config.yaml` para:
- Habilitar/deshabilitar sitios
- Ajustar prioridades
- Configurar selectores CSS personalizados
- Modificar parÃ¡metros de scraping

### Configurar Metadatos

Edita `config/metadata_schema.yaml` para:
- Agregar nuevos campos de metadatos
- Modificar patrones de extracciÃ³n
- Definir Ã¡reas del derecho adicionales

## ğŸ’¾ Base de Datos

### Esquema de Tablas

**leyes**: Tabla principal con todos los metadatos
**historial_scraping**: Registro de cada sesiÃ³n de scraping
**estadisticas_globales**: EstadÃ­sticas agregadas
**areas_derecho**: CatÃ¡logo de Ã¡reas del derecho

### Consultas de Ejemplo

```python
from scraper.database import LawDatabase

with LawDatabase() as db:
    # Buscar leyes por Ã¡rea
    leyes_laborales = db.buscar_ley(area_derecho="Laboral")

    # Buscar leyes vigentes
    leyes_vigentes = db.buscar_ley(vigente=True)

    # Obtener estadÃ­sticas
    stats = db.obtener_estadisticas()

    # Exportar a CSV
    db.exportar_a_csv("exports/leyes.csv")
```

## ğŸ“¤ Formatos de ExportaciÃ³n

### CSV
Archivo plano compatible con Excel, ideal para anÃ¡lisis de datos

### JSON
Formato estructurado con todos los metadatos, ideal para APIs

### Excel
Archivo .xlsx con formato, ideal para reportes

## ğŸ” Ejemplos de Uso

### Ejemplo 1: Scrapear solo sitios especÃ­ficos

```python
from scraper.multi_site_scraper import MultiSiteScraper

scraper = MultiSiteScraper()
# Modificar config para habilitar solo ciertos sitios
resultados = scraper.scrapear_todos_los_sitios(max_workers=3)
```

### Ejemplo 2: Procesar un documento especÃ­fico

```python
from scraper.document_processor import DocumentProcessor
from scraper.metadata import MetadataExtractor

processor = DocumentProcessor()
extractor = MetadataExtractor()

# Procesar documento
resultado = processor.procesar_documento("mi_ley.pdf")

# Extraer metadatos
metadatos = extractor.extraer_metadatos(
    resultado['texto'],
    archivo_path="mi_ley.pdf",
    sitio_web="Gaceta Oficial",
    url_origen="https://..."
)
```

### Ejemplo 3: Dividir un PDF grande

```python
from scraper.pdf_splitter import PDFSplitter

splitter = PDFSplitter()

# Dividir PDF
archivos = splitter.dividir_pdf(
    "ley_grande.pdf",
    max_paginas_por_seccion=30,
    dividir_por_estructura=True
)

# Agregar metadatos a cada secciÃ³n
for archivo in archivos:
    splitter.agregar_metadatos_a_seccion(archivo, metadatos)
```

## ğŸ“ˆ Flujo de Trabajo Recomendado

### Para Scraping Completo (Primera Vez)

```bash
# 1. Scrapear todos los sitios (puede tardar horas)
python main.py --scrapear --workers 5

# 2. Scrapear el TCP con Selenium (puede tardar varias horas)
python main.py --tcp

# 3. Procesar documentos con OCR y divisiÃ³n
python main.py --procesar --ocr --dividir-pdfs

# 4. Exportar a todos los formatos
python main.py --exportar --formato csv json excel

# 5. Ver estadÃ­sticas
python main.py --stats
```

### Para Scraping Solo del TCP

```bash
# Flujo completo del TCP en un solo comando
python main.py --tcp-completo --ocr --dividir-pdfs
```

### Para Actualizaciones PeriÃ³dicas

```bash
# Flujo completo con menos hilos (mÃ¡s respetuoso)
python main.py --completo --workers 3 --ocr
```

## âš™ï¸ OptimizaciÃ³n y Rendimiento

### Scraping
- **Concurrent workers**: Ajusta `--workers` segÃºn tu conexiÃ³n (3-10)
- **Delay entre requests**: Configurado en 2 segundos por defecto
- **Retry attempts**: 3 intentos automÃ¡ticos por defecto

### Procesamiento
- OCR puede ser lento: desactiva con `--ocr` si no es necesario
- DivisiÃ³n de PDFs: solo para documentos >50 pÃ¡ginas por defecto

### Base de Datos
- Ãndices automÃ¡ticos en campos clave
- Consultas optimizadas
- Backups automÃ¡ticos recomendados

## ğŸ› SoluciÃ³n de Problemas

### Error: "Tesseract not found"
Instala Tesseract OCR y agrega al PATH del sistema

### Error: "LibreOffice not found"
La conversiÃ³n DOCâ†’PDF requiere LibreOffice (opcional)

### PDFs sin texto extraÃ­do
Activa `--ocr` para aplicar OCR a documentos escaneados

### Sitio web no responde
Algunos sitios pueden estar temporalmente caÃ­dos o bloquear bots

## ğŸ“œ Licencia

Este proyecto estÃ¡ bajo licencia MIT. Ver archivo LICENSE para mÃ¡s detalles.

## ğŸ‘¥ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“§ Contacto

Para preguntas, sugerencias o reportar problemas, abre un issue en GitHub.

## ğŸ™ Agradecimientos

- A todos los sitios gubernamentales bolivianos por hacer pÃºblica la informaciÃ³n legal
- A la comunidad de cÃ³digo abierto por las excelentes bibliotecas utilizadas

---

**ğŸ¦‰ BÃšHO** - Scraper Profesional de Leyes Bolivianas | 2024
