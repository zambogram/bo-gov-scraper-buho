# ğŸ¦‰ BÃšHO - Sistema de Scraping Gubernamental de Bolivia

**FASE 9: Sites Reales + Parsers Avanzados + Delta-Update**

Sistema completo de scraping, parsing avanzado y actualizaciÃ³n incremental de documentos legales de instituciones gubernamentales bolivianas.

## ğŸŒŸ CaracterÃ­sticas

- âœ… **5 Scrapers de sitios reales** del Estado Boliviano
- âœ… **Parsing avanzado de PDFs** con detecciÃ³n automÃ¡tica de OCR
- âœ… **Sistema de actualizaciÃ³n incremental** (Delta-Update)
- âœ… **CLI completo** con mÃºltiples comandos
- âœ… **Arquitectura multisite** extensible
- âœ… **GestiÃ³n de Ã­ndices** con hash MD5
- âœ… **EstadÃ­sticas detalladas** por sitio

## ğŸ›ï¸ Sitios Soportados

| CÃ³digo | InstituciÃ³n | Documentos |
|--------|-------------|------------|
| `tcp` | Tribunal Constitucional Plurinacional | SC, SCP, SCA |
| `tsj` | Tribunal Supremo de Justicia | Autos Supremos |
| `contraloria` | ContralorÃ­a General del Estado | Resoluciones |
| `asfi` | ASFI | Resoluciones, Circulares, Comunicados |
| `sin` | Servicio de Impuestos Nacionales | RND, RA, RM |

## ğŸš€ InstalaciÃ³n RÃ¡pida

```bash
# Clonar repositorio
git clone https://github.com/zambogram/bo-gov-scraper-buho.git
cd bo-gov-scraper-buho

# Instalar dependencias
pip install -r requirements.txt

# Opcional: Instalar Tesseract para OCR
# Ubuntu/Debian:
sudo apt-get install tesseract-ocr tesseract-ocr-spa

# Verificar instalaciÃ³n
python main.py --version
```

## ğŸ“– Uso

### Comandos BÃ¡sicos

```bash
# Listar sitios disponibles
python main.py listar

# Scraping de un sitio especÃ­fico
python main.py scrape tcp

# Actualizar todos los sitios
python main.py actualizar-todos

# Ver estadÃ­sticas
python main.py estadisticas tcp

# Resumen general
python main.py resumen
```

### Opciones Avanzadas

```bash
# Solo documentos nuevos
python main.py scrape tcp --solo-nuevos

# Solo documentos modificados
python main.py scrape tsj --solo-modificados

# Limitar cantidad
python main.py scrape asfi --limit 10

# Actualizar sitios especÃ­ficos
python main.py actualizar-todos --sitios tcp,tsj,asfi

# Limpiar Ã­ndice
python main.py limpiar-index tcp
```

## ğŸ“Š Sistema Delta-Update

El sistema mantiene un Ã­ndice incremental que:

- âœ… Evita descargas duplicadas
- âœ… Detecta documentos nuevos automÃ¡ticamente
- âœ… Identifica modificaciones por hash MD5
- âœ… Mantiene estadÃ­sticas histÃ³ricas
- âœ… Optimiza uso de recursos

### Estructura de Salidas

```
outputs/
â”œâ”€â”€ tcp/
â”‚   â”œâ”€â”€ index.json          # Ãndice incremental
â”‚   â”œâ”€â”€ pdfs/               # PDFs descargados
â”‚   â”‚   â”œâ”€â”€ SCP_0001_2024.pdf
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ json/               # Documentos parseados
â”‚       â”œâ”€â”€ SCP_0001_2024.json
â”‚       â””â”€â”€ ...
â”œâ”€â”€ tsj/
â”œâ”€â”€ contraloria/
â”œâ”€â”€ asfi/
â””â”€â”€ sin/
```

## ğŸ”§ Arquitectura

```
scraper/
â”œâ”€â”€ core/                   # MÃ³dulos base
â”‚   â”œâ”€â”€ base_scraper.py    # Clase abstracta
â”‚   â”œâ”€â”€ pdf_parser.py      # Parser con OCR
â”‚   â”œâ”€â”€ delta_manager.py   # Delta-Update
â”‚   â””â”€â”€ utils.py           # Utilidades
â”‚
â””â”€â”€ sites/                  # Scrapers especÃ­ficos
    â”œâ”€â”€ tcp_scraper.py
    â”œâ”€â”€ tsj_scraper.py
    â”œâ”€â”€ contraloria_scraper.py
    â”œâ”€â”€ asfi_scraper.py
    â””â”€â”€ sin_scraper.py
```

## ğŸ“„ Parsing de PDFs

El sistema parsea automÃ¡ticamente las secciones especÃ­ficas de cada tipo de documento:

### Tribunal Constitucional (TCP)
- VISTOS
- ANTECEDENTES
- PROBLEMÃTICA
- CONSIDERANDO
- FUNDAMENTOS JURÃDICOS
- POR TANTO

### Tribunal Supremo (TSJ)
- RESULTANDOS
- CONSIDERANDOS
- PARTE RESOLUTIVA

### Otros Sitios
- Estructura por artÃ­culos
- Numerales romanos
- Metadata especÃ­fica

## ğŸ”Œ API ProgramÃ¡tica

```python
from scraper import get_scraper

# Obtener scraper
scraper = get_scraper('tcp')

# Ejecutar scraping
stats = scraper.run(only_new=True, limit=10)

# Resultados
print(f"Nuevos: {stats['total_new']}")
print(f"Modificados: {stats['total_modified']}")
```

## ğŸ“š DocumentaciÃ³n

Ver documentaciÃ³n completa en:
- [FASE9_SITES_REALES.md](docs/FASE9_SITES_REALES.md)

## ğŸ§ª Ejemplos

### Scraping Completo

```bash
# Primera vez: scraping completo
python main.py actualizar-todos --limit 50

# Actualizaciones diarias: solo nuevos
python main.py actualizar-todos --solo-nuevos

# Verificar cambios: solo modificados
python main.py actualizar-todos --solo-modificados
```

### Uso ProgramÃ¡tico

```python
from scraper import TCPScraper

# Crear scraper
tcp = TCPScraper()

# Ejecutar
stats = tcp.run(only_new=True)

# Acceder al Ã­ndice
docs = tcp.delta_manager.get_all_document_ids()
print(f"Total documentos: {len(docs)}")
```

## ğŸ› Troubleshooting

### Error de OCR
```bash
# Verificar Tesseract
tesseract --version
tesseract --list-langs | grep spa
```

### Documentos duplicados
```bash
# Limpiar Ã­ndice
python main.py limpiar-index tcp

# Usar flags apropiados
python main.py scrape tcp --solo-nuevos
```

## ğŸ¯ Roadmap

- [ ] Soporte para mÃ¡s sitios gubernamentales
- [ ] API REST
- [ ] Dashboard web
- [ ] BÃºsqueda full-text
- [ ] AnÃ¡lisis con NLP
- [ ] Notificaciones automÃ¡ticas
- [ ] ExportaciÃ³n a mÃºltiples formatos

## ğŸ“Š EstadÃ­sticas del Proyecto

- **5 Sitios** gubernamentales
- **3 Tipos** de parsers especializados
- **7+ Formatos** de documentos soportados
- **100% Python** con arquitectura modular

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/nuevo-sitio`)
3. Commit cambios (`git commit -am 'Agregar nuevo sitio'`)
4. Push a la rama (`git push origin feature/nuevo-sitio`)
5. Crear Pull Request

## ğŸ“ Licencia

Este proyecto es parte del sistema BÃšHO de scraping gubernamental.

## ğŸ‘¥ Autor

Proyecto BÃšHO - Sistema de Scraping Gubernamental de Bolivia

---

**VersiÃ³n 9.0.0** - FASE 9 Completa
Enero 2025
