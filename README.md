# ğŸ‡§ğŸ‡´ Scraper de Sitios Gubernamentales de Bolivia

Sistema integral de scraping y gestiÃ³n de documentos legales y normativos de sitios web gubernamentales de Bolivia.

## ğŸ¯ DescripciÃ³n

Este proyecto permite recolectar, organizar y gestionar documentaciÃ³n oficial del Estado Plurinacional de Bolivia, incluyendo leyes, decretos, resoluciones, sentencias y normativa regulatoria de diferentes instituciones.

## ğŸš€ Estado del Proyecto

### FASE 2 - OLA 1: SCRAPERS IMPLEMENTADOS âœ…

Los siguientes sitios de prioridad mÃ¡xima estÃ¡n completamente implementados:

- âœ… **Gaceta Oficial de Bolivia** - Leyes, decretos y resoluciones
- âœ… **TSJ GENESIS** - Jurisprudencia del Tribunal Supremo
- âœ… **TCP** - Sentencias del Tribunal Constitucional Plurinacional
- âœ… **ASFI** - Normativa del sistema financiero
- âœ… **SIN** - Normativa tributaria

## ğŸ“‹ CaracterÃ­sticas

- **CatÃ¡logo centralizado** de sitios gubernamentales con configuraciÃ³n YAML
- **Scrapers modulares** con interfaz comÃºn y fÃ¡cil extensiÃ³n
- **Sistema de Ã­ndices** para evitar duplicados y tracking de cambios
- **CLI completo** para operaciones desde lÃ­nea de comandos
- **Interfaz web** con Streamlit para uso visual
- **Modo demo** para pruebas sin conexiÃ³n a sitios reales
- **Logging detallado** de todas las operaciones

## ğŸ“¦ InstalaciÃ³n

### Requisitos

- Python 3.8 o superior
- pip

### Pasos

```bash
# Clonar el repositorio
git clone <repo-url>
cd bo-gov-scraper-buho

# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

## ğŸ® Uso

### Interfaz de LÃ­nea de Comandos (CLI)

#### Listar sitios del catÃ¡logo

```bash
# Listar todos los sitios
python main.py list

# Filtrar por prioridad
python main.py list --prioridad 1

# Filtrar por categorÃ­a
python main.py list --categoria judicial

# Filtrar por estado del scraper
python main.py list --estado implementado
```

#### Ejecutar scraping de un sitio

```bash
# Scraping bÃ¡sico (modo demo)
python main.py scrape gaceta_oficial --limit 10 --demo

# Scraping con lÃ­mite
python main.py scrape tcp --limit 5

# Scraping sin lÃ­mite, solo nuevos
python main.py scrape asfi --solo-nuevos

# Scraping sin filtro de nuevos
python main.py scrape sin --limit 20
```

#### Demo de la Ola 1 completa

```bash
# Ejecutar demo de todos los sitios de la Ola 1
python main.py demo-ola1 --limit 5
```

#### Ver estadÃ­sticas

```bash
# Mostrar estadÃ­sticas de todos los sitios scrapeados
python main.py stats
```

### Interfaz Web (Streamlit)

```bash
# Iniciar la aplicaciÃ³n web
streamlit run app/streamlit_app.py
```

La interfaz web incluye:
- **Dashboard**: Vista general de scrapers y estadÃ­sticas
- **CatÃ¡logo de Sitios**: ExploraciÃ³n con filtros
- **Ejecutar Scraping**: Interfaz visual para ejecutar scrapers
- **EstadÃ­sticas**: AnÃ¡lisis detallado de documentos recolectados
- **Ayuda**: DocumentaciÃ³n integrada

## ğŸ“ Estructura del Proyecto

```
bo-gov-scraper-buho/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sites_catalog.yaml          # CatÃ¡logo de sitios
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ catalog.py                  # Gestor del catÃ¡logo
â”‚   â”œâ”€â”€ base.py                     # Clase base de scrapers
â”‚   â””â”€â”€ sites/                      # Scrapers por sitio
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ gaceta_oficial.py
â”‚       â”œâ”€â”€ tsj_genesis.py
â”‚       â”œâ”€â”€ tcp.py
â”‚       â”œâ”€â”€ asfi.py
â”‚       â””â”€â”€ sin.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Documentos scrapeados
â”‚   â”‚   â””â”€â”€ <site_id>/
â”‚   â”‚       â”œâ”€â”€ documentos.json     # Datos de documentos
â”‚   â”‚       â””â”€â”€ pdfs/               # PDFs descargados
â”‚   â””â”€â”€ index/                      # Ãndices de tracking
â”‚       â””â”€â”€ <site_id>.json
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py            # Interfaz web
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ USO_PRACTICO.md             # GuÃ­a de uso detallada
â”œâ”€â”€ main.py                         # CLI principal
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ ConfiguraciÃ³n

### CatÃ¡logo de Sitios

El archivo `config/sites_catalog.yaml` contiene la configuraciÃ³n de todos los sitios:

```yaml
sitios:
  gaceta_oficial:
    nombre: "Gaceta Oficial de Bolivia"
    prioridad: 1
    url_base: "http://www.gacetaoficialdebolivia.gob.bo"
    estado_scraper: "implementado"
    # ... mÃ¡s configuraciÃ³n
```

### Agregar un Nuevo Sitio

1. Agregar configuraciÃ³n en `config/sites_catalog.yaml`
2. Crear scraper en `scraper/sites/nuevo_sitio.py` heredando de `BaseSiteScraper`
3. Registrar en `scraper/sites/__init__.py`
4. Implementar mÃ©todo `scrape()`

## ğŸ“Š Datos Generados

### Documentos Scrapeados

Cada documento tiene la siguiente estructura:

```json
{
  "site_id": "gaceta_oficial",
  "document_id": "GACETA_OFICIAL-LEY-1234-20250115",
  "titulo": "Ley 1234 - Ley de ...",
  "tipo_norma": "Ley",
  "numero_norma": "1234",
  "fecha_publicacion": "2025-01-15",
  "url_detalle": "http://...",
  "url_pdf": "http://.../pdf",
  "path_pdf": "data/raw/gaceta_oficial/pdfs/...",
  "hash_contenido": "md5hash...",
  "estado": "nuevo",
  "fecha_scraping": "2025-01-18T10:30:00"
}
```

### Ãndices

Los Ã­ndices en `data/index/<site_id>.json` rastrean documentos para evitar duplicados:

```json
{
  "GACETA_OFICIAL-LEY-1234-20250115": {
    "hash": "md5hash...",
    "titulo": "Ley 1234...",
    "fecha_publicacion": "2025-01-15",
    "fecha_ultima_vez_visto": "2025-01-18T10:30:00",
    "estado": "nuevo",
    "url_pdf": "http://..."
  }
}
```

## ğŸ§ª Modo Demo

Todos los scrapers soportan un modo demo que genera datos realistas sin conectarse a sitios reales:

```bash
# CLI
python main.py scrape gaceta_oficial --limit 10 --demo

# En cÃ³digo
scraper = GacetaOficialScraper(site_config, modo_demo=True)
```

Ãštil para:
- Desarrollo y pruebas
- Demos y presentaciones
- Entornos sin conexiÃ³n a internet

## ğŸ›£ï¸ PrÃ³ximas Fases

### Fase 3: Procesamiento de Texto
- ExtracciÃ³n de texto de PDFs
- OCR para documentos escaneados
- Parsers legales especializados
- ExtracciÃ³n de metadatos avanzados

### Fase 4: IntegraciÃ³n con Supabase
- Sync automÃ¡tico con base de datos
- API REST para consultas
- Sistema de notificaciones
- Dashboard analytics

## ğŸ“ Logs

Los logs se guardan en `scraper.log` y tambiÃ©n se muestran en consola.

Nivel de logging configurable en `main.py`:

```python
logging.basicConfig(level=logging.INFO)  # DEBUG, INFO, WARNING, ERROR
```

## ğŸ¤ Contribuir

Para agregar un nuevo sitio o mejorar scrapers existentes:

1. Fork el repositorio
2. Crea una rama para tu feature
3. Implementa cambios siguiendo la estructura existente
4. Prueba con modo demo
5. EnvÃ­a un pull request

## ğŸ“„ Licencia

[Especificar licencia]

## ğŸ†˜ Soporte

Para problemas o preguntas:
- Revisa `docs/USO_PRACTICO.md`
- Consulta los logs en `scraper.log`
- Abre un issue en el repositorio

---

**VersiÃ³n**: Fase 2 - Ola 1
**Ãšltima actualizaciÃ³n**: 2025-01-18
**Estado**: ProducciÃ³n
