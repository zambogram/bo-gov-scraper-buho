# ğŸ¦‰ BÃšHO - Bolivian Government Document Scraper

Scraper completo de pÃ¡ginas del Estado boliviano con OCR, metadatos, parsing legal y sincronizaciÃ³n con Supabase.

## ğŸŒŸ CaracterÃ­sticas

### FASE 10 - ImplementaciÃ³n Completa

- âœ… **Scrapers MÃºltiples**: TCP, TSJ, ASFI, SIN, ContralorÃ­a
- âœ… **Parsing Legal AutomÃ¡tico**: ExtracciÃ³n de artÃ­culos y estructura legal
- âœ… **Delta Update**: DetecciÃ³n de documentos nuevos y modificados por MD5
- âœ… **Interfaz Web Streamlit**: UI completa para gestiÃ³n y visualizaciÃ³n
- âœ… **ExportaciÃ³n JSONL**: ExportaciÃ³n automÃ¡tica de documentos y artÃ­culos
- âœ… **SincronizaciÃ³n Supabase**: Sync bidireccional con base de datos
- âœ… **Scheduler AutomÃ¡tico**: Scraping diario automatizado
- âœ… **Logs Detallados**: Sistema completo de logging

## ğŸ—ï¸ Arquitectura del Proyecto

```
bo-gov-scraper-buho/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py          # Interfaz web Streamlit
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ base_scraper.py           # Clase base para scrapers
â”‚   â”œâ”€â”€ tcp_scraper.py            # Scraper TCP
â”‚   â”œâ”€â”€ tsj_scraper.py            # Scraper TSJ
â”‚   â”œâ”€â”€ asfi_scraper.py           # Scraper ASFI
â”‚   â”œâ”€â”€ sin_scraper.py            # Scraper SIN
â”‚   â”œâ”€â”€ contraloria_scraper.py    # Scraper ContralorÃ­a
â”‚   â”œâ”€â”€ parser.py                 # Parser legal
â”‚   â””â”€â”€ metadata.py               # Extractor de metadatos
â”œâ”€â”€ sync/
â”‚   â””â”€â”€ supabase_sync.py          # SincronizaciÃ³n Supabase
â”œâ”€â”€ scheduler/
â”‚   â””â”€â”€ run_daily.py              # Scheduler automÃ¡tico
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ index/                    # Ãndices JSON
â”‚   â””â”€â”€ articles/                 # ArtÃ­culos parseados
â”œâ”€â”€ exports/                      # Exportaciones JSONL
â”œâ”€â”€ logs/                         # Logs de operaciones
â”œâ”€â”€ docs/                         # DocumentaciÃ³n
â”œâ”€â”€ main.py                       # CLI principal
â””â”€â”€ requirements.txt              # Dependencias
```

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/zambogram/bo-gov-scraper-buho.git
cd bo-gov-scraper-buho
```

### 2. Crear entorno virtual

```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Supabase (Opcional)

Copia el archivo de ejemplo y configura tus credenciales:

```bash
cp .env.example .env
```

Edita `.env` con tus credenciales de Supabase:

```
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-key-here
```

## ğŸ“– Uso

### CLI - Interfaz de LÃ­nea de Comandos

#### Listar todos los sitios disponibles

```bash
python main.py listar
```

#### Scrapear un sitio especÃ­fico

```bash
# Scrapear TCP (lÃ­mite de 10 documentos)
python main.py scrape tcp --limit 10

# Scrapear solo documentos nuevos
python main.py scrape tcp --limit 20 --solo-nuevos
```

#### Actualizar todos los sitios

```bash
# Actualizar todos (lÃ­mite de 10 por sitio)
python main.py actualizar-todos --limit 10

# Solo nuevos documentos
python main.py actualizar-todos --limit 10 --solo-nuevos
```

#### Exportar a JSONL

```bash
# Exportar un sitio
python main.py export-jsonl tcp

# Exportar todos
python main.py export-jsonl all
```

#### Ver estadÃ­sticas

```bash
python main.py stats tcp
```

### ğŸ–¥ï¸ Interfaz Web Streamlit

#### Lanzar la UI

```bash
# OpciÃ³n 1: Desde el CLI
python main.py ui

# OpciÃ³n 2: Directamente con Streamlit
streamlit run app/streamlit_app.py
```

La interfaz estarÃ¡ disponible en `http://localhost:8501`

#### Funcionalidades de la UI

**Sidebar:**
- Lista de sitios con estadÃ­sticas en tiempo real
- Selector de sitio
- Control de lÃ­mite de scraping
- Botones de acciÃ³n:
  - Scrapear sitio individual
  - Scrapear todos los sitios
  - Exportar JSONL (individual/todos)
  - Sincronizar con Supabase (nuevos/todos)

**Panel Central:**
- **PestaÃ±a Documentos**: Tabla con todos los documentos del Ã­ndice
- **PestaÃ±a ArtÃ­culos**: Tabla con artÃ­culos parseados
- **PestaÃ±a EstadÃ­sticas**: GrÃ¡ficas y anÃ¡lisis
  - Volumen por sitio (barras)
  - DistribuciÃ³n de documentos (pie)
  - ComparaciÃ³n entre sitios
- **PestaÃ±a Logs**: VisualizaciÃ³n de logs de sync

### â˜ï¸ SincronizaciÃ³n con Supabase

#### Crear las tablas en Supabase

Ejecuta este SQL en tu proyecto Supabase:

```sql
-- Tabla de documentos
CREATE TABLE documents (
  id TEXT PRIMARY KEY,
  site TEXT NOT NULL,
  url TEXT NOT NULL,
  title TEXT,
  content TEXT,
  md5 TEXT UNIQUE,
  metadata JSONB,
  scraped_at TIMESTAMP,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP
);

-- Tabla de artÃ­culos
CREATE TABLE articles (
  id TEXT PRIMARY KEY,
  document_id TEXT REFERENCES documents(id),
  site TEXT NOT NULL,
  article_number INTEGER,
  content TEXT,
  metadata JSONB,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP
);

-- Ãndices
CREATE INDEX idx_documents_site ON documents(site);
CREATE INDEX idx_documents_md5 ON documents(md5);
CREATE INDEX idx_articles_document ON articles(document_id);
CREATE INDEX idx_articles_site ON articles(site);
```

#### Sincronizar desde Python

```python
from sync.supabase_sync import (
    sync_documents_to_supabase,
    sync_articles_to_supabase,
    sync_all_sites
)

# Sincronizar un sitio (solo nuevos)
sync_documents_to_supabase('tcp', only_new=True)
sync_articles_to_supabase('tcp', only_new=True)

# Sincronizar todos los sitios
sync_all_sites(only_new=True)

# Sincronizar todo (incluyendo modificados)
sync_all_sites(only_new=False)
```

### â° Scheduler AutomÃ¡tico

#### Ejecutar scraping inmediato

```bash
python scheduler/run_daily.py --now
```

#### Ejecutar como daemon (scraping diario a las 2 AM)

```bash
python scheduler/run_daily.py --daemon
```

#### Configurar como servicio systemd (Linux)

Crea `/etc/systemd/system/buho-scraper.service`:

```ini
[Unit]
Description=BÃšHO Daily Scraper
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/path/to/bo-gov-scraper-buho
ExecStart=/path/to/venv/bin/python scheduler/run_daily.py --daemon
Restart=always

[Install]
WantedBy=multi-user.target
```

Activar:

```bash
sudo systemctl enable buho-scraper
sudo systemctl start buho-scraper
sudo systemctl status buho-scraper
```

## ğŸ“Š Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Scraping  â”‚  â† TCP, TSJ, ASFI, SIN, ContralorÃ­a
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Delta Update   â”‚  â† DetecciÃ³n MD5 (nuevo/modificado/sin cambios)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSON Index     â”‚  â† data/index/{site}_index.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Legal Parser   â”‚  â† ExtracciÃ³n de artÃ­culos
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSON Articles   â”‚  â† data/articles/{site}_articles.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                     â”‚
         â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSONL Export   â”‚   â”‚ Supabase Sync    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Sitios Soportados

| Sitio | CÃ³digo | URL Base | Tipo de Documento |
|-------|--------|----------|-------------------|
| Tribunal Constitucional Plurinacional | `tcp` | tcpbolivia.bo | Sentencias Constitucionales |
| Tribunal Supremo de Justicia | `tsj` | tsj.bo | Autos Supremos |
| ASFI | `asfi` | asfi.gob.bo | Resoluciones Financieras |
| SIN | `sin` | impuestos.gob.bo | Resoluciones Normativas |
| ContralorÃ­a | `contraloria` | contraloria.gob.bo | Informes de AuditorÃ­a |

## ğŸ“ Formato JSONL

### Documentos

```jsonl
{"id": "tcp-000001", "site": "tcp", "url": "...", "title": "...", "content": "...", "md5": "...", "metadata": {...}}
{"id": "tcp-000002", "site": "tcp", "url": "...", "title": "...", "content": "...", "md5": "...", "metadata": {...}}
```

### ArtÃ­culos

```jsonl
{"id": "tcp-000001-art-001", "document_id": "tcp-000001", "site": "tcp", "article_number": 1, "content": "...", "metadata": {...}}
{"id": "tcp-000001-art-002", "document_id": "tcp-000001", "site": "tcp", "article_number": 2, "content": "...", "metadata": {...}}
```

## ğŸ§ª Pruebas

```bash
# Test scraping
python main.py listar
python main.py scrape tcp --limit 2
python main.py scrape tsj --limit 2

# Test export
python main.py export-jsonl tcp

# Test UI
streamlit run app/streamlit_app.py

# Test scheduler
python scheduler/run_daily.py --now
```

## ğŸ“š DocumentaciÃ³n

- [FASE10_COMPLETO.md](docs/FASE10_COMPLETO.md) - DocumentaciÃ³n tÃ©cnica completa

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Ver archivo [LICENSE](LICENSE)

## ğŸ‘¤ Autor

Zambogram - bo-gov-scraper-buho

## ğŸ™ Agradecimientos

- Comunidad boliviana de datos abiertos
- Instituciones del Estado Plurinacional de Bolivia
- Contribuidores del proyecto

---

**ğŸ¦‰ BÃšHO - Haciendo accesible la informaciÃ³n pÃºblica boliviana**
