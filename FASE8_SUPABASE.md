# FASE 8: EXPORTACIONES PROFESIONALES PARA SUPABASE

## Memoria Legal DinÃ¡mica (MLD) de BÃšHO

Esta fase implementa un pipeline completo de exportaciÃ³n de datos extraÃ­dos a formato JSONL listo para importar en Supabase, permitiendo alimentar el sistema de Memoria Legal DinÃ¡mica basado en pgvector.

---

## ðŸ“‹ Ãndice

1. [Arquitectura del Sistema](#arquitectura-del-sistema)
2. [Estructura de Datos](#estructura-de-datos)
3. [Uso del Exportador](#uso-del-exportador)
4. [ImportaciÃ³n en Supabase](#importaciÃ³n-en-supabase)
5. [GeneraciÃ³n de Embeddings](#generaciÃ³n-de-embeddings)
6. [Consultas SQL Ãštiles](#consultas-sql-Ãºtiles)
7. [IntegraciÃ³n con MLD](#integraciÃ³n-con-mld)
8. [Troubleshooting](#troubleshooting)

---

## ðŸ—ï¸ Arquitectura del Sistema

### Componentes Principales

```
bo-gov-scraper-buho/
â”‚
â”œâ”€â”€ schema/
â”‚   â””â”€â”€ supabase_schema.sql      # Schema completo de la base de datos
â”‚
â”œâ”€â”€ exporter/
â”‚   â”œâ”€â”€ __init__.py              # Exports del mÃ³dulo
â”‚   â”œâ”€â”€ export_supabase.py       # LÃ³gica principal de exportaciÃ³n
â”‚   â””â”€â”€ utils.py                 # Utilidades de limpieza y validaciÃ³n
â”‚
â”œâ”€â”€ data/                         # Datos JSON extraÃ­dos (input)
â”‚   â””â”€â”€ *.json
â”‚
â”œâ”€â”€ exports/                      # Archivos JSONL generados (output)
â”‚   â”œâ”€â”€ documents_supabase_*.jsonl
â”‚   â”œâ”€â”€ articles_supabase_*.jsonl
â”‚   â””â”€â”€ export_stats_*.json
â”‚
â””â”€â”€ main.py                       # CLI principal
```

### Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scraper/OCR    â”‚
â”‚  (Fases 1-7)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   data/*.json   â”‚  â—„â”€â”€ Documentos extraÃ­dos en JSON
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Exportador    â”‚  â—„â”€â”€ Procesa, limpia, valida
â”‚  (FASE 8)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ exports/*.jsonl â”‚  â—„â”€â”€ JSONL listo para Supabase
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Supabase DB   â”‚  â—„â”€â”€ PostgreSQL + pgvector
â”‚   (MLD BÃšHO)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š Estructura de Datos

### Tabla: `documents`

Almacena documentos normativos completos.

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `id_documento` | TEXT (PK) | ID Ãºnico: `{sitio}_{tipo}_{numero}_{fecha}` |
| `sitio` | TEXT | Nombre del sitio fuente (ej: gaceta, abi) |
| `tipo_norma` | TEXT | Tipo: ley, decreto_supremo, resolucion, etc. |
| `numero_norma` | TEXT | NÃºmero de la norma |
| `fecha_norma` | TEXT | Fecha de promulgaciÃ³n (YYYY-MM-DD) |
| `titulo` | TEXT | TÃ­tulo del documento |
| `url_fuente` | TEXT | URL de la pÃ¡gina fuente |
| `url_pdf` | TEXT | URL del PDF (si existe) |
| `filename_pdf` | TEXT | Nombre del archivo PDF descargado |
| `metodo_extraccion` | TEXT | pdf_text, ocr, html, api |
| `paginas` | INTEGER | NÃºmero de pÃ¡ginas |
| `caracteres` | INTEGER | Total de caracteres extraÃ­dos |
| `total_articulos` | INTEGER | NÃºmero de artÃ­culos en el documento |
| `fecha_extraccion` | TIMESTAMP | Fecha de extracciÃ³n |
| `estado` | TEXT | extraido, procesado, vectorizado, error |
| `raw_metadata` | JSONB | Metadatos originales en JSON |

### Tabla: `articles`

Almacena artÃ­culos individuales de cada documento.

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `id_articulo` | TEXT (PK) | ID Ãºnico: `{id_documento}_art{numero}` |
| `id_documento` | TEXT (FK) | Referencia a documents |
| `numero_articulo` | TEXT | NÃºmero del artÃ­culo |
| `titulo_articulo` | TEXT | TÃ­tulo del artÃ­culo (si existe) |
| `contenido` | TEXT | Texto completo del artÃ­culo |
| `tipo_norma` | TEXT | Heredado del documento |
| `fecha_norma` | TEXT | Heredado del documento |
| `sitio` | TEXT | Nombre del sitio fuente |
| `orden` | INTEGER | Orden dentro del documento |
| `raw` | TEXT | Texto sin procesar |

### Tabla: `embeddings`

Almacena vectores de embeddings para bÃºsqueda semÃ¡ntica.

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `id` | SERIAL (PK) | ID autoincrementable |
| `id_articulo` | TEXT (FK) | Referencia a articles |
| `embedding` | VECTOR(1536) | Vector de 1536 dimensiones (OpenAI) |
| `modelo` | TEXT | Modelo usado (ej: text-embedding-ada-002) |
| `created_at` | TIMESTAMP | Fecha de creaciÃ³n |

### Tabla: `sources`

CatÃ¡logo de sitios fuente de normativa.

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `sitio` | TEXT (PK) | Identificador del sitio |
| `nombre` | TEXT | Nombre completo |
| `url` | TEXT | URL del sitio |
| `descripcion` | TEXT | DescripciÃ³n del sitio |
| `configuracion` | JSONB | ConfiguraciÃ³n del scraper |

---

## ðŸš€ Uso del Exportador

### InstalaciÃ³n de Dependencias

```bash
pip install -r requirements.txt
```

### Comandos CLI

#### 1. Exportar Todos los Datos

Procesa todos los archivos JSON en `data/` y genera JSONL:

```bash
python main.py --export-supabase
```

Salida:
- `exports/documents_supabase_YYYYMMDD_HHMMSS.jsonl`
- `exports/articles_supabase_YYYYMMDD_HHMMSS.jsonl`
- `exports/export_stats_YYYYMMDD_HHMMSS.json`

#### 2. Exportar Solo un Sitio EspecÃ­fico

Filtra documentos por sitio fuente:

```bash
python main.py --export-supabase --sitio gaceta
```

```bash
python main.py --export-supabase --sitio abi
```

#### 3. Exportar un Documento Individual

Procesa un solo archivo JSON:

```bash
python main.py --export-documento data/mi_documento.json
```

Con sitio explÃ­cito:

```bash
python main.py --export-documento data/mi_documento.json --sitio gaceta
```

#### 4. Ver InformaciÃ³n del Proyecto

Muestra estadÃ­sticas y archivos generados:

```bash
python main.py --info
```

### Uso ProgramÃ¡tico

TambiÃ©n puedes usar el exportador desde cÃ³digo Python:

```python
from exporter import export_supabase_ready

# Exportar todos los datos
resultados = export_supabase_ready(
    data_dir='data',
    export_dir='exports',
    sitio=None  # o 'gaceta' para filtrar
)

print(resultados['documents'])  # Ruta al archivo de documentos
print(resultados['articles'])   # Ruta al archivo de artÃ­culos
print(resultados['stats'])      # Ruta al archivo de estadÃ­sticas
```

---

## ðŸ“¥ ImportaciÃ³n en Supabase

### MÃ©todo 1: Dashboard de Supabase (GUI)

1. **Crear la Base de Datos**

   Ve a tu proyecto en Supabase â†’ SQL Editor

2. **Ejecutar el Schema**

   Copia y pega el contenido de `schema/supabase_schema.sql` y ejecuta.

3. **Importar Documentos**

   ```sql
   -- OpciÃ³n A: Desde interfaz grÃ¡fica
   -- Table Editor â†’ documents â†’ Import data â†’ Upload JSONL

   -- OpciÃ³n B: Desde SQL Editor
   COPY documents FROM '/path/to/documents_supabase.jsonl';
   ```

4. **Importar ArtÃ­culos**

   ```sql
   COPY articles FROM '/path/to/articles_supabase.jsonl';
   ```

### MÃ©todo 2: Cliente Supabase Python

```python
from supabase import create_client, Client
import json

# Configurar cliente
url = "https://tu-proyecto.supabase.co"
key = "tu-api-key"
supabase: Client = create_client(url, key)

# Leer JSONL
documents = []
with open('exports/documents_supabase_20231115_120000.jsonl', 'r') as f:
    for line in f:
        documents.append(json.loads(line))

# Insertar en batch
for doc in documents:
    supabase.table('documents').insert(doc).execute()
```

### MÃ©todo 3: psycopg2 Directo

```python
import psycopg2
import json

# Conectar a Supabase
conn = psycopg2.connect(
    host="db.tu-proyecto.supabase.co",
    database="postgres",
    user="postgres",
    password="tu-password"
)

cursor = conn.cursor()

# Insertar documentos
with open('exports/documents_supabase.jsonl', 'r') as f:
    for line in f:
        doc = json.loads(line)
        cursor.execute("""
            INSERT INTO documents (
                id_documento, sitio, tipo_norma, numero_norma,
                fecha_norma, titulo, url_fuente, url_pdf,
                metodo_extraccion, estado, raw_metadata
            ) VALUES (
                %(id_documento)s, %(sitio)s, %(tipo_norma)s, %(numero_norma)s,
                %(fecha_norma)s, %(titulo)s, %(url_fuente)s, %(url_pdf)s,
                %(metodo_extraccion)s, %(estado)s, %(raw_metadata)s
            ) ON CONFLICT (id_documento) DO NOTHING;
        """, doc)

conn.commit()
conn.close()
```

---

## ðŸ¤– GeneraciÃ³n de Embeddings

### Usando OpenAI API

```python
import openai
from supabase import create_client

# Configurar clientes
openai.api_key = "tu-openai-api-key"
supabase = create_client("url", "key")

# Obtener artÃ­culos sin embeddings
articulos = supabase.table('articles') \
    .select('id_articulo, contenido') \
    .execute()

for articulo in articulos.data:
    # Generar embedding
    response = openai.Embedding.create(
        input=articulo['contenido'],
        model="text-embedding-ada-002"
    )

    embedding = response['data'][0]['embedding']

    # Insertar en tabla embeddings
    supabase.table('embeddings').insert({
        'id_articulo': articulo['id_articulo'],
        'embedding': embedding,
        'modelo': 'text-embedding-ada-002'
    }).execute()
```

### Script Batch para Embeddings

```python
import json
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm

client = OpenAI(api_key="tu-api-key")
supabase = create_client("url", "key")

# Obtener artÃ­culos sin embeddings
result = supabase.from_('view_articles_enriched') \
    .select('id_articulo, contenido') \
    .filter('tiene_embedding', 'eq', False) \
    .execute()

print(f"Procesando {len(result.data)} artÃ­culos...")

for articulo in tqdm(result.data):
    try:
        # Generar embedding
        response = client.embeddings.create(
            input=articulo['contenido'][:8000],  # Truncar si es muy largo
            model="text-embedding-ada-002"
        )

        embedding = response.data[0].embedding

        # Insertar
        supabase.table('embeddings').insert({
            'id_articulo': articulo['id_articulo'],
            'embedding': embedding,
            'modelo': 'text-embedding-ada-002'
        }).execute()

    except Exception as e:
        print(f"Error en {articulo['id_articulo']}: {e}")
        continue

print("âœ… Embeddings generados exitosamente!")
```

---

## ðŸ” Consultas SQL Ãštiles

### BÃºsqueda por Tipo de Norma

```sql
SELECT
    id_documento,
    tipo_norma,
    numero_norma,
    fecha_norma,
    titulo,
    total_articulos
FROM documents
WHERE tipo_norma = 'ley'
ORDER BY fecha_norma DESC
LIMIT 20;
```

### ArtÃ­culos con Metadata del Documento

```sql
SELECT
    a.numero_articulo,
    a.contenido,
    d.tipo_norma,
    d.numero_norma,
    d.fecha_norma,
    d.titulo AS titulo_documento,
    d.sitio
FROM articles a
JOIN documents d ON a.id_documento = d.id_documento
WHERE d.tipo_norma = 'decreto_supremo'
AND d.fecha_norma >= '2023-01-01'
ORDER BY d.fecha_norma DESC, a.orden ASC;
```

### BÃºsqueda de Texto Completo

```sql
SELECT
    id_articulo,
    numero_articulo,
    contenido,
    ts_rank(to_tsvector('spanish', contenido), query) AS rank
FROM articles,
     to_tsquery('spanish', 'salud & educaciÃ³n') AS query
WHERE to_tsvector('spanish', contenido) @@ query
ORDER BY rank DESC
LIMIT 10;
```

### BÃºsqueda SemÃ¡ntica por Similitud

```sql
-- Primero obtener el embedding de la consulta desde tu app
-- Luego buscar artÃ­culos similares

SELECT
    id_articulo,
    numero_articulo,
    contenido,
    tipo_norma,
    fecha_norma,
    1 - (embedding <=> '[vector de consulta]'::vector) AS similarity
FROM embeddings e
JOIN articles a USING (id_articulo)
WHERE 1 - (embedding <=> '[vector de consulta]'::vector) > 0.7
ORDER BY embedding <=> '[vector de consulta]'::vector
LIMIT 10;
```

### EstadÃ­sticas por Sitio

```sql
SELECT
    sitio,
    COUNT(DISTINCT id_documento) AS total_documentos,
    COUNT(DISTINCT a.id_articulo) AS total_articulos,
    COUNT(DISTINCT e.id) AS total_embeddings
FROM documents d
LEFT JOIN articles a ON d.id_documento = a.id_documento
LEFT JOIN embeddings e ON a.id_articulo = e.id_articulo
GROUP BY sitio
ORDER BY total_documentos DESC;
```

### Documentos Pendientes de VectorizaciÃ³n

```sql
SELECT
    d.id_documento,
    d.tipo_norma,
    d.numero_norma,
    d.titulo,
    COUNT(a.id_articulo) AS articulos_totales,
    COUNT(e.id) AS articulos_vectorizados
FROM documents d
LEFT JOIN articles a ON d.id_documento = a.id_documento
LEFT JOIN embeddings e ON a.id_articulo = e.id_articulo
GROUP BY d.id_documento
HAVING COUNT(a.id_articulo) > COUNT(e.id);
```

---

## ðŸ§  IntegraciÃ³n con MLD (Memoria Legal DinÃ¡mica)

### Arquitectura de BÃºsqueda HÃ­brida

La MLD de BÃšHO combina:

1. **BÃºsqueda por Keywords** (PostgreSQL Full-Text Search)
2. **BÃºsqueda SemÃ¡ntica** (pgvector + OpenAI embeddings)
3. **Filtros Estructurados** (por tipo, fecha, sitio)

### Ejemplo de Consulta HÃ­brida

```python
def buscar_normativa(
    query: str,
    tipo_norma: Optional[str] = None,
    fecha_desde: Optional[str] = None,
    fecha_hasta: Optional[str] = None,
    usar_semantico: bool = True
) -> List[Dict]:

    if usar_semantico:
        # Generar embedding de la consulta
        embedding = openai.Embedding.create(
            input=query,
            model="text-embedding-ada-002"
        )['data'][0]['embedding']

        # Buscar por similitud vectorial
        query_sql = """
            SELECT
                a.id_articulo,
                a.numero_articulo,
                a.contenido,
                d.tipo_norma,
                d.numero_norma,
                d.fecha_norma,
                d.titulo,
                1 - (e.embedding <=> %s::vector) AS similarity
            FROM embeddings e
            JOIN articles a ON e.id_articulo = a.id_articulo
            JOIN documents d ON a.id_documento = d.id_documento
            WHERE 1 = 1
        """
        params = [embedding]

    else:
        # BÃºsqueda por texto completo
        query_sql = """
            SELECT
                a.id_articulo,
                a.numero_articulo,
                a.contenido,
                d.tipo_norma,
                d.numero_norma,
                d.fecha_norma,
                d.titulo,
                ts_rank(to_tsvector('spanish', a.contenido), query) AS similarity
            FROM articles a
            JOIN documents d ON a.id_documento = d.id_documento,
                 to_tsquery('spanish', %s) AS query
            WHERE to_tsvector('spanish', a.contenido) @@ query
        """
        params = [query]

    # Agregar filtros
    if tipo_norma:
        query_sql += " AND d.tipo_norma = %s"
        params.append(tipo_norma)

    if fecha_desde:
        query_sql += " AND d.fecha_norma >= %s"
        params.append(fecha_desde)

    if fecha_hasta:
        query_sql += " AND d.fecha_norma <= %s"
        params.append(fecha_hasta)

    query_sql += " ORDER BY similarity DESC LIMIT 20"

    # Ejecutar
    cursor.execute(query_sql, params)
    return cursor.fetchall()
```

---

## ðŸ› ï¸ Troubleshooting

### Problema: Duplicados en la ImportaciÃ³n

**SoluciÃ³n:** El exportador ya elimina duplicados automÃ¡ticamente por ID. Si persisten:

```sql
-- Eliminar duplicados de documents
DELETE FROM documents a USING (
    SELECT MIN(ctid) as ctid, id_documento
    FROM documents
    GROUP BY id_documento HAVING COUNT(*) > 1
) b
WHERE a.id_documento = b.id_documento
AND a.ctid <> b.ctid;

-- Eliminar duplicados de articles
DELETE FROM articles a USING (
    SELECT MIN(ctid) as ctid, id_articulo
    FROM articles
    GROUP BY id_articulo HAVING COUNT(*) > 1
) b
WHERE a.id_articulo = b.id_articulo
AND a.ctid <> b.ctid;
```

### Problema: Encoding UTF-8

**SoluciÃ³n:** El exportador normaliza automÃ¡ticamente a UTF-8. Si hay problemas:

```python
# En utils.py, la funciÃ³n limpiar_texto() ya maneja esto
# Pero si necesitas forzar:
import unicodedata

texto = unicodedata.normalize('NFKC', texto)
```

### Problema: ArtÃ­culos sin ID de Documento

**Error:** `foreign key violation`

**SoluciÃ³n:** Importa primero los documentos, luego los artÃ­culos:

```bash
# 1. Importar documentos
psql -h db.supabase.co -d postgres -U postgres \
  -c "\COPY documents FROM 'documents.jsonl' CSV QUOTE E'\x01' DELIMITER E'\x02';"

# 2. Importar artÃ­culos
psql -h db.supabase.co -d postgres -U postgres \
  -c "\COPY articles FROM 'articles.jsonl' CSV QUOTE E'\x01' DELIMITER E'\x02';"
```

### Problema: Embeddings muy Lentos

**SoluciÃ³n:** Procesar en batch y usar rate limiting:

```python
import time
from tqdm import tqdm

BATCH_SIZE = 100
RATE_LIMIT_DELAY = 1  # segundos

articulos = obtener_articulos_sin_embeddings()

for i in tqdm(range(0, len(articulos), BATCH_SIZE)):
    batch = articulos[i:i+BATCH_SIZE]
    procesar_batch(batch)
    time.sleep(RATE_LIMIT_DELAY)
```

---

## ðŸ“š Referencias

- [Supabase Docs](https://supabase.com/docs)
- [pgvector](https://github.com/pgvector/pgvector)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [PostgreSQL Full-Text Search](https://www.postgresql.org/docs/current/textsearch.html)

---

## ðŸŽ¯ PrÃ³ximos Pasos

1. âœ… Implementar scrapers para sitios bolivianos (Fases 1-7)
2. âœ… Exportar datos a Supabase (FASE 8 - Completada)
3. ðŸ”„ Generar embeddings para todos los artÃ­culos
4. ðŸ”„ Implementar API de bÃºsqueda hÃ­brida
5. ðŸ”„ Crear interfaz web con Streamlit
6. ðŸ”„ Automatizar scraping periÃ³dico
7. ðŸ”„ Implementar sistema de alertas

---

**Desarrollado para BÃšHO - Memoria Legal DinÃ¡mica**

*FASE 8 - Noviembre 2025*
