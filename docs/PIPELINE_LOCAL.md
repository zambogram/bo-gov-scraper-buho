# üìñ Pipeline de Procesamiento Local

Documentaci√≥n detallada del pipeline de scraping ‚Üí procesamiento ‚Üí almacenamiento local de BO-GOV-SCRAPER-BUHO.

---

## üìã Tabla de Contenidos

- [Visi√≥n General](#visi√≥n-general)
- [Arquitectura](#arquitectura)
- [Flujo Paso a Paso](#flujo-paso-a-paso)
- [Estructura de Almacenamiento](#estructura-de-almacenamiento)
- [Sistema de Delta Updates](#sistema-de-delta-updates)
- [Formato de Datos](#formato-de-datos)
- [Extensibilidad](#extensibilidad)

---

## üéØ Visi√≥n General

El pipeline de B√öHO procesa documentos legales bolivianos de forma autom√°tica, estructurada y controlable:

```
Sitio Web ‚Üí Descarga PDF ‚Üí Extracci√≥n de Texto ‚Üí Parsing Legal ‚Üí JSON + TXT ‚Üí √çndice
```

### Objetivos

1. **Automatizaci√≥n completa**: Desde descarga hasta estructuraci√≥n
2. **Control granular**: Decidir qu√© guardar (PDF, TXT, JSON)
3. **Eficiencia**: Delta updates para procesar solo lo nuevo
4. **Calidad**: Texto limpio y estructura legal precisa
5. **Trazabilidad**: Logs y hashes para auditor√≠a

---

## üèóÔ∏è Arquitectura

### Componentes Principales

```
scraper/
‚îú‚îÄ‚îÄ pipeline.py              # Orquestador principal
‚îú‚îÄ‚îÄ models.py                # Modelos de datos
‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îî‚îÄ‚îÄ pdf_extractor.py     # Extracci√≥n de texto/OCR
‚îú‚îÄ‚îÄ parsers/
‚îÇ   ‚îî‚îÄ‚îÄ legal_parser.py      # Parsing de estructura legal
‚îî‚îÄ‚îÄ sites/
    ‚îî‚îÄ‚îÄ {site}_scraper.py    # Scrapers espec√≠ficos por sitio
```

### Flujo de Datos

```mermaid
graph LR
    A[Sitio Web] --> B[Scraper]
    B --> C[PDF]
    C --> D[Extractor]
    D --> E[Texto]
    E --> F[Parser]
    F --> G[Art√≠culos]
    G --> H[JSON]
    E --> I[TXT]
    H --> J[√çndice]
    I --> J
```

---

## üîÑ Flujo Paso a Paso

### 1. Inicializaci√≥n

```python
from scraper import run_site_pipeline

result = run_site_pipeline(
    site_id='tcp',
    mode='delta',          # 'delta' o 'full'
    limit=50,
    save_pdf=False,
    save_txt=True,
    save_json=True
)
```

**Acciones:**
- Cargar configuraci√≥n del sitio desde `config/sites_catalog.yaml`
- Inicializar scraper espec√≠fico del sitio
- Cargar √≠ndice existente (para delta updates)
- Crear directorios si no existen

### 2. Listado de Documentos

```python
# En el scraper espec√≠fico
documentos_metadata = scraper.listar_documentos(limite=50)
```

**Retorna:** Lista de diccionarios con metadata:
```python
{
    'id_documento': 'tcp_sc_0001_2024',
    'tipo_documento': 'Sentencia Constitucional',
    'numero_norma': '0001/2024',
    'fecha': '2024-01-15',
    'titulo': 'Sentencia Constitucional 0001/2024-S1',
    'url': 'https://...',
    'sumilla': '...'
}
```

### 3. Descarga de PDF

**Opciones:**

**A) Guardar PDF (si `save_pdf=True`)**
```
Destino: data/raw/{site}/pdfs/{id_documento}.pdf
```

**B) Archivo Temporal (si `save_pdf=False`)**
```python
temp_file = tempfile.NamedTemporaryFile(suffix='.pdf', delete=False)
# Procesar...
# Borrar al final
```

**Implementaci√≥n:**
```python
success = scraper.descargar_pdf(url, ruta_destino)
```

### 4. Extracci√≥n de Texto

**Componente:** `PDFExtractor`

**M√©todos de extracci√≥n:**

1. **PyPDF2** (primera opci√≥n)
   - Para PDFs digitales (texto seleccionable)
   - R√°pido y eficiente

2. **Tesseract OCR** (fallback)
   - Para PDFs escaneados
   - Requiere conversi√≥n PDF ‚Üí Imagen ‚Üí OCR
   - M√°s lento pero necesario para documentos antiguos

**Proceso:**
```python
extractor = PDFExtractor(usar_ocr=True)
texto = extractor.extraer_texto(pdf_path)
```

**Normalizaci√≥n:**
- Eliminar caracteres de control
- Normalizar espacios en blanco
- M√°ximo 2 saltos de l√≠nea consecutivos
- Trim de espacios por l√≠nea

**Salida:**
```
Destino: data/normalized/{site}/text/{id_documento}.txt
Codificaci√≥n: UTF-8
```

### 5. Parsing Legal

**Componente:** `LegalParser`

**Objetivo:** Dividir el texto en unidades sem√°nticas legales

**Detecci√≥n de Patrones:**

```python
# Art√≠culos
ART√çCULO 1.- Contenido del art√≠culo...
Art. 5¬∞.- Contenido...

# Secciones
SECCI√ìN I - DISPOSICIONES GENERALES
CAP√çTULO II - DE LOS DERECHOS

# Disposiciones
DISPOSICI√ìN TRANSITORIA PRIMERA.- ...
DISPOSICI√ìN FINAL.- ...
```

**Algoritmo:**

1. Recorrer texto l√≠nea por l√≠nea
2. Detectar inicio de art√≠culo/secci√≥n con regex
3. Acumular contenido hasta siguiente art√≠culo
4. Crear objeto `Articulo` con:
   - `numero`: "1", "5", "I", etc.
   - `titulo`: Si existe (ej: "DEL OBJETO")
   - `contenido`: Texto completo
   - `tipo_unidad`: articulo, seccion, capitulo, disposicion

**Salida:**
```python
articulos = [
    Articulo(
        id_articulo='tcp_sc_0001_2024_art_1',
        numero='1',
        titulo='DEL OBJETO',
        contenido='El presente decreto...',
        tipo_unidad='articulo'
    ),
    # ...
]
```

### 6. Generaci√≥n de JSON

**Estructura del Documento:**

```json
{
  "id_documento": "tcp_sc_0001_2024",
  "site": "tcp",
  "tipo_documento": "Sentencia Constitucional",
  "numero_norma": "0001/2024",
  "fecha": "2024-01-15",
  "titulo": "Sentencia Constitucional 0001/2024-S1",
  "url_origen": "https://...",
  "texto_completo": "...",
  "ruta_pdf": "data/raw/tcp/pdfs/tcp_sc_0001_2024.pdf",
  "ruta_txt": "data/normalized/tcp/text/tcp_sc_0001_2024.txt",
  "ruta_json": "data/normalized/tcp/json/tcp_sc_0001_2024.json",
  "hash_contenido": "a1b2c3d4...",
  "fecha_scraping": "2024-11-18T10:30:00",
  "fecha_ultima_actualizacion": "2024-11-18T10:30:00",
  "articulos": [
    {
      "id_articulo": "tcp_sc_0001_2024_art_1",
      "id_documento": "tcp_sc_0001_2024",
      "numero": "1",
      "titulo": "DEL OBJETO",
      "contenido": "El presente decreto...",
      "tipo_unidad": "articulo",
      "metadata": {}
    }
  ],
  "metadata": {
    "sumilla": "...",
    "otros_datos": "..."
  }
}
```

**Destino:**
```
data/normalized/{site}/json/{id_documento}.json
```

### 7. Actualizaci√≥n del √çndice

**Archivo:** `data/index/{site}/index.json`

**Prop√≥sito:**
- Tracking de documentos procesados
- Delta updates (evitar reprocesar)
- Metadatos de √∫ltima actualizaci√≥n

**Estructura:**

```json
{
  "last_update": "2024-11-18T10:30:00",
  "total_documentos": 150,
  "documentos": {
    "tcp_sc_0001_2024": {
      "hash": "a1b2c3d4...",
      "fecha_actualizacion": "2024-11-18T10:30:00",
      "ruta_pdf": "data/raw/tcp/pdfs/tcp_sc_0001_2024.pdf",
      "ruta_txt": "data/normalized/tcp/text/tcp_sc_0001_2024.txt",
      "ruta_json": "data/normalized/tcp/json/tcp_sc_0001_2024.json"
    }
  }
}
```

**L√≥gica de Delta Update:**

```python
# Verificar si el documento ya existe
if index.documento_existe(id_doc):
    if index.documento_cambio(id_doc, hash_nuevo):
        # Procesar (cambi√≥)
    else:
        # Saltar (no cambi√≥)
else:
    # Procesar (nuevo)
```

---

## üìÇ Estructura de Almacenamiento

```
data/
‚îú‚îÄ‚îÄ raw/{site}/
‚îÇ   ‚îî‚îÄ‚îÄ pdfs/
‚îÇ       ‚îú‚îÄ‚îÄ tcp_sc_0001_2024.pdf
‚îÇ       ‚îú‚îÄ‚îÄ tcp_sc_0002_2024.pdf
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ normalized/{site}/
‚îÇ   ‚îú‚îÄ‚îÄ text/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tcp_sc_0001_2024.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ json/
‚îÇ       ‚îú‚îÄ‚îÄ tcp_sc_0001_2024.json
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ index/{site}/
    ‚îî‚îÄ‚îÄ index.json
```

### Directorios por Sitio

Cada sitio tiene su propia estructura:
- `tcp/`, `tsj/`, `asfi/`, `sin/`, `contraloria/`, `gaceta_oficial/`

### Rutas Configurables

Definidas en `config/settings.py` mediante `SiteConfig`:

```python
site_config.raw_pdf_dir        # data/raw/{site}/pdfs
site_config.normalized_text_dir # data/normalized/{site}/text
site_config.normalized_json_dir # data/normalized/{site}/json
site_config.index_file         # data/index/{site}/index.json
site_config.logs_dir           # logs/{site}
```

---

## üîÑ Sistema de Delta Updates

### Concepto

**Problema:** Reprocesar todo el archivo hist√≥rico en cada corrida es ineficiente.

**Soluci√≥n:** Sistema de √≠ndices con hashing MD5.

### Funcionamiento

1. **Primera corrida (sitio vac√≠o)**
   - Todos los documentos son nuevos
   - Procesar todos (hasta l√≠mite)
   - Crear √≠ndice

2. **Corridas subsecuentes (modo delta)**
   - Cargar √≠ndice existente
   - Comparar cada documento:
     - Si `id_documento` no existe en √≠ndice ‚Üí **PROCESAR**
     - Si existe pero `hash` cambi√≥ ‚Üí **PROCESAR**
     - Si existe y `hash` igual ‚Üí **SALTAR**

3. **Modo full (hist√≥rico completo)**
   - Ignora delta check
   - Procesa todos (respetando l√≠mite)
   - √ötil para reprocesar con nueva l√≥gica de parsing

### C√°lculo de Hash

```python
def calcular_hash(documento):
    content = f"{documento.texto_completo}{len(documento.articulos)}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()
```

**Factores:**
- Texto completo
- Cantidad de art√≠culos

**Cambio detectado si:**
- Texto fue corregido
- Parsing mejorado (m√°s/menos art√≠culos)

---

## üìä Formato de Datos

### Archivo TXT

```
Texto normalizado, limpio, UTF-8
Una ley/sentencia por archivo
Saltos de l√≠nea preservados pero normalizados
```

### Archivo JSON

Ver secci√≥n "Generaci√≥n de JSON" arriba.

**Ventajas:**
- Estructura sem√°ntica expl√≠cita
- F√°cil consulta (jq, Python, etc.)
- Listo para base de datos
- Incluye metadata completa

### √çndice JSON

Ligero, solo metadata esencial para delta updates.

---

## üîå Extensibilidad

### Agregar Nuevo Sitio

**1. Configurar en `config/sites_catalog.yaml`:**

```yaml
nuevo_sitio:
  id: nuevo_sitio
  nombre: "Nombre del Sitio"
  tipo: "Tribunal"
  url_base: "https://..."
  activo: true
  # ...
```

**2. Crear Scraper:**

`scraper/sites/nuevo_sitio_scraper.py`

```python
from .base_scraper import BaseScraper

class NuevoSitioScraper(BaseScraper):
    def __init__(self):
        super().__init__('nuevo_sitio')

    def listar_documentos(self, limite=None):
        # Implementar l√≥gica espec√≠fica
        pass

    def descargar_pdf(self, url, ruta_destino):
        # Implementar descarga
        pass
```

**3. Registrar en `scraper/sites/__init__.py`:**

```python
from .nuevo_sitio_scraper import NuevoSitioScraper

SCRAPERS = {
    # ...
    'nuevo_sitio': NuevoSitioScraper,
}
```

**4. Probar:**

```bash
python main.py scrape nuevo_sitio --limit 1
```

### Personalizar Parser Legal

Si un sitio tiene formato legal diferente, crear parser espec√≠fico:

```python
class GacetaParser(LegalParser):
    PATRONES_ARTICULO = [
        # Patrones espec√≠ficos para Gaceta
    ]
```

### Agregar Procesamiento Post-Download

Heredar y extender:

```python
def procesar_pdf(self, ruta_pdf, ruta_txt):
    texto = super().procesar_pdf(ruta_pdf, ruta_txt)
    # Procesamiento adicional
    return texto_procesado
```

---

## üö¶ Manejo de Errores

### Por Documento

Si un documento falla:
1. Se registra el error en `PipelineResult.errores`
2. Se contin√∫a con el siguiente documento
3. No se actualiza el √≠ndice para ese documento
4. Se puede reintentar en pr√≥xima corrida

### Logging

```
logs/{site}/scrape_20241118_103000.log
```

Incluye:
- Timestamp de cada operaci√≥n
- Errores detallados con stack trace
- Progreso (X/Y documentos)

---

## üìà Optimizaciones Futuras

1. **Procesamiento paralelo** de documentos (multiprocessing)
2. **Cach√© de PDFs** descargados
3. **Compresi√≥n** de archivos antiguos
4. **Base de datos** SQLite/Postgres para b√∫squedas r√°pidas
5. **Webhooks** para notificaciones de nuevos documentos
6. **Validaci√≥n de estructura** legal (linting)
7. **Extracci√≥n de entidades** (NER): nombres, fechas, referencias

---

## ‚úÖ Checklist de Integraci√≥n de Gaceta Oficial

- [x] Estructura de carpetas lista
- [x] Entrada en cat√°logo configurada
- [x] Pipeline gen√©rico preparado
- [ ] Implementar `GacetaScraper`
- [ ] Implementar l√≥gica de paginaci√≥n por ediciones
- [ ] Parser espec√≠fico para diferentes tipos de normas
- [ ] Testing con muestra de ediciones
- [ ] Documentar peculiaridades del sitio

---

**√öltima actualizaci√≥n:** 2025-11-18
