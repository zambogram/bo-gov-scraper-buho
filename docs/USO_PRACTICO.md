# Gu√≠a de Uso Pr√°ctico - Scraper Gubernamental Bolivia

## Tabla de Contenidos

1. [Inicio R√°pido](#inicio-r√°pido)
2. [Comandos B√°sicos](#comandos-b√°sicos)
3. [Ejemplos de Uso](#ejemplos-de-uso)
4. [Interfaz Web](#interfaz-web)
5. [Interpretaci√≥n de Resultados](#interpretaci√≥n-de-resultados)
6. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)

---

## Inicio R√°pido

### Instalaci√≥n

```bash
# 1. Clonar el repositorio
git clone <repo-url>
cd bo-gov-scraper-buho

# 2. Crear entorno virtual
python -m venv venv
source venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Verificar instalaci√≥n
python main.py --help
```

### Primer Scraping

```bash
# Ejecutar demo de un sitio
python main.py scrape gaceta_oficial --limit 5 --demo

# Ver resultados
python main.py stats
```

---

## Comandos B√°sicos

### 1. Listar Sitios (`list`)

```bash
# Ver todos los sitios
python main.py list

# Solo sitios de prioridad 1 (Ola 1)
python main.py list --prioridad 1

# Solo sitios judiciales
python main.py list --categoria judicial

# Solo scrapers implementados
python main.py list --estado implementado
```

**Salida esperada:**
```
================================================================================
CAT√ÅLOGO DE SITIOS GUBERNAMENTALES
================================================================================

ID: gaceta_oficial
  Nombre: Gaceta Oficial de Bolivia
  Categor√≠a: legislativo
  Prioridad: 1
  Estado Scraper: implementado
  URL: http://www.gacetaoficialdebolivia.gob.bo
  Tipos: Leyes, Decretos Supremos, Resoluciones Ministeriales...
  Notas: Sitio principal para normativa nacional...
```

### 2. Ejecutar Scraping (`scrape`)

```bash
# Scraping b√°sico con l√≠mite
python main.py scrape gaceta_oficial --limit 10 --demo

# Sin l√≠mite (todos los documentos disponibles)
python main.py scrape tcp --demo

# Solo documentos nuevos o modificados
python main.py scrape asfi --limit 20 --solo-nuevos --demo
```

**Par√°metros:**
- `site_id`: ID del sitio a scrapear (requerido)
- `--limit N`: Limitar a N documentos (opcional)
- `--solo-nuevos`: Solo procesar documentos nuevos o modificados (default: True)
- `--demo`: Modo demostraci√≥n (genera datos de prueba)

**Salida esperada:**
```
================================================================================
SCRAPING: Gaceta Oficial de Bolivia
================================================================================
Site ID: gaceta_oficial
URL: http://www.gacetaoficialdebolivia.gob.bo/normas/buscar
Modo: DEMO
L√≠mite: 10
Solo nuevos: S√≠
================================================================================

2025-01-18 10:30:15 - scraper.gaceta_oficial - INFO - Iniciando scraping...
2025-01-18 10:30:15 - scraper.gaceta_oficial - INFO - Generando datos demo...
2025-01-18 10:30:15 - scraper.gaceta_oficial - INFO - Scraping completado. 10 documentos procesados.

================================================================================
RESUMEN DE SCRAPING
================================================================================
Sitio: gaceta_oficial
Total encontrados: 10
  - Nuevos: 10
  - Modificados: 0
  - Sin cambios: 0
  - Con PDF: 10
Fecha: 2025-01-18T10:30:15
================================================================================

Ejemplos de documentos:
  - [NUEVO] Ley 1400 - Ley de modificaci√≥n al C√≥digo Tributario Boliviano
    Fecha: 2025-01-15 | ID: GACETA_OFICIAL-LEY-1400-20250115
  - [NUEVO] Decreto Supremo 1401 - Reglamentaci√≥n de la Ley de Empresas
    Fecha: 2025-01-12 | ID: GACETA_OFICIAL-DECRETO-SUPREMO-1401-20250112
  ... y 8 m√°s

Datos guardados en: data/raw/gaceta_oficial/
√çndice guardado en: data/index/gaceta_oficial.json
```

### 3. Demo de la Ola 1 (`demo-ola1`)

Ejecuta scraping de todos los sitios de prioridad 1:

```bash
# Demo de todos los sitios Ola 1 con 5 documentos cada uno
python main.py demo-ola1 --limit 5
```

**Salida esperada:**
```
================================================================================
DEMO - OLA 1 (SCRAPERS DE PRIORIDAD M√ÅXIMA)
================================================================================

Sitios a procesar: gaceta_oficial, tsj_genesis, tcp, asfi, sin

‚ñ∂Ô∏è  Procesando gaceta_oficial...
   ‚úÖ 5 documentos | 5 nuevos | 5 con PDF

‚ñ∂Ô∏è  Procesando tsj_genesis...
   ‚úÖ 5 documentos | 5 nuevos | 5 con PDF

‚ñ∂Ô∏è  Procesando tcp...
   ‚úÖ 5 documentos | 5 nuevos | 5 con PDF

‚ñ∂Ô∏è  Procesando asfi...
   ‚úÖ 5 documentos | 5 nuevos | 5 con PDF

‚ñ∂Ô∏è  Procesando sin...
   ‚úÖ 5 documentos | 5 nuevos | 5 con PDF


================================================================================
RESUMEN FINAL - OLA 1
================================================================================

Sitios procesados: 5/5
Total documentos: 25
Nuevos: 25

================================================================================
```

### 4. Ver Estad√≠sticas (`stats`)

```bash
python main.py stats
```

**Salida esperada:**
```
================================================================================
ESTAD√çSTICAS DE SCRAPERS
================================================================================

Gaceta Oficial de Bolivia (gaceta_oficial)
  Total documentos: 10
  √öltima actualizaci√≥n: 2025-01-18 10:30:15

Tribunal Supremo de Justicia - GENESIS (tsj_genesis)
  Total documentos: 5
  √öltima actualizaci√≥n: 2025-01-18 10:32:20

================================================================================
```

---

## Ejemplos de Uso

### Caso 1: Recolecci√≥n Inicial de Datos

**Objetivo:** Recolectar las primeras 50 normas de cada sitio de la Ola 1.

```bash
# Opci√≥n 1: Uno por uno
python main.py scrape gaceta_oficial --limit 50 --demo
python main.py scrape tsj_genesis --limit 50 --demo
python main.py scrape tcp --limit 50 --demo
python main.py scrape asfi --limit 50 --demo
python main.py scrape sin --limit 50 --demo

# Opci√≥n 2: Demo r√°pido
python main.py demo-ola1 --limit 50

# Ver estad√≠sticas
python main.py stats
```

### Caso 2: Actualizaci√≥n Diaria

**Objetivo:** Actualizar solo documentos nuevos o modificados.

```bash
# Ejecutar sin l√≠mite, solo nuevos (default)
python main.py scrape gaceta_oficial --demo

# O especificar expl√≠citamente
python main.py scrape tcp --solo-nuevos --demo
```

### Caso 3: Exploraci√≥n de un Sitio Espec√≠fico

**Objetivo:** Ver qu√© documentos tiene un sitio antes de scrapear todo.

```bash
# Ver info del sitio
python main.py list --estado implementado | grep -A 10 "tcp"

# Scrapear muestra peque√±a
python main.py scrape tcp --limit 5 --demo

# Ver los datos generados
cat data/raw/tcp/documentos.json | python -m json.tool

# Ver el √≠ndice
cat data/index/tcp.json | python -m json.tool
```

### Caso 4: Monitoreo y Validaci√≥n

**Objetivo:** Verificar que los scrapers est√°n funcionando correctamente.

```bash
# 1. Ejecutar demo completo
python main.py demo-ola1 --limit 10

# 2. Ver estad√≠sticas
python main.py stats

# 3. Verificar archivos generados
ls -lh data/raw/*/
ls -lh data/index/

# 4. Revisar logs
tail -f scraper.log
```

---

## Interfaz Web

### Iniciar la Interfaz

```bash
streamlit run app/streamlit_app.py
```

Se abrir√° autom√°ticamente en `http://localhost:8501`

### P√°ginas Disponibles

#### 1. Dashboard
- Vista general de todos los sitios
- M√©tricas de documentos totales
- Estado de scrapers de la Ola 1
- √öltima actualizaci√≥n por sitio

#### 2. Cat√°logo de Sitios
- Lista completa de sitios con filtros:
  - Por prioridad (1, 2, ...)
  - Por categor√≠a (legislativo, judicial, regulatorio)
  - Por estado (implementado, pendiente)
- Detalles de cada sitio

#### 3. Ejecutar Scraping
- Selector de sitio
- Configuraci√≥n de par√°metros:
  - L√≠mite de documentos
  - Solo nuevos
  - Modo demo
- Bot√≥n de ejecuci√≥n
- Resultados en tiempo real
- Vista previa de documentos

#### 4. Estad√≠sticas
- Estad√≠sticas por sitio:
  - Total de documentos
  - √öltima actualizaci√≥n
  - Distribuci√≥n por estado (nuevo, modificado, sin cambios)
  - Distribuci√≥n por tipo de norma

#### 5. Ayuda
- Documentaci√≥n integrada
- Ejemplos de comandos CLI
- Estructura de datos

### Flujo de Trabajo en la UI

1. **Explorar el cat√°logo:**
   - Ir a "Cat√°logo de Sitios"
   - Filtrar por prioridad 1
   - Revisar sitios disponibles

2. **Ejecutar scraping:**
   - Ir a "Ejecutar Scraping"
   - Seleccionar sitio (ej: "Gaceta Oficial")
   - Configurar l√≠mite: 10
   - Marcar "Modo demo"
   - Click en "üöÄ Ejecutar Scraping"
   - Ver resultados

3. **Ver estad√≠sticas:**
   - Ir a "Estad√≠sticas"
   - Expandir sitios de inter√©s
   - Analizar distribuci√≥n de documentos

---

## Interpretaci√≥n de Resultados

### Estados de Documentos

Los documentos pueden tener tres estados:

- **`nuevo`**: Documento visto por primera vez
- **`modificado`**: Documento existente con cambios en el contenido
- **`sin_cambios`**: Documento ya visto sin cambios

### Estructura de Datos

#### Archivo de Documentos (`data/raw/<site_id>/documentos.json`)

```json
[
  {
    "site_id": "gaceta_oficial",
    "document_id": "GACETA_OFICIAL-LEY-1400-20250115",
    "titulo": "Ley 1400 - Ley de modificaci√≥n al C√≥digo Tributario",
    "tipo_norma": "Ley",
    "numero_norma": "1400",
    "fecha_publicacion": "2025-01-15",
    "url_detalle": "http://...",
    "url_pdf": "http://.../gaceta_1400.pdf",
    "path_pdf": null,
    "hash_contenido": "a1b2c3d4e5f6...",
    "estado": "nuevo",
    "metadata_extra": {"modo": "demo", "indice": 0},
    "fecha_scraping": "2025-01-18T10:30:15.123456"
  }
]
```

#### Archivo de √çndice (`data/index/<site_id>.json`)

```json
{
  "GACETA_OFICIAL-LEY-1400-20250115": {
    "hash": "a1b2c3d4e5f6...",
    "titulo": "Ley 1400...",
    "fecha_publicacion": "2025-01-15",
    "fecha_ultima_vez_visto": "2025-01-18T10:30:15",
    "estado": "nuevo",
    "url_pdf": "http://..."
  }
}
```

### C√≥mo Usar los Datos

#### En Python:

```python
import json
from pathlib import Path

# Leer documentos
with open('data/raw/gaceta_oficial/documentos.json', 'r') as f:
    documentos = json.load(f)

# Filtrar leyes
leyes = [d for d in documentos if d['tipo_norma'] == 'Ley']
print(f"Total leyes: {len(leyes)}")

# Documentos recientes
from datetime import datetime, timedelta
fecha_limite = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
recientes = [d for d in documentos if d['fecha_publicacion'] >= fecha_limite]
print(f"Documentos √∫ltimos 30 d√≠as: {len(recientes)}")
```

#### En la Terminal:

```bash
# Contar documentos
cat data/raw/gaceta_oficial/documentos.json | python -m json.tool | grep "document_id" | wc -l

# Ver solo nuevos
cat data/raw/gaceta_oficial/documentos.json | python -m json.tool | grep -A 5 '"estado": "nuevo"'

# Listar tipos de normas
cat data/raw/gaceta_oficial/documentos.json | python -m json.tool | grep "tipo_norma" | sort | uniq
```

---

## Soluci√≥n de Problemas

### Error: "No se encontr√≥ el cat√°logo"

**Problema:** El archivo `config/sites_catalog.yaml` no existe.

**Soluci√≥n:**
```bash
# Verificar que existe
ls config/sites_catalog.yaml

# Si no existe, verificar que est√°s en el directorio correcto
pwd  # Debe ser /path/to/bo-gov-scraper-buho
```

### Error: "No hay scraper implementado"

**Problema:** Intentaste scrapear un sitio de la Ola 2 u otro no implementado.

**Soluci√≥n:**
```bash
# Ver solo sitios implementados
python main.py list --estado implementado

# Usar solo site_ids de la Ola 1:
# - gaceta_oficial
# - tsj_genesis
# - tcp
# - asfi
# - sin
```

### Error: ImportError con m√≥dulos

**Problema:** Falta instalar dependencias.

**Soluci√≥n:**
```bash
# Reinstalar dependencias
pip install -r requirements.txt

# Verificar instalaci√≥n
pip list | grep -E "pyyaml|requests|beautifulsoup4|streamlit"
```

### Sin Datos en Estad√≠sticas

**Problema:** `python main.py stats` dice "No hay datos".

**Soluci√≥n:**
```bash
# Primero ejecuta alg√∫n scraper
python main.py demo-ola1 --limit 5

# Ahora verifica
python main.py stats

# Verifica manualmente
ls -lh data/index/
```

### Logs para Debugging

```bash
# Ver logs en tiempo real
tail -f scraper.log

# Buscar errores
grep ERROR scraper.log

# Logs de un sitio espec√≠fico
grep "gaceta_oficial" scraper.log
```

### Limpiar Datos y Empezar de Nuevo

```bash
# CUIDADO: Esto borra todos los datos scrapeados
rm -rf data/raw/*
rm -rf data/index/*
rm scraper.log

# Verificar limpieza
python main.py stats
# Debe decir "No hay datos de scraping todav√≠a"
```

---

## Pr√≥ximos Pasos

Una vez dominado el uso b√°sico:

1. **Fase 3 (Pr√≥xima):** Procesamiento de texto y OCR
2. **Fase 4 (Futura):** Integraci√≥n con Supabase
3. **Fase 5 (Futura):** API REST y sistema de notificaciones

---

**Fecha:** 2025-01-18
**Versi√≥n:** Fase 2 - Ola 1
**Autor:** Sistema Scraper Gubernamental Bolivia
