# FASE 9 - SITES REALES + PARSERS AVANZADOS + DELTA-UPDATE

## ğŸ“‹ DescripciÃ³n General

La FASE 9 del proyecto BÃšHO implementa un sistema completo de scraping de sitios gubernamentales bolivianos con las siguientes capacidades:

- **Scraping de sitios reales** de 5 instituciones del Estado
- **Parsing avanzado de PDFs** con detecciÃ³n automÃ¡tica de OCR
- **Sistema de actualizaciÃ³n incremental** (Delta-Update) para evitar procesamiento duplicado
- **CLI completo** con mÃºltiples comandos y opciones
- **Arquitectura multisite** extensible y modular

## ğŸ›ï¸ Sitios Implementados

### 1. Tribunal Constitucional Plurinacional (TCP)
- **CÃ³digo**: `tcp`
- **URL**: https://buscador.tcpbolivia.bo
- **Documentos**: Sentencias Constitucionales (SC, SCP, SCA)
- **Secciones parseadas**:
  - VISTOS
  - ANTECEDENTES
  - PROBLEMÃTICA
  - CONSIDERANDO
  - FUNDAMENTOS JURÃDICOS
  - POR TANTO

### 2. Tribunal Supremo de Justicia (TSJ)
- **CÃ³digo**: `tsj`
- **URL**: https://tsj.bo
- **Documentos**: Autos Supremos de diferentes salas
- **Salas**: Penal, Civil, Social, Contencioso Administrativa
- **Secciones parseadas**:
  - RESULTANDOS
  - CONSIDERANDOS
  - PARTE RESOLUTIVA

### 3. ContralorÃ­a General del Estado
- **CÃ³digo**: `contraloria`
- **URL**: https://www.contraloria.gob.bo
- **Documentos**: Resoluciones de ContralorÃ­a
- **Parsing**: Estructura por numerales romanos (I, II, III, etc.)

### 4. ASFI (Autoridad de SupervisiÃ³n del Sistema Financiero)
- **CÃ³digo**: `asfi`
- **URL**: https://www.asfi.gob.bo
- **Documentos**:
  - Resoluciones Administrativas
  - Circulares
  - Comunicados
- **Parsing**: Articulado y estructura formal

### 5. SIN (Servicio de Impuestos Nacionales)
- **CÃ³digo**: `sin`
- **URL**: https://www.impuestos.gob.bo
- **Documentos**:
  - RND (Normas de Directorio)
  - RA (Resoluciones Administrativas)
  - RM (Resoluciones Ministeriales)
- **Parsing**: Articulado especÃ­fico de normativa tributaria

## ğŸ—ï¸ Arquitectura del Sistema

```
bo-gov-scraper-buho/
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ core/                    # MÃ³dulos base
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_scraper.py     # Clase base abstracta
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py       # Parser avanzado con OCR
â”‚   â”‚   â”œâ”€â”€ delta_manager.py    # Sistema delta-update
â”‚   â”‚   â””â”€â”€ utils.py            # Utilidades comunes
â”‚   â”‚
â”‚   â”œâ”€â”€ sites/                   # Scrapers especÃ­ficos
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tcp_scraper.py      # Tribunal Constitucional
â”‚   â”‚   â”œâ”€â”€ tsj_scraper.py      # Tribunal Supremo
â”‚   â”‚   â”œâ”€â”€ contraloria_scraper.py
â”‚   â”‚   â”œâ”€â”€ asfi_scraper.py
â”‚   â”‚   â””â”€â”€ sin_scraper.py
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py             # API pÃºblica del paquete
â”‚
â”œâ”€â”€ outputs/                     # Salidas por sitio
â”‚   â”œâ”€â”€ tcp/
â”‚   â”‚   â”œâ”€â”€ index.json          # Ãndice delta-update
â”‚   â”‚   â”œâ”€â”€ pdfs/               # PDFs descargados
â”‚   â”‚   â””â”€â”€ json/               # Documentos parseados
â”‚   â”œâ”€â”€ tsj/
â”‚   â”œâ”€â”€ contraloria/
â”‚   â”œâ”€â”€ asfi/
â”‚   â””â”€â”€ sin/
â”‚
â”œâ”€â”€ docs/                        # DocumentaciÃ³n
â”‚   â””â”€â”€ FASE9_SITES_REALES.md   # Este archivo
â”‚
â”œâ”€â”€ main.py                      # CLI principal
â””â”€â”€ requirements.txt             # Dependencias
```

## ğŸ”§ InstalaciÃ³n

### Requisitos Previos

- Python 3.8 o superior
- Tesseract OCR (para PDFs escaneados)

### InstalaciÃ³n de Dependencias

```bash
# Instalar dependencias Python
pip install -r requirements.txt

# Instalar Tesseract OCR (Ubuntu/Debian)
sudo apt-get install tesseract-ocr tesseract-ocr-spa

# Instalar Tesseract OCR (macOS)
brew install tesseract tesseract-lang

# Instalar Tesseract OCR (Windows)
# Descargar desde: https://github.com/UB-Mannheim/tesseract/wiki
```

## ğŸš€ Uso del Sistema

### Comandos Principales

#### 1. Listar sitios disponibles

```bash
python main.py listar
```

#### 2. Ejecutar scraper de un sitio especÃ­fico

```bash
# Scraping completo
python main.py scrape tcp

# Solo documentos nuevos
python main.py scrape tcp --solo-nuevos

# Solo documentos modificados
python main.py scrape tsj --solo-modificados

# Limitar cantidad de documentos
python main.py scrape asfi --limit 10
```

#### 3. Actualizar todos los sitios

```bash
# Actualizar todos
python main.py actualizar-todos

# Actualizar solo algunos sitios
python main.py actualizar-todos --sitios tcp,tsj,asfi

# Solo documentos nuevos de todos los sitios
python main.py actualizar-todos --solo-nuevos

# Con lÃ­mite por sitio
python main.py actualizar-todos --limit 5
```

#### 4. Ver estadÃ­sticas

```bash
# EstadÃ­sticas de un sitio
python main.py estadisticas tcp

# Resumen de todos los sitios
python main.py resumen
```

#### 5. Limpiar Ã­ndice

```bash
# Eliminar del Ã­ndice documentos cuyos archivos no existen
python main.py limpiar-index tcp
```

## ğŸ“Š Sistema de ActualizaciÃ³n Incremental (Delta-Update)

### Funcionamiento

El sistema mantiene un archivo `index.json` por cada sitio que registra:

- **ID de documentos** procesados
- **Hash MD5** de cada PDF para detectar modificaciones
- **Metadata** (fecha, URL, tÃ­tulo, etc.)
- **EstadÃ­sticas** de ejecuciones

### Estructura del index.json

```json
{
  "site": "tcp",
  "created_at": "2024-01-15 10:30:00",
  "last_updated": "2024-01-15 14:25:30",
  "total_documents": 150,
  "documents": {
    "SCP_0001_2024": {
      "id": "SCP_0001_2024",
      "title": "SCP 0001/2024 - Amparo Constitucional",
      "date": "2024-01-15",
      "url": "https://...",
      "hash": "a1b2c3d4e5f6...",
      "pdf_path": "outputs/tcp/pdfs/SCP_0001_2024.pdf",
      "json_path": "outputs/tcp/json/SCP_0001_2024.json",
      "registered_at": "2024-01-15 10:35:12",
      "status": "processed"
    }
  },
  "statistics": {
    "total_processed": 15,
    "total_new": 5,
    "total_modified": 2,
    "total_skipped": 8,
    "last_run": "2024-01-15 14:25:30"
  }
}
```

### Ventajas

1. **Evita descargas duplicadas**: Solo descarga documentos nuevos o modificados
2. **DetecciÃ³n de cambios**: Compara hash MD5 para detectar modificaciones
3. **EstadÃ­sticas precisas**: Mantiene registro de todas las ejecuciones
4. **Eficiencia**: Reduce tiempo y ancho de banda

## ğŸ“„ Parsing Avanzado de PDFs

### DetecciÃ³n AutomÃ¡tica de OCR

El sistema detecta automÃ¡ticamente si un PDF es:
- **Digital**: Extrae texto directamente
- **Escaneado**: Aplica OCR con Tesseract

```python
from scraper.core.pdf_parser import PDFParser

parser = PDFParser("documento.pdf")
texto = parser.extract_text()  # Detecta automÃ¡ticamente
```

### Parsers EspecÃ­ficos por Tipo de Documento

Cada tipo de documento tiene un parser especializado:

```python
# Tribunal Constitucional
parsed = parser.parse_tribunal_constitucional()
# Retorna: vistos, antecedentes, problemÃ¡tica, considerando, fundamentos, por_tanto

# Tribunal Supremo
parsed = parser.parse_tribunal_supremo()
# Retorna: resultandos, considerandos, parte_resolutiva

# ContralorÃ­a
parsed = parser.parse_contraloria()
# Retorna: estructura por numerales romanos

# Parser genÃ©rico
parsed = parser.parse_generic()
# Retorna: metadata + texto completo
```

### Estructura de Salida

Cada documento parseado se guarda en formato JSON:

```json
{
  "id": "SCP_0001_2024",
  "title": "SCP 0001/2024 - Amparo Constitucional",
  "url": "https://...",
  "date": "2024-01-15",
  "tipo": "SCP",
  "parsed_data": {
    "tipo": "tribunal_constitucional",
    "metadata": {
      "filename": "SCP_0001_2024.pdf",
      "file_size": 524288,
      "is_scanned": false,
      "pages": 45
    },
    "secciones": {
      "vistos": "La presente acciÃ³n de amparo...",
      "antecedentes": "El accionante seÃ±ala que...",
      "problematica": "Se debe determinar si...",
      "considerando": "El Tribunal considera...",
      "fundamentos": "Conforme a los artÃ­culos...",
      "por_tanto": "RESUELVE: 1Â° CONCEDER..."
    },
    "info_adicional": {
      "magistrado_relator": "Dr. Juan PÃ©rez",
      "sala": "SALA PRIMERA",
      "tipo_accion": "AMPARO CONSTITUCIONAL"
    }
  },
  "processed_at": "2024-01-15 10:35:12"
}
```

## ğŸ”Œ API ProgramÃ¡tica

### Uso desde Python

```python
from scraper import get_scraper, SCRAPERS

# Obtener un scraper
scraper = get_scraper('tcp')

# Ejecutar scraping
stats = scraper.run(only_new=True, limit=10)

# Ver resultados
print(f"Nuevos: {stats['total_new']}")
print(f"Modificados: {stats['total_modified']}")

# Acceder al delta manager
delta = scraper.delta_manager
print(f"Total documentos: {delta.index['total_documents']}")
```

### Crear un Scraper Personalizado

```python
from scraper.core import BaseScraper, PDFParser

class MiScraper(BaseScraper):
    def __init__(self):
        super().__init__(
            site_name="mi_sitio",
            base_url="https://ejemplo.gob.bo"
        )

    def fetch_document_list(self):
        # Implementar lÃ³gica de scraping
        return [
            {
                'id': 'DOC_001',
                'title': 'Documento 1',
                'url': 'https://...',
                'date': '2024-01-15'
            }
        ]

    def parse_document(self, pdf_path):
        parser = PDFParser(pdf_path)
        return parser.parse_generic()
```

## ğŸ› Troubleshooting

### Problema: Error de OCR

**SÃ­ntoma**: `Error en extracciÃ³n OCR`

**SoluciÃ³n**:
```bash
# Verificar instalaciÃ³n de Tesseract
tesseract --version

# Verificar idioma espaÃ±ol instalado
tesseract --list-langs | grep spa

# Reinstalar si es necesario
sudo apt-get install --reinstall tesseract-ocr-spa
```

### Problema: PDF no se descarga

**SÃ­ntoma**: `Error descargando PDF: 403 Forbidden`

**SoluciÃ³n**:
- Verificar que el sitio web estÃ© accesible
- Algunos sitios requieren autenticaciÃ³n o cookies
- Modificar headers en el scraper especÃ­fico

### Problema: Secciones no se extraen

**SÃ­ntoma**: Las secciones del PDF estÃ¡n vacÃ­as

**SoluciÃ³n**:
- El formato del PDF puede haber cambiado
- Ajustar los patrones regex en el parser especÃ­fico
- Usar modo debug para ver el texto extraÃ­do:

```python
parser = PDFParser("documento.pdf")
texto = parser.extract_text()
print(texto)  # Ver texto raw para ajustar patrones
```

### Problema: Documentos duplicados

**SÃ­ntoma**: Se procesan documentos ya existentes

**SoluciÃ³n**:
```bash
# Verificar Ã­ndice
python main.py estadisticas tcp

# Limpiar Ã­ndice si es necesario
python main.py limpiar-index tcp

# Usar flag --solo-nuevos
python main.py scrape tcp --solo-nuevos
```

## ğŸ“ˆ Consideraciones de Rendimiento

### Optimizaciones Implementadas

1. **Delta-Update**: Evita procesamiento innecesario
2. **Streaming de descarga**: No carga archivos completos en memoria
3. **Cache de Ã­ndices**: Mantiene Ã­ndices en memoria durante ejecuciÃ³n
4. **LÃ­mites configurables**: Permite procesar en lotes

### Recomendaciones

```bash
# Para primera ejecuciÃ³n completa
python main.py actualizar-todos --limit 50

# Para actualizaciones diarias
python main.py actualizar-todos --solo-nuevos

# Para verificar cambios
python main.py actualizar-todos --solo-modificados

# Para sitio especÃ­fico con mucho volumen
python main.py scrape tcp --solo-nuevos --limit 100
```

## ğŸ”’ Consideraciones de Seguridad

1. **ValidaciÃ³n de URLs**: El sistema valida URLs antes de descargar
2. **TamaÃ±o de archivos**: LÃ­mites en tamaÃ±o de descarga
3. **SanitizaciÃ³n de nombres**: Limpia nombres de archivos
4. **Manejo de errores**: No expone informaciÃ³n sensible en errores

## ğŸš¦ Roadmap Futuro

### Posibles Mejoras

- [ ] Soporte para mÃ¡s sitios gubernamentales
- [ ] BÃºsqueda full-text en documentos parseados
- [ ] API REST para consultas
- [ ] Dashboard web con Streamlit
- [ ] ExportaciÃ³n a formatos adicionales (XML, CSV)
- [ ] Notificaciones de nuevos documentos
- [ ] IntegraciÃ³n con bases de datos
- [ ] AnÃ¡lisis de texto con NLP
- [ ] ExtracciÃ³n de entidades (personas, leyes citadas)
- [ ] ClasificaciÃ³n automÃ¡tica de documentos

## ğŸ“ Licencia

Este proyecto es parte del sistema BÃšHO de scraping gubernamental.

## ğŸ‘¥ Contribuciones

Para contribuir:
1. Crear un nuevo scraper en `scraper/sites/`
2. Heredar de `BaseScraper`
3. Implementar `fetch_document_list()` y `parse_document()`
4. Agregar a `SCRAPERS` en `scraper/__init__.py`
5. Documentar el nuevo sitio en este archivo

## ğŸ“ Soporte

Para reportar problemas o sugerencias, abrir un issue en el repositorio del proyecto.

---

**BÃšHO FASE 9** - Sistema Completo de Scraping Gubernamental de Bolivia
VersiÃ³n 9.0.0 - Enero 2025
