# UPGRADE: Scraping HistÃ³rico Real + Metadata Profesional

**Fecha**: 2025-11-18
**VersiÃ³n**: 2.0 - Scraping Real y Metadata Site-Aware

## ğŸ¯ Resumen de Cambios

Este upgrade transforma el proyecto de **datos de ejemplo** a **scraping histÃ³rico real** con **metadata profesional especÃ­fica por sitio**.

---

## ğŸ“Š BLOQUE 1: SCRAPING HISTÃ“RICO REAL

### âœ… Todos los Scrapers Actualizados

**8 sitios con implementaciÃ³n real:**
- âœ… **Gaceta Oficial** (`gaceta_scraper.py`)
- âœ… **TCP** - Tribunal Constitucional (`tcp_scraper.py`)
- âœ… **TSJ** - Tribunal Supremo (`tsj_scraper.py`)
- âœ… **ASFI** - SupervisiÃ³n Financiera (`asfi_scraper.py`)
- âœ… **SIN** - Impuestos Nacionales (`sin_scraper.py`)
- âœ… **ContralorÃ­a** (`contraloria_scraper.py`)
- âœ… **ATT** - Telecomunicaciones y Transportes (`att_scraper.py`)
- âœ… **MinTrabajo** - Ministerio de Trabajo (`mintrabajo_scraper.py`)

### ğŸ”§ CaracterÃ­sticas de Scraping Real

1. **Scraping con requests + BeautifulSoup**
   - Descarga HTML real de los sitios
   - Parseo de tablas, enlaces, y estructuras HTML
   - MÃºltiples patrones de extracciÃ³n (tablas, cards, enlaces directos)

2. **Manejo Robusto de Errores**
   - Reintentos con URLs alternativas
   - Logging detallado de fallos
   - Fallback a mÃ©todos alternativos

3. **Descarga Real de PDFs**
   - Descarga streaming con chunks
   - ValidaciÃ³n de tamaÃ±o de archivo
   - Manejo de timeouts y reconexiones

4. **ExtracciÃ³n Inteligente de Metadata**
   - NÃºmero de norma desde texto y URL
   - AÃ±o automÃ¡tico con regex
   - Tipo de documento desde clasificaciÃ³n
   - ID Ãºnico basado en hash cuando no hay nÃºmero

### ğŸ“ Ejemplo de Metadata ExtraÃ­da

```python
{
    'id_documento': 'tcp_sc_123_2024',
    'tipo_documento': 'Sentencia Constitucional',
    'numero_norma': '123/2024',
    'anio': 2024,
    'fecha': '2024-06-15',
    'titulo': 'SC 123/2024 - AcciÃ³n de Amparo Constitucional',
    'url': 'https://www.tcpbolivia.bo/sentencias/2024/sc-123-2024.pdf',
    'sumilla': 'AcciÃ³n de Amparo Constitucional',
    'metadata_extra': {
        'tipo_accion': 'Amparo Constitucional',
        'tribunal': 'TCP',
        'metodo_scraping': 'real'
    }
}
```

---

## ğŸ“Š BLOQUE 2: METADATA PROFESIONAL SITE-AWARE

### âœ… Extractor de Metadata Mejorado

**Archivo**: `scraper/metadata_extractor.py`

#### Nuevo MÃ©todo: `extraer_metadata_sitio_especifico()`

Extrae metadata **especÃ­fica por sitio** con lÃ³gica inteligente.

### ğŸ¯ Metadata EspecÃ­fica por Sitio

#### TCP - Tribunal Constitucional
```python
{
    'tribunal': 'TCP',
    'tipo_accion_constitucional': 'Amparo Constitucional',
    'sala': 'Primera Sala',
    'magistrado_ponente': 'Carlos Alberto CalderÃ³n'
}
```

#### TSJ - Tribunal Supremo
```python
{
    'tribunal': 'TSJ',
    'materia': 'Civil',
    'sala': 'Sala Civil',
    'tipo_recurso': 'CasaciÃ³n'
}
```

#### ASFI - SupervisiÃ³n Financiera
```python
{
    'entidad_reguladora': 'ASFI',
    'tipo_entidad_regulada': 'Banco',
    'ambito_regulatorio': 'GestiÃ³n de Riesgos'
}
```

#### SIN - Impuestos Nacionales
```python
{
    'entidad': 'SIN',
    'tipo_tributo': 'IVA',
    'procedimiento': 'FiscalizaciÃ³n'
}
```

#### Gaceta Oficial
```python
{
    'fuente_publicacion': 'Gaceta Oficial',
    'edicion_gaceta': 1234,
    'ministerio_emisor': 'Ministerio de EconomÃ­a'
}
```

### ğŸ”— IntegraciÃ³n en Pipeline

El pipeline ahora **automÃ¡ticamente** extrae metadata site-aware:

```python
# Pipeline ejecuta esto automÃ¡ticamente:
metadata_sitio = metadata_extractor.extraer_metadata_sitio_especifico(
    site_id=site_id,
    texto=texto,
    titulo=documento.titulo,
    documento_base=documento.metadata
)
documento.metadata.update(metadata_sitio)
```

---

## ğŸš€ CÃ“MO USAR

### 1. Listar Sitios Disponibles

```bash
python main.py listar
```

**Salida**:
```
ğŸ¦‰ BÃšHO - Sitios disponibles
--------------------------------------------------------------------------------
ğŸ“ Gaceta Oficial de Bolivia
   ID: gaceta_oficial
   Prioridad: 1 | Ola: 1
   Activo: âœ“
...
Total sitios activos: 8
```

### 2. Scraping HistÃ³rico Completo

```bash
# Scraping completo (recorre TODO el histÃ³rico)
python main.py scrape gaceta_oficial --mode full

# Con lÃ­mite de documentos
python main.py scrape tcp --mode full --limit 50

# Scraping incremental (solo nuevos)
python main.py scrape tsj --mode delta
```

### 3. Scraping de Todos los Sitios

```bash
# Scraping masivo de todos los sitios activos
python main.py scrape all --mode full --limit 20
```

### 4. Ver Archivos Generados

**Estructura de archivos:**

```
data/
â”œâ”€â”€ normalized/
â”‚   â”œâ”€â”€ gaceta_oficial/
â”‚   â”‚   â”œâ”€â”€ json/
â”‚   â”‚   â”‚   â””â”€â”€ gaceta_ley_001_2024.json  â† JSON con metadata completa
â”‚   â”‚   â””â”€â”€ text/
â”‚   â”‚       â””â”€â”€ gaceta_ley_001_2024.txt   â† Texto extraÃ­do
â”‚   â”œâ”€â”€ tcp/
â”‚   â”œâ”€â”€ tsj/
â”‚   â””â”€â”€ ...

exports/
â”œâ”€â”€ gaceta_oficial/
â”‚   â””â”€â”€ 20251118_101223/
â”‚       â”œâ”€â”€ documentos.csv               â† CSV con metadata de documentos
â”‚       â”œâ”€â”€ articulos.csv                â† CSV con artÃ­culos
â”‚       â”œâ”€â”€ registro_historico.jsonl     â† JSONL con historial
â”‚       â””â”€â”€ reporte_scraping.json        â† Reporte de la sesiÃ³n
â””â”€â”€ ...
```

### 5. Ejemplo de JSON Normalizado

**Archivo**: `data/normalized/tcp/json/tcp_sc_123_2024.json`

```json
{
  "id_documento": "tcp_sc_123_2024",
  "site": "tcp",
  "tipo_documento": "Sentencia Constitucional",
  "numero_norma": "123/2024",
  "fecha": "2024-06-15",
  "titulo": "SC 123/2024 - AcciÃ³n de Amparo Constitucional",
  "url_origen": "https://www.tcpbolivia.bo/sentencias/2024/sc-123-2024.pdf",
  "ruta_json": "/path/to/tcp_sc_123_2024.json",
  "texto_completo": "...",
  "metadata": {
    "tipo_norma": "Sentencia Constitucional",
    "jerarquia": 10,
    "area_principal": "constitucional",
    "areas_derecho": ["constitucional"],
    "estado_vigencia": "vigente",
    "palabras_clave": ["amparo", "derechos fundamentales", "protecciÃ³n"],
    "tribunal": "TCP",
    "tipo_accion_constitucional": "Amparo Constitucional",
    "sala": "Primera Sala",
    "estadisticas": {
      "total_caracteres": 15000,
      "total_palabras": 2500,
      "estimado_paginas": 5
    }
  },
  "articulos": []
}
```

---

## ğŸ“ Archivos Modificados

### Scrapers (8 archivos)
- âœ… `scraper/sites/gaceta_scraper.py` - 402 lÃ­neas (NUEVO COMPLETO)
- âœ… `scraper/sites/tcp_scraper.py` - 385 lÃ­neas (NUEVO COMPLETO)
- âœ… `scraper/sites/tsj_scraper.py` - 200 lÃ­neas (REESCRITO)
- âœ… `scraper/sites/asfi_scraper.py` - 175 lÃ­neas (REESCRITO)
- âœ… `scraper/sites/sin_scraper.py` - 175 lÃ­neas (REESCRITO)
- âœ… `scraper/sites/contraloria_scraper.py` - 175 lÃ­neas (REESCRITO)
- âœ… `scraper/sites/att_scraper.py` - 150 lÃ­neas (REESCRITO)
- âœ… `scraper/sites/mintrabajo_scraper.py` - 150 lÃ­neas (REESCRITO)

### Metadata Extractor
- âœ… `scraper/metadata_extractor.py` - +120 lÃ­neas (MÃ‰TODOS SITE-AWARE AGREGADOS)

### Pipeline
- âœ… `scraper/pipeline.py` - 7 lÃ­neas modificadas (INTEGRACIÃ“N SITE-AWARE)

---

## ğŸ”§ Dependencias Nuevas

```bash
pip install beautifulsoup4 requests lxml
```

**Ya instaladas en el entorno actual**.

---

## âš ï¸ IMPORTANTE: Limitaciones del Scraping Real

### 1. Conectividad
- **Requiere acceso a internet** para conectarse a sitios gubernamentales bolivianos
- Los sitios pueden estar temporalmente fuera de lÃ­nea
- Algunos sitios pueden tener captchas o protecciones anti-scraping

### 2. Estructura HTML Variable
- Los sitios gubernamentales pueden cambiar su HTML en cualquier momento
- Los selectores CSS/XPATH pueden necesitar ajustes
- **RecomendaciÃ³n**: Revisar y ajustar selectores periÃ³dicamente

### 3. Ajustes Necesarios
Cada scraper tiene comentarios `# AJUSTAR ESTOS SELECTORES` indicando dÃ³nde personalizar para el HTML real del sitio.

**Ejemplo** (`gaceta_scraper.py:132`):
```python
# AJUSTAR ESTOS SELECTORES segÃºn la estructura HTML real del sitio
# PatrÃ³n 1: Buscar enlaces a ediciones
enlaces_ediciones = soup.select('a[href*="/ediciones/"]')

if not enlaces_ediciones:
    # PatrÃ³n 2: Buscar tabla de ediciones
    enlaces_ediciones = soup.select('table.ediciones a')

if not enlaces_ediciones:
    # PatrÃ³n 3: Buscar divs con clase relacionada
    enlaces_ediciones = soup.select('.gaceta-edicion a, .edicion-link')
```

### 4. Delays y Rate Limiting
- Todos los scrapers respetan `delay_entre_requests` configurado en `sites_catalog.yaml`
- **RecomendaciÃ³n**: No hacer scraping masivo sin pausas
- Respetar los tÃ©rminos de servicio de cada sitio

---

## ğŸ“Š EstadÃ­sticas del Upgrade

| Componente | Antes | DespuÃ©s |
|---|---|---|
| **Scrapers reales** | 0 | 8 |
| **Metadata site-aware** | No | SÃ­ |
| **Campos metadata TCP** | 10 | 15+ |
| **Campos metadata TSJ** | 10 | 14+ |
| **Campos metadata ASFI** | 10 | 13+ |
| **Campos metadata SIN** | 10 | 13+ |
| **Descarga real PDFs** | No | SÃ­ |
| **Manejo de errores** | BÃ¡sico | Robusto |

---

## ğŸ¯ PrÃ³ximos Pasos Recomendados

1. **Ajustar selectores HTML** para cada sitio segÃºn estructura real
2. **Probar scraping real** con conectividad a internet
3. **Validar metadata extraÃ­da** contra documentos reales
4. **Agregar mÃ¡s sitios** (hay ~30 sitios estatales bolivianos)
5. **Mejorar UI Streamlit** para explotar metadata nueva
6. **Implementar cache** para reducir requests repetidos
7. **Agregar exportaciÃ³n a Supabase** (ya implementado en rama `sync`)

---

## ğŸ“ Soporte

Para ajustar scrapers a la estructura HTML real de los sitios:

1. Inspeccionar HTML del sitio con DevTools del navegador
2. Identificar selectores CSS correctos
3. Actualizar los patrones de bÃºsqueda en el scraper
4. Probar con `--limit 5` antes de scraping masivo

---

## âœ… Checklist de VerificaciÃ³n

- [x] Scrapers implementados con scraping real
- [x] Metadata extractor con lÃ³gica site-aware
- [x] Pipeline integrado con metadata site-aware
- [x] Descarga real de PDFs funcional
- [x] Manejo robusto de errores
- [x] Dependencias instaladas (beautifulsoup4, requests)
- [x] DocumentaciÃ³n completa
- [ ] Ajustar selectores HTML segÃºn sitios reales (requiere internet)
- [ ] Validar con scraping en producciÃ³n
- [ ] Mejorar UI para nuevos campos

---

**Â¡El sistema estÃ¡ listo para scraping histÃ³rico REAL!** ğŸš€
