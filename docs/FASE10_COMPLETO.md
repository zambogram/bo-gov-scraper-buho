# FASE 10 - DocumentaciÃ³n TÃ©cnica Completa

## ğŸ¯ VisiÃ³n General

FASE 10 representa la implementaciÃ³n completa del sistema BÃšHO (Bolivian Government Document Scraper), integrando todas las capacidades de scraping, parsing, exportaciÃ³n y sincronizaciÃ³n en una plataforma unificada.

## ğŸ“ Arquitectura General

### Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BÃšHO System                            â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚     CLI     â”‚   â”‚  Streamlit  â”‚   â”‚   Scheduler  â”‚       â”‚
â”‚  â”‚   main.py   â”‚   â”‚     UI      â”‚   â”‚  run_daily   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                  â”‚                  â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                            â”‚                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚    Scraper Module         â”‚                   â”‚
â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
â”‚              â”‚ â€¢ BaseScraper             â”‚                   â”‚
â”‚              â”‚ â€¢ TCPScraper              â”‚                   â”‚
â”‚              â”‚ â€¢ TSJScraper              â”‚                   â”‚
â”‚              â”‚ â€¢ ASFIScraper             â”‚                   â”‚
â”‚              â”‚ â€¢ SINScraper              â”‚                   â”‚
â”‚              â”‚ â€¢ ContraloriaScraper      â”‚                   â”‚
â”‚              â”‚ â€¢ LegalParser             â”‚                   â”‚
â”‚              â”‚ â€¢ MetadataExtractor       â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                            â”‚                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚    Storage Layer          â”‚                   â”‚
â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
â”‚              â”‚ â€¢ JSON Index              â”‚                   â”‚
â”‚              â”‚ â€¢ JSON Articles           â”‚                   â”‚
â”‚              â”‚ â€¢ JSONL Exports           â”‚                   â”‚
â”‚              â”‚ â€¢ Sync Logs               â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                            â”‚                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚    Sync Module            â”‚                   â”‚
â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
â”‚              â”‚ â€¢ SupabaseSync            â”‚                   â”‚
â”‚              â”‚ â€¢ Duplicate Detection     â”‚                   â”‚
â”‚              â”‚ â€¢ Stats Aggregation       â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                            â”‚                                  â”‚
â”‚                            â–¼                                  â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                   â”‚   Supabase DB  â”‚                         â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Componentes Principales

### 1. Scraper Module

#### BaseScraper

Clase base abstracta que proporciona funcionalidad comÃºn a todos los scrapers.

**Responsabilidades:**
- GestiÃ³n de sesiones HTTP
- Almacenamiento de Ã­ndices y artÃ­culos
- CÃ¡lculo de MD5 para detecciÃ³n de cambios
- ExportaciÃ³n a JSONL
- EstadÃ­sticas

**MÃ©todos Principales:**
```python
class BaseScraper:
    def scrape(limit, only_new) -> Dict
    def load_index() -> List[Dict]
    def save_index(documents)
    def load_articles() -> List[Dict]
    def save_articles(articles)
    def compute_md5(content) -> str
    def export_jsonl() -> Dict
    def get_stats() -> Dict
```

#### Scrapers EspecÃ­ficos

Cada scraper hereda de `BaseScraper` e implementa la lÃ³gica especÃ­fica del sitio:

1. **TCPScraper**: Tribunal Constitucional Plurinacional
   - Sentencias constitucionales
   - Formato: SC-XXXX/YYYY

2. **TSJScraper**: Tribunal Supremo de Justicia
   - Autos supremos
   - MÃºltiples salas

3. **ASFIScraper**: Autoridad de SupervisiÃ³n del Sistema Financiero
   - Resoluciones financieras
   - Normativa bancaria

4. **SINScraper**: Servicio de Impuestos Nacionales
   - Resoluciones normativas tributarias
   - Circulares

5. **ContraloriaScraper**: ContralorÃ­a General del Estado
   - Informes de auditorÃ­a
   - Resoluciones administrativas

### 2. Legal Parser

#### LegalParser

Parser especializado para documentos legales bolivianos.

**Funcionalidades:**
- DetecciÃ³n automÃ¡tica de artÃ­culos
- ExtracciÃ³n de estructura legal
- Soporte para mÃºltiples formatos
- PreservaciÃ³n de contexto

**Patrones Soportados:**
```regex
ArtÃ­culo \d+[Â°Âº]?
Art\. \d+[Â°Âº]?
ARTÃCULO \d+[Â°Âº]?
```

**Ejemplo de Parsing:**
```python
from scraper.parser import LegalParser

document = {
    'id': 'tcp-000001',
    'content': 'ArtÃ­culo 1.- Contenido...\nArtÃ­culo 2.- MÃ¡s contenido...'
}

articles = LegalParser.parse_document(document)
# [
#   {'id': 'tcp-000001-art-001', 'article_number': 1, 'content': '...'},
#   {'id': 'tcp-000001-art-002', 'article_number': 2, 'content': '...'}
# ]
```

### 3. Metadata Extractor

#### MetadataExtractor

Extractor inteligente de metadatos de documentos legales.

**Capacidades:**
- ExtracciÃ³n de fechas (mÃºltiples formatos)
- ExtracciÃ³n de nÃºmeros de documento
- DetecciÃ³n de tipo de documento
- Enriquecimiento automÃ¡tico

**Formatos de Fecha Soportados:**
- "15 de enero de 2024"
- "15/01/2024"
- "2024-01-15"

### 4. Supabase Sync Module

#### SupabaseSync

Gestor de sincronizaciÃ³n bidireccional con Supabase.

**CaracterÃ­sticas:**
- DetecciÃ³n de duplicados por MD5
- InserciÃ³n de nuevos documentos
- ActualizaciÃ³n de modificados
- Logging detallado
- Manejo de errores

**Flujo de SincronizaciÃ³n:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Local JSON â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check MD5       â”‚
â”‚ in Supabase     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€ Exists + Same MD5 â”€â”€â†’ Skip
       â”‚
       â”œâ”€â”€â”€ Exists + Diff MD5 â”€â”€â†’ Update
       â”‚
       â””â”€â”€â”€ Not Exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Insert
```

**Funciones PÃºblicas:**
```python
sync_documents_to_supabase(site, only_new)
sync_articles_to_supabase(site, only_new)
sync_all_sites(only_new)
verify_duplicates(site)
get_stats_from_supabase(site)
```

### 5. Streamlit UI

#### Interfaz Completa

**Componentes:**

1. **Sidebar**
   - Selector de sitio
   - EstadÃ­sticas en tiempo real
   - Controles de scraping
   - Botones de exportaciÃ³n
   - Botones de sincronizaciÃ³n

2. **PestaÃ±a Documents**
   - Tabla interactiva
   - Visor de documentos
   - Filtros
   - MÃ©tricas

3. **PestaÃ±a Articles**
   - Tabla de artÃ­culos
   - Visor de contenido
   - Metadatos expandibles

4. **PestaÃ±a Statistics**
   - GrÃ¡ficas de volumen
   - ComparaciÃ³n entre sitios
   - DistribuciÃ³n (pie chart)
   - MÃ©tricas agregadas

5. **PestaÃ±a Logs**
   - VisualizaciÃ³n de logs de sync
   - Historial de operaciones

### 6. Scheduler

#### Scheduler AutomÃ¡tico

Sistema de tareas programadas para scraping automÃ¡tico.

**CaracterÃ­sticas:**
- Scraping diario a las 2 AM
- SincronizaciÃ³n automÃ¡tica con Supabase
- Logging detallado
- ExportaciÃ³n automÃ¡tica
- Soporte para ejecuciÃ³n inmediata

**Modos de EjecuciÃ³n:**

1. **Inmediato**: `python scheduler/run_daily.py --now`
2. **Daemon**: `python scheduler/run_daily.py --daemon`
3. **Systemd**: IntegraciÃ³n con systemd para ejecuciÃ³n persistente

## ğŸ“Š Flujos de Datos

### Flujo de Scraping

```
1. Iniciar Scraper
   â†“
2. Cargar Ãndice Existente
   â†“
3. Hacer Request HTTP
   â†“
4. Parsear HTML
   â†“
5. Extraer Contenido
   â†“
6. Calcular MD5
   â†“
7. Comparar con Existente
   â†“
8. Clasificar (Nuevo/Modificado/Sin Cambios)
   â†“
9. Actualizar Ãndice
   â†“
10. Guardar JSON
```

### Flujo de Parsing

```
1. Cargar Documentos
   â†“
2. Para cada documento:
   â”œâ”€ Aplicar RegEx de ArtÃ­culos
   â”œâ”€ Extraer NÃºmeros
   â”œâ”€ Extraer Contenido
   â””â”€ Crear Objeto Article
   â†“
3. Guardar Articles JSON
```

### Flujo de ExportaciÃ³n

```
1. Cargar JSON
   â†“
2. Para cada documento/artÃ­culo:
   â””â”€ Escribir lÃ­nea JSONL
   â†“
3. Guardar exports/{site}/documents.jsonl
4. Guardar exports/{site}/articles.jsonl
```

### Flujo de SincronizaciÃ³n

```
1. Cargar JSON Local
   â†“
2. Conectar a Supabase
   â†“
3. Para cada documento:
   â”œâ”€ Query por MD5
   â”œâ”€ Si existe y MD5 igual â†’ Skip
   â”œâ”€ Si existe y MD5 diferente â†’ Update
   â””â”€ Si no existe â†’ Insert
   â†“
4. Log Results
   â†“
5. Guardar logs/sync/{timestamp}.json
```

## ğŸ—„ï¸ Esquemas de Datos

### Document Schema

```json
{
  "id": "tcp-000001",
  "site": "tcp",
  "url": "https://www.tcpbolivia.bo/...",
  "title": "Sentencia Constitucional SC-0001/2024",
  "content": "Texto completo del documento...",
  "md5": "a1b2c3d4e5f6...",
  "status": "new",
  "scraped_at": "2024-01-15T10:30:00",
  "metadata": {
    "tipo": "Sentencia Constitucional",
    "numero": "SC-0001/2024",
    "fecha": "2024-01-15"
  }
}
```

### Article Schema

```json
{
  "id": "tcp-000001-art-001",
  "document_id": "tcp-000001",
  "site": "tcp",
  "article_number": 1,
  "content": "Contenido del artÃ­culo...",
  "metadata": {
    "document_title": "Sentencia Constitucional SC-0001/2024",
    "document_url": "https://...",
    "document_type": "Sentencia Constitucional",
    "parsed_at": "2024-01-15T10:30:00"
  }
}
```

### Sync Log Schema

```json
{
  "site": "tcp",
  "timestamp": "2024-01-15T10:30:00",
  "inserted": 5,
  "updated": 2,
  "skipped": 10,
  "errors": 0
}
```

## ğŸ”§ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos

- Python 3.8+
- pip
- ConexiÃ³n a Internet
- (Opcional) Cuenta de Supabase

### Pasos de InstalaciÃ³n

1. **Clonar Repositorio**
   ```bash
   git clone https://github.com/zambogram/bo-gov-scraper-buho.git
   cd bo-gov-scraper-buho
   ```

2. **Crear Entorno Virtual**
   ```bash
   python -m venv venv
   source venv/bin/activate
   ```

3. **Instalar Dependencias**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar Supabase** (Opcional)
   ```bash
   cp .env.example .env
   # Editar .env con credenciales
   ```

5. **Crear Tablas en Supabase**
   - Ejecutar SQL schema (ver README.md)

## ğŸ§ª Testing

### Tests Manuales

```bash
# Test 1: Listar sitios
python main.py listar

# Test 2: Scrape individual
python main.py scrape tcp --limit 2

# Test 3: Scrape todos
python main.py actualizar-todos --limit 2

# Test 4: Export
python main.py export-jsonl tcp

# Test 5: UI
streamlit run app/streamlit_app.py

# Test 6: Scheduler
python scheduler/run_daily.py --now
```

### ValidaciÃ³n de Datos

```python
# Verificar Ã­ndice
import json
with open('data/index/tcp_index.json') as f:
    docs = json.load(f)
    print(f"Total documents: {len(docs)}")
    print(f"First doc: {docs[0]}")

# Verificar artÃ­culos
with open('data/articles/tcp_articles.json') as f:
    arts = json.load(f)
    print(f"Total articles: {len(arts)}")

# Verificar export
with open('exports/tcp/documents.jsonl') as f:
    lines = f.readlines()
    print(f"JSONL lines: {len(lines)}")
```

## ğŸ“ˆ MÃ©tricas y Monitoreo

### MÃ©tricas Clave

1. **Scraping**
   - Documentos nuevos por dÃ­a
   - Documentos modificados por dÃ­a
   - Tasa de error
   - Tiempo de ejecuciÃ³n

2. **Parsing**
   - ArtÃ­culos extraÃ­dos por documento
   - Tasa de Ã©xito de parsing

3. **SincronizaciÃ³n**
   - Documentos sincronizados
   - Duplicados detectados
   - Fallos de sincronizaciÃ³n

### Logs

```
logs/
â”œâ”€â”€ sync/
â”‚   â”œâ”€â”€ sync_tcp_20240115_103000.json
â”‚   â”œâ”€â”€ sync_tsj_20240115_103100.json
â”‚   â””â”€â”€ ...
â””â”€â”€ auto/
    â”œâ”€â”€ scraping_20240115_020000.json
    â””â”€â”€ ...
```

## ğŸš€ Deployment

### ProducciÃ³n

1. **Configurar Servidor**
   - Ubuntu 20.04+ / Debian 11+
   - Python 3.8+
   - Nginx (para UI)

2. **Setup Systemd**
   ```bash
   sudo cp buho-scraper.service /etc/systemd/system/
   sudo systemctl enable buho-scraper
   sudo systemctl start buho-scraper
   ```

3. **Setup Nginx para Streamlit**
   ```nginx
   location / {
       proxy_pass http://localhost:8501;
       proxy_http_version 1.1;
       proxy_set_header Upgrade $http_upgrade;
       proxy_set_header Connection "upgrade";
   }
   ```

## ğŸ”’ Seguridad

### Mejores PrÃ¡cticas

1. **Credenciales**
   - Nunca commitear .env
   - Usar variables de entorno
   - Rotar keys regularmente

2. **Rate Limiting**
   - Respetar robots.txt
   - Implementar delays entre requests
   - No sobrecargar servidores

3. **ValidaciÃ³n**
   - Validar datos antes de insertar
   - Sanitizar contenido
   - Escapar SQL (Supabase lo hace automÃ¡ticamente)

## ğŸ“ Extensibilidad

### Agregar Nuevo Scraper

1. Crear `scraper/nuevo_scraper.py`:
   ```python
   from scraper.base_scraper import BaseScraper

   class NuevoScraper(BaseScraper):
       def __init__(self):
           super().__init__(name='nuevo', base_url='https://...')

       def scrape(self, limit=None, only_new=False):
           # Implementar lÃ³gica
           pass
   ```

2. Registrar en `scraper/__init__.py`:
   ```python
   from scraper.nuevo_scraper import NuevoScraper

   SCRAPERS = {
       # ...
       'nuevo': NuevoScraper
   }
   ```

## ğŸ“š Referencias

- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Supabase Python Client](https://supabase.com/docs/reference/python/introduction)
- [Schedule Library](https://schedule.readthedocs.io/)

## ğŸ¤ Soporte

Para preguntas o problemas:
- GitHub Issues: https://github.com/zambogram/bo-gov-scraper-buho/issues
- Email: [contacto]

---

**VersiÃ³n**: FASE 10
**Fecha**: 2025-01-15
**Autor**: Zambogram
