# TCP Jurisprudencia Scraper

## üéØ Objetivo

Scraper especializado para obtener **TODA la jurisprudencia** del Tribunal Constitucional Plurinacional (TCP) mediante sus APIs internas.

**IMPORTANTE**: Este NO es el scraper de gacetas TCP (tomos/gu√≠as). Este scraper obtiene sentencias, resoluciones y autos constitucionales.

## üåê Sitios objetivo

- https://buscador.tcpbolivia.bo/
  - `/busqueda-resolucion`
  - `/busqueda-unificacion`
  - `/busqueda-avocacion`
  - `/busqueda-jurisprudencia`
  - `/busqueda-fecha-ingreso`
  - `/busqueda-fecha-resolucion`

- https://jurisprudencia.tcpbolivia.bo/

## üèóÔ∏è Arquitectura

### Dise√±o basado en API

Este scraper est√° **100% basado en APIs REST**, NO en scraping HTML.

**Caracter√≠sticas clave:**
- Configuraci√≥n externalizada en `config/sites_catalog.yaml`
- Endpoints de API configurables
- Paginaci√≥n autom√°tica con validaci√≥n
- Iteraci√≥n por a√±os para cobertura total
- Mapeo de campos JSON personalizable

### Estructura de archivos

```
bo-gov-scraper-buho/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ sites_catalog.yaml         # Configuraci√≥n del scraper
‚îú‚îÄ‚îÄ scraper/
‚îÇ   ‚îî‚îÄ‚îÄ sites/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ tcp_jurisprudencia_scraper.py  # Scraper principal
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ validar_cobertura_tcp.py   # Script de validaci√≥n
‚îî‚îÄ‚îÄ main.py                        # CLI actualizado
```

## üìã Configuraci√≥n

### 1. Inspeccionar la API real

‚ö†Ô∏è **IMPORTANTE**: Antes de usar el scraper, debes inspeccionar el sitio web con las herramientas de desarrollador del navegador.

**Pasos:**

1. Abre https://buscador.tcpbolivia.bo/ en Chrome/Firefox
2. Abre DevTools (F12) ‚Üí pesta√±a **Network**
3. Filtra por **XHR** o **Fetch**
4. Realiza una b√∫squeda en el sitio
5. Identifica la request de la API

**Datos que necesitas obtener:**

- **URL del endpoint**: ej. `https://buscador.tcpbolivia.bo/api/buscar`
- **M√©todo**: GET o POST
- **Par√°metros**:
  - Nombre del par√°metro de p√°gina: `page`, `pagina`, `offset`
  - Nombre del par√°metro de tama√±o: `size`, `limit`, `pageSize`
  - Otros par√°metros: a√±o, tipo, materia, etc.
- **Estructura del JSON de respuesta**:
  - Campo que contiene la lista de resultados: `data.items`, `results`, `documents`
  - Campo que contiene el total: `data.total`, `totalElements`, `pagination.total`

### 2. Actualizar config/sites_catalog.yaml

Edita `config/sites_catalog.yaml` y actualiza la secci√≥n `tcp_jurisprudencia`:

```yaml
sites:
  tcp_jurisprudencia:
    scraper:
      endpoints:
        busqueda_general:
          url: "/api/jurisprudencia/buscar"  # ‚Üê ACTUALIZAR con URL real
          metodo: "POST"  # ‚Üê ACTUALIZAR (GET o POST)

      paginacion:
        parametros:
          pagina: "page"      # ‚Üê ACTUALIZAR con nombre real del par√°metro
          tama√±o: "size"      # ‚Üê ACTUALIZAR con nombre real del par√°metro

      mapeo_campos:
        campo_resultados: "data.items"  # ‚Üê ACTUALIZAR con path real en JSON
        campo_total: "data.total"       # ‚Üê ACTUALIZAR con path real en JSON

        # Campos de cada documento
        id: "id"                               # ‚Üê ACTUALIZAR
        numero_resolucion: "numeroResolucion"  # ‚Üê ACTUALIZAR
        tipo_documento: "tipoDocumento"        # ‚Üê ACTUALIZAR
        fecha_resolucion: "fechaResolucion"    # ‚Üê ACTUALIZAR
        # ... etc
```

### 3. Configurar URL de PDF

Si el JSON no incluye la URL del PDF directamente:

```yaml
extraccion:
  generar_url_pdf: true
  patron_url_pdf: "https://buscador.tcpbolivia.bo/documentos/{id}.pdf"  # ‚Üê ACTUALIZAR
```

Reemplaza `{id}` con el campo que contiene el ID del documento.

## üöÄ Uso

### Comandos b√°sicos

```bash
# Modo TEST: solo 5 p√°ginas (para probar configuraci√≥n)
python main.py scrape tcp_jurisprudencia --mode test --limit 100

# Modo FULL: cobertura total (itera todos los a√±os)
python main.py scrape tcp_jurisprudencia --mode full --save-pdf

# Modo INCREMENTAL: solo √∫ltimos 30 d√≠as
python main.py scrape tcp_jurisprudencia --mode incremental

# Sin descargar PDFs (solo metadatos)
python main.py scrape tcp_jurisprudencia --mode full --no-pdf

# Con l√≠mite espec√≠fico
python main.py scrape tcp_jurisprudencia --mode full --limit 5000
```

### Modos de scraping

| Modo | Descripci√≥n | Uso |
|------|-------------|-----|
| `test` | Solo primeras 5 p√°ginas | Probar configuraci√≥n |
| `full` | Itera TODOS los a√±os (1999-presente) | Cobertura total |
| `incremental` | Solo √∫ltimos 30 d√≠as | Actualizaci√≥n diaria |

## üìä Validaci√≥n de cobertura

### Durante el scraping

El scraper muestra autom√°ticamente:

```
üìã VALIDACI√ìN DE COBERTURA
----------------------------------------------------------
Total documentos encontrados: 15234
Total PDFs descargados: 14987
Porcentaje √©xito descarga: 98.4%
```

### Despu√©s del scraping

Usa el script de validaci√≥n:

```bash
# Validar desde JSON
python scripts/validar_cobertura_tcp.py \
  --json data/raw/tcp_jurisprudencia/documentos_20250118_123456.json

# Comparar con total esperado de la API
python scripts/validar_cobertura_tcp.py \
  --json data/raw/tcp_jurisprudencia/documentos_20250118_123456.json \
  --total-esperado 15500
```

### C√≥mo obtener el total esperado

1. Inspecciona la primera llamada a la API en Network tab
2. Busca en el JSON de respuesta el campo que indica el total
3. Ejemplo de JSON:
   ```json
   {
     "data": {
       "total": 15500,  ‚Üê Este es el total esperado
       "items": [...]
     }
   }
   ```

### Verificar cobertura manualmente

```python
import json
import pandas as pd

# Cargar datos
with open('data/raw/tcp_jurisprudencia/documentos_TIMESTAMP.json') as f:
    docs = json.load(f)

# Convertir a DataFrame
df = pd.DataFrame(docs)

# An√°lisis
print(f"Total documentos: {len(df)}")
print(f"\nDocumentos por a√±o:")
df['a√±o'] = pd.to_datetime(df['fecha_resolucion']).dt.year
print(df['a√±o'].value_counts().sort_index())

# Verificar duplicados
print(f"\nDuplicados: {df.duplicated(subset=['id_documento']).sum()}")

# Verificar PDFs
print(f"\nCon PDF: {df['ruta_pdf'].notna().sum()} / {len(df)}")
```

### Indicadores de cobertura completa

‚úÖ **Buenas se√±ales:**
- Diferencia < 5% entre total obtenido y total esperado
- Distribuci√≥n uniforme por a√±os (no hay a√±os con 0-5 documentos)
- Sin duplicados de ID
- > 95% de PDFs descargados

‚ö†Ô∏è **Se√±ales de alerta:**
- Diferencia > 10% con total esperado ‚Üí revisar paginaci√≥n
- Varios a√±os con < 5 documentos ‚Üí paginaci√≥n cortada
- Muchos duplicados ‚Üí revisar l√≥gica de ID √∫nico
- < 80% PDFs descargados ‚Üí revisar URLs de PDF

## üîß Troubleshooting

### Error: "No se pudo obtener la p√°gina principal"

**Causa**: URL del endpoint incorrecta o API cambi√≥.

**Soluci√≥n**:
1. Inspecciona Network tab del navegador
2. Actualiza `endpoints.busqueda_general.url` en config

### Error: "Error parseando JSON"

**Causa**: La API devuelve HTML en lugar de JSON, o estructura cambi√≥.

**Soluci√≥n**:
1. Verifica que el m√©todo (GET/POST) sea correcto
2. Verifica headers (Content-Type: application/json)
3. Inspecciona la respuesta real en Network tab

### Cobertura incompleta (diferencia > 10%)

**Causas posibles:**
1. Paginaci√≥n cortada prematuramente
2. Par√°metros incorrectos (ej: usando `page` cuando deber√≠a ser `offset`)
3. Tama√±o de p√°gina limitado por la API

**Soluci√≥n**:
1. Verifica par√°metros de paginaci√≥n en config
2. Reduce `tama√±o_pagina_default` (ej: de 100 a 50)
3. Verifica que `campo_resultados` apunte al array correcto

### Muchos campos vac√≠os en CSV

**Causa**: Mapeo de campos incorrecto.

**Soluci√≥n**:
1. Inspecciona el JSON de un documento en Network tab
2. Actualiza `mapeo_campos` en config con los nombres reales
3. Usa notaci√≥n de punto para campos anidados: `data.documento.numero`

## üìÅ Archivos generados

Despu√©s del scraping, encontrar√°s en `data/raw/tcp_jurisprudencia/`:

```
data/raw/tcp_jurisprudencia/
‚îú‚îÄ‚îÄ documentos_20250118_123456.json    # Metadatos completos
‚îú‚îÄ‚îÄ documentos_20250118_123456.csv     # CSV para an√°lisis
‚îî‚îÄ‚îÄ pdfs/                              # PDFs descargados
    ‚îú‚îÄ‚îÄ Sentencia_Constitucional_0001-2024_abc123.pdf
    ‚îú‚îÄ‚îÄ Auto_Constitucional_0002-2024_def456.pdf
    ‚îî‚îÄ‚îÄ ...
```

### Estructura del JSON

```json
[
  {
    "id_documento": "abc123",
    "numero_resolucion": "0001/2024",
    "tipo_documento": "Sentencia Constitucional",
    "fecha_resolucion": "2024-01-15",
    "fecha_ingreso": "2024-01-10",
    "materia": "Amparo Constitucional",
    "sumilla": "Resumen del caso...",
    "expediente": "EXP-2024-001",
    "partes": "Juan P√©rez vs. Estado",
    "magistrado": "Dr. Magistrado Nombre",
    "url_pdf": "https://...",
    "ruta_pdf": "/path/to/pdf",
    "site_id": "tcp_jurisprudencia",
    "area_derecho": "Constitucional",
    "fecha_scraping": "2025-01-18T12:34:56",
    "json_raw": { ... }
  },
  ...
]
```

## üîÑ Actualizaci√≥n continua

Para mantener la base de datos actualizada:

```bash
# Ejecutar diariamente (cron)
0 2 * * * cd /path/to/buho && python main.py scrape tcp_jurisprudencia --mode incremental
```

Esto descargar√° solo documentos de los √∫ltimos 30 d√≠as.

## üÜö Diferencias con tcp_gaceta

| Caracter√≠stica | tcp_gaceta | tcp_jurisprudencia |
|----------------|------------|-------------------|
| Fuente | Gacetas TCP (tomos, gu√≠as) | Buscador de jurisprudencia |
| Tipo | PDF simple (listados) | Sentencias individuales |
| Tecnolog√≠a | Scraping HTML | API REST |
| Cobertura | Gacetas publicadas | Toda la jurisprudencia |
| Scraper | (otro scraper) | `tcp_jurisprudencia_scraper.py` |

**NO mezcles los dos scrapers. Son completamente independientes.**

## üêõ Debugging

Habilitar logs detallados:

```python
# En tcp_jurisprudencia_scraper.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

Ver requests:

```python
# Agregar al scraper
import http.client as http_client
http_client.HTTPConnection.debuglevel = 1
```

## üìö Referencias

- Sitios TCP:
  - https://buscador.tcpbolivia.bo/
  - https://jurisprudencia.tcpbolivia.bo/

- Documentaci√≥n del proyecto:
  - `README.md`: Documentaci√≥n general
  - `config/sites_catalog.yaml`: Configuraci√≥n completa
  - `scraper/sites/tcp_jurisprudencia_scraper.py`: C√≥digo fuente
