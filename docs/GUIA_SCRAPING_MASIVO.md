# üéØ GU√çA COMPLETA: Scraping Masivo de 30+ Sitios con Metadata Extendida

**Sistema Completo de Scraping ‚Üí Metadata ‚Üí Exportaci√≥n ‚Üí An√°lisis Continuo**

---

## üìã Tabla de Contenidos

1. [Visi√≥n General del Sistema](#visi√≥n-general-del-sistema)
2. [Preparaci√≥n del Entorno](#preparaci√≥n-del-entorno)
3. [Configuraci√≥n de Sitios (30+)](#configuraci√≥n-de-sitios-30)
4. [Scraping Hist√≥rico Completo](#scraping-hist√≥rico-completo)
5. [Sistema de Metadata Extendida](#sistema-de-metadata-extendida)
6. [Exportaci√≥n y Registro](#exportaci√≥n-y-registro)
7. [An√°lisis Continuo de Nuevos Documentos](#an√°lisis-continuo-de-nuevos-documentos)
8. [Monitoreo y M√©tricas](#monitoreo-y-m√©tricas)

---

## üéØ Visi√≥n General del Sistema

### Capacidades Completas

El sistema B√öHO ahora incluye:

‚úÖ **Pipeline Extendido**
```
Sitio Web ‚Üí PDF ‚Üí Texto ‚Üí Art√≠culos ‚Üí Metadata Extendida ‚Üí Exportaci√≥n ‚Üí Registro Hist√≥rico
```

‚úÖ **Metadata Autom√°tica**
- N√∫mero de norma
- Tipo de norma (Ley, DS, Resoluci√≥n, etc.)
- √Årea del derecho (constitucional, penal, tributario, etc.)
- Jerarqu√≠a normativa (1-99)
- Estado de vigencia
- Entidad emisora
- Normas modificadas/derogadas
- Palabras clave

‚úÖ **Exportaci√≥n Simult√°nea**
- CSV de documentos
- CSV de art√≠culos
- Registro hist√≥rico (JSONL)
- Reportes por sesi√≥n

‚úÖ **Tracking Hist√≥rico**
- Progreso por sitio
- Estad√≠sticas globales
- Historial de sesiones

---

## üöÄ Preparaci√≥n del Entorno

### Paso 1: Instalaci√≥n Base

```bash
# 1. Clonar (si no est√° clonado)
git clone https://github.com/zambogram/bo-gov-scraper-buho.git
cd bo-gov-scraper-buho

# 2. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Instalar dependencias adicionales para OCR (opcional pero recomendado)
# En Ubuntu/Debian:
sudo apt-get install tesseract-ocr tesseract-ocr-spa
pip install pytesseract pdf2image

# 5. Configurar variables de entorno
cp .env.example .env
# Editar .env seg√∫n necesidades
```

### Paso 2: Verificar Instalaci√≥n

```bash
# Verificar que el sistema est√° operativo
python main.py listar

# Debe mostrar los 5 sitios activos
```

---

## üìù Configuraci√≥n de Sitios (30+)

### Agregar Nuevos Sitios al Cat√°logo

Editar `config/sites_catalog.yaml` y agregar sitios adicionales:

```yaml
sites:
  # ... sitios existentes ...

  ministerio_trabajo:
    id: ministerio_trabajo
    nombre: "Ministerio de Trabajo, Empleo y Previsi√≥n Social"
    tipo: "Ministerio"
    categoria: "Laboral"
    url_base: "https://www.mintrabajo.gob.bo"
    url_search: "https://www.mintrabajo.gob.bo/normativa"
    prioridad: 2
    ola: 2
    activo: true
    metadatos:
      tipo_documentos:
        - "Resoluci√≥n Ministerial"
        - "Decreto Ejecutivo"
      fecha_inicio: "2000-01-01"
      idiomas: ["es"]
      formato_principal: "PDF"
      requiere_ocr: false
    scraper:
      tipo: "static"
      paginacion: true
      items_por_pagina: 50
      delay_entre_requests: 2

  tribunal_agroambiental:
    id: tribunal_agroambiental
    nombre: "Tribunal Agroambiental"
    tipo: "Tribunal"
    categoria: "Ambiental"
    url_base: "https://www.agroambiental.gob.bo"
    url_search: "https://www.agroambiental.gob.bo/jurisprudencia"
    prioridad: 2
    ola: 3
    activo: true
    metadatos:
      tipo_documentos:
        - "Sentencia Agroambiental"
        - "Resoluci√≥n"
      fecha_inicio: "2014-01-01"
      idiomas: ["es"]
      formato_principal: "PDF"
      requiere_ocr: true
    scraper:
      tipo: "dynamic"
      paginacion: true
      items_por_pagina: 20
      delay_entre_requests: 3

  # Agregar m√°s sitios siguiendo el mismo patr√≥n...
  # Total objetivo: 30+ sitios
```

### Crear Scrapers para Nuevos Sitios

Para cada nuevo sitio, crear un scraper en `scraper/sites/`:

**Ejemplo: `scraper/sites/ministerio_trabajo_scraper.py`**

```python
"""
Scraper para Ministerio de Trabajo
"""
from typing import List, Dict, Any, Optional
from pathlib import Path
import logging
from .base_scraper import BaseScraper

logger = logging.getLogger(__name__)


class MinisterioTrabajoScraper(BaseScraper):
    """Scraper para Ministerio de Trabajo"""

    def __init__(self):
        super().__init__('ministerio_trabajo')
        logger.info(f"Inicializado scraper para {self.config.nombre}")

    def listar_documentos(self, limite: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Listar documentos del Ministerio de Trabajo

        IMPLEMENTAR L√ìGICA REAL DE SCRAPING AQU√ç
        Por ahora, datos de ejemplo
        """
        # TODO: Implementar scraping real del sitio
        documentos_ejemplo = [
            {
                'id_documento': 'mteps_rm_0001_2024',
                'tipo_documento': 'Resoluci√≥n Ministerial',
                'numero_norma': '001/2024',
                'fecha': '2024-01-10',
                'titulo': 'RM 001/2024 - Salario M√≠nimo',
                'url': f'{self.config.url_base}/normativa/rm-001-2024.pdf',
                'sumilla': 'Incremento del salario m√≠nimo nacional'
            }
        ]

        if limite:
            documentos_ejemplo = documentos_ejemplo[:limite]

        return documentos_ejemplo

    def descargar_pdf(self, url: str, ruta_destino: Path) -> bool:
        """Descargar PDF"""
        # TODO: Implementar descarga real
        # Por ahora, usar m√©todo base
        return self._download_file(url, ruta_destino)
```

**Registrar el nuevo scraper en `scraper/sites/__init__.py`:**

```python
from .ministerio_trabajo_scraper import MinisterioTrabajoScraper

SCRAPERS = {
    # ... scrapers existentes ...
    'ministerio_trabajo': MinisterioTrabajoScraper,
}
```

---

## üé¨ Scraping Hist√≥rico Completo

### Estrategia para 30+ Sitios

#### Opci√≥n 1: Scraping Secuencial (Recomendado para Primera Vez)

```bash
# Ejecutar sitios en orden de prioridad
# Prioridad 1 (m√°s importantes)
python main.py scrape tcp --mode full --limit 100 --save-txt --save-json
python main.py scrape tsj --mode full --limit 100 --save-txt --save-json
python main.py scrape gaceta_oficial --mode full --limit 500 --save-txt --save-json

# Prioridad 2
python main.py scrape asfi --mode full --limit 200 --save-txt --save-json
python main.py scrape sin --mode full --limit 200 --save-txt --save-json
python main.py scrape contraloria --mode full --limit 100 --save-txt --save-json

# Prioridad 3 y m√°s...
# Continuar con todos los sitios
```

#### Opci√≥n 2: Scraping Masivo (Todos los Sitios)

```bash
# Procesar todos los sitios activos de una vez
# L√≠mite conservador para primera pasada
python main.py scrape all --mode full --limit 50 --save-txt --save-json

# Para scraping masivo sin l√≠mite (CUIDADO: puede tomar d√≠as)
python main.py scrape all --mode full --save-txt --save-json
```

#### Opci√≥n 3: Script Automatizado

Crear `scripts/scrape_historico_completo.sh`:

```bash
#!/bin/bash
#  Script para scraping hist√≥rico completo de todos los sitios

SITIOS=(
  "tcp"
  "tsj"
  "asfi"
  "sin"
  "contraloria"
  "gaceta_oficial"
  "ministerio_trabajo"
  "tribunal_agroambiental"
  # ... agregar los 30+ sitios
)

LIMITE=100  # L√≠mite por sitio

for sitio in "${SITIOS[@]}"; do
  echo "======================================"
  echo "Procesando: $sitio"
  echo "======================================"

  python main.py scrape "$sitio" --mode full --limit $LIMITE --save-txt --save-json

  # Pausa entre sitios para no sobrecargar
  sleep 30
done

echo "======================================"
echo "Scraping hist√≥rico completado"
echo "======================================"

# Mostrar estad√≠sticas
python main.py stats
```

Ejecutar:

```bash
chmod +x scripts/scrape_historico_completo.sh
./scripts/scrape_historico_completo.sh
```

---

## üè∑Ô∏è Sistema de Metadata Extendida

### Metadata Autom√°tica Extra√≠da

Para cada documento, el sistema extrae autom√°ticamente:

1. **Identificaci√≥n**
   - `numero_norma`: "1234", "456/2024", etc.
   - `tipo_norma`: "Ley", "Decreto Supremo", "Resoluci√≥n", etc.
   - `jerarquia`: 1-99 (1=CPE, 2=Ley, 3=DS, etc.)

2. **Clasificaci√≥n**
   - `area_principal`: "constitucional", "penal", "tributario", etc.
   - `areas_derecho`: Lista de √°reas detectadas (top 3)

3. **Estado**
   - `estado_vigencia`: "vigente", "modificada", "derogada"
   - `entidad_emisora`: "Asamblea Legislativa", "Presidencia", etc.

4. **Relaciones**
   - `modifica_normas`: Lista de normas que modifica
   - `deroga_normas`: Lista de normas que deroga

5. **Contenido**
   - `palabras_clave`: Lista de palabras clave del documento
   - `sumilla_generada`: Sumilla autom√°tica si no existe
   - `estadisticas`: Total caracteres, palabras, p√°ginas estimadas

### Consultar Metadata

La metadata se guarda en:

1. **JSON del documento** (`data/normalized/{site}/json/{id}.json`)
2. **CSV de exportaci√≥n** (`exports/{site}/{timestamp}/documentos.csv`)
3. **Registro hist√≥rico** (`exports/{site}/{timestamp}/registro_historico.jsonl`)

---

## üì§ Exportaci√≥n y Registro

### Archivos Generados por Sesi√≥n

Cada sesi√≥n de scraping genera:

```
exports/
‚îî‚îÄ‚îÄ {site_id}/
    ‚îî‚îÄ‚îÄ {timestamp}/
        ‚îú‚îÄ‚îÄ documentos.csv          # Tabla de documentos
        ‚îú‚îÄ‚îÄ articulos.csv           # Tabla de art√≠culos
        ‚îú‚îÄ‚îÄ registro_historico.jsonl # Log detallado (JSONL)
        ‚îî‚îÄ‚îÄ reporte_scraping.json   # Reporte de la sesi√≥n
```

### Formato de Exportaciones

**documentos.csv:**
```csv
id_documento,site,tipo_documento,numero_norma,fecha,titulo,area_principal,areas_derecho,jerarquia,estado_vigencia,total_articulos,...
tcp_sc_0001_2024,tcp,Sentencia Constitucional,0001/2024,2024-01-15,SC 0001/2024,constitucional,"constitucional,procesal",10,vigente,25,...
```

**articulos.csv:**
```csv
id_articulo,id_documento,numero,titulo,tipo_unidad,contenido_preview
tcp_sc_0001_2024_art_1,tcp_sc_0001_2024,1,DEL OBJETO,articulo,El presente decreto tiene por objeto...
```

**registro_historico.jsonl:** (una l√≠nea JSON por documento)
```json
{"timestamp":"2024-11-18T10:30:00","id_documento":"tcp_sc_0001_2024","area_principal":"constitucional","jerarquia":10,"total_articulos":25,"metadata_completa":{...}}
```

### Tracking Hist√≥rico Global

El archivo `data/tracking_historico.json` mantiene registro de:

```json
{
  "inicio_proyecto": "2024-11-18T09:00:00",
  "sitios": {
    "tcp": {
      "primera_sesion": "2024-11-18T09:05:00",
      "ultima_sesion": "2024-11-18T10:30:00",
      "total_sesiones": 5,
      "total_documentos": 150,
      "total_articulos": 3750,
      "sesiones": [...]
    }
  },
  "estadisticas_globales": {
    "total_documentos": 15000,
    "total_articulos": 375000,
    "total_sesiones": 50
  }
}
```

---

## üîÑ An√°lisis Continuo de Nuevos Documentos

### Configurar Scraping Peri√≥dico

Una vez completado el scraping hist√≥rico, configurar an√°lisis continuo:

**1. Script de Delta Update (`scripts/scrape_delta_daily.sh`):**

```bash
#!/bin/bash
# Script para delta updates diarios

SITIOS=(
  "tcp" "tsj" "asfi" "sin" "contraloria" "gaceta_oficial"
  # ... todos los sitios
)

for sitio in "${SITIOS[@]}"; do
  echo "Actualizando: $sitio"
  python main.py scrape "$sitio" --mode delta --limit 50 --save-txt --save-json
done

echo "Delta update completado"
```

**2. Configurar Cron Job (Linux/Mac):**

```bash
# Editar crontab
crontab -e

# Agregar l√≠nea para ejecutar diariamente a las 2 AM
0 2 * * * /ruta/a/bo-gov-scraper-buho/scripts/scrape_delta_daily.sh >> /var/log/buho_scraper.log 2>&1
```

**3. Configurar Tarea Programada (Windows):**

Usar Task Scheduler con:
- Trigger: Diario a las 2 AM
- Action: Ejecutar `python main.py scrape all --mode delta --limit 50`

### Ciclo Completo de Nuevos Documentos

Cada nuevo documento pasa por:

1. ‚úÖ **Detecci√≥n** (modo delta)
2. ‚úÖ **Descarga PDF**
3. ‚úÖ **Extracci√≥n de texto** (con OCR si necesario)
4. ‚úÖ **Parsing en art√≠culos**
5. ‚úÖ **Metadata extendida** (√°rea, tipo, jerarqu√≠a, etc.)
6. ‚úÖ **Guardado** (TXT, JSON)
7. ‚úÖ **Exportaci√≥n** (CSV, JSONL)
8. ‚úÖ **Registro hist√≥rico**
9. ‚úÖ **√çndice actualizado**

---

## üìä Monitoreo y M√©tricas

### Ver Estad√≠sticas Globales

```bash
# Estad√≠sticas de todos los sitios
python main.py stats

# Ver tracking hist√≥rico
cat data/tracking_historico.json | jq '.'

# Ver progreso de un sitio espec√≠fico
cat data/index/tcp/index.json | jq '.total_documentos'
```

### An√°lisis de Exportaciones

**Documentos por √°rea del derecho:**

```bash
# Contar documentos por √°rea
cat exports/tcp/*/documentos.csv | cut -d',' -f7 | sort | uniq -c | sort -rn

# Ejemplo de salida:
#  450 constitucional
#  320 civil
#  180 penal
#  ...
```

**Documentos por jerarqu√≠a:**

```bash
# Ver distribuci√≥n de jerarqu√≠as
cat exports/tcp/*/documentos.csv | cut -d',' -f9 | sort | uniq -c

# 10 = Sentencias Constitucionales
# 2 = Leyes
# etc.
```

### Dashboards (Futuro)

El sistema est√° preparado para integrar dashboards usando:
- Streamlit (ya implementado b√°sico)
- Grafana + InfluxDB
- PowerBI / Tableau (importar CSVs)

---

## ‚úÖ Checklist de Implementaci√≥n

### Fase 1: Configuraci√≥n (Semana 1)

- [ ] Instalar dependencias completas
- [ ] Configurar 30+ sitios en `sites_catalog.yaml`
- [ ] Crear scrapers para cada sitio nuevo
- [ ] Probar scraping de 1-2 documentos por sitio

### Fase 2: Scraping Hist√≥rico (Semanas 2-4)

- [ ] Ejecutar scraping hist√≥rico sitio por sitio
- [ ] Verificar metadata extendida
- [ ] Revisar exportaciones CSV/JSONL
- [ ] Validar tracking hist√≥rico

### Fase 3: An√°lisis Continuo (Semana 5+)

- [ ] Configurar delta updates autom√°ticos
- [ ] Implementar monitoreo de errores
- [ ] Crear reportes semanales
- [ ] Optimizar scrapers lentos

---

## üöÄ Comandos R√°pidos de Referencia

```bash
# Listar sitios
python main.py listar

# Scraping hist√≥rico completo de un sitio
python main.py scrape tcp --mode full --limit 100 --save-txt --save-json

# Delta update de todos los sitios
python main.py scrape all --mode delta --limit 50

# Estad√≠sticas
python main.py stats

# Ver tracking
cat data/tracking_historico.json | jq '.estadisticas_globales'

# Ver exportaciones de √∫ltimo scraping
ls -lh exports/tcp/

# Contar documentos procesados
find data/normalized/*/json -name "*.json" | wc -l
```

---

## üìû Troubleshooting

**Problema: OCR muy lento**
```bash
# Soluci√≥n: Desactivar OCR para sitios con PDFs digitales
# En sites_catalog.yaml, configurar:
metadatos:
  requiere_ocr: false
```

**Problema: Memoria insuficiente**
```bash
# Soluci√≥n: Procesar por lotes peque√±os
python main.py scrape tcp --mode full --limit 20
# Repetir hasta completar
```

**Problema: Sitio web bloquea scraping**
```bash
# Soluci√≥n: Aumentar delay
# En sites_catalog.yaml:
scraper:
  delay_entre_requests: 5  # Aumentar a 5 segundos
```

---

**√öltima actualizaci√≥n:** 2025-11-18
**Versi√≥n del sistema:** 2.0 (Metadata Extendida + Exportaci√≥n)
